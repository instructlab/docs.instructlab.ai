*[checkpoint]: Snapshots during training. They are scored individually and the best is selected.
*[CUDA]: Compute Unified Device Architecture - a parallel computing platform and API for general computing on GPUs by NVIDIA
*[DeepSpeed]: Deep learning optimization library for PyTorch
*[Granite]: Open source licensed LLM released by IBM
*[FSDP]: Full Sharded Data Parallel - a wrapper for sharding module parameters across data parallel workers, used within PyTorch
*[LAB]: Large-Scale Alignment for ChatBots
*[Labradorite]: LAB-enhanced Llama2 model
*[Llama]: LLM released by Meta
*[Llama CPP]: A C++ library for inference of Llama models, similar to vLLM
*[LLMs]: Large Language Models
*[LoRA]: Low Rank Adapter - fine-tuning algorithm used within PyTorch
*[Merlinite]: LAB-enhanced Mistral model developed by IBM
*[Mistral]: LLM released by Mistral AI
*[Mixtral]: LLM using Mixture of Experts by Mistral AI
*[MMLU]: Massive Multitask Language Understanding - an evaluation scheme used for knowledge benchmarking
*[MLX]: An array framework for machine learning research on Apple Silicon chips
*[MPS]: Metal Performance Shaders - a MacOS hardware accelerator, similar to CUDA kernels
*[MT-Bench]: Multi-turn benchmark - an evaluation scheme used for skills benchmarking
*[PEFT]: Parameter Efficient Fine-Tuning
*[PR-bench]: Evaluation scheme used for skills PR benchmarking
*[PR-mmlu]: Evaluation scheme used for knowledge PR benchmarking
*[PyTorch]: Library supporting tensors and dynamic neural networks in Python with strong GPU acceleration
*[QLoRA]: Quantized Low Rank Adapter - fine-tuning algorithm used within PyTorch
*[Quantization]: Process of reducing resource needs for a model by decreasing the range of the data type
*[SDG]: Synthetic Data Generation - the process where a model artificially generates data based on provided examples
*[tl;dr]: Too Long; Didn't Read
*[vLLM]: A library for LLM inference and serving, similar to Llama CCP. Provides an OpenAI-compatible API.
*[YAML]: Yet Another Markdown Language