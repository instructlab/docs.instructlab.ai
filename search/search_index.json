{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the \ud83d\udc36 InstructLab Project","text":"<p>InstructLab is a model-agnostic open source AI project that facilitates contributions to Large Language Models (LLMs).</p> <p>We are on a mission to let anyone shape generative AI by enabling contributed updates to existing LLMs in an accessible way.</p> <p>Our community welcomes all those who would like to help us enable everyone to shape the future of generative AI.</p>"},{"location":"#why-instructlab","title":"Why InstructLab","text":"<p>There are many projects rapidly embracing and extending permissively licensed AI models, but they are faced with three main challenges:</p> <ul> <li>Contribution to LLMs is not possible directly. They show up as forks, which forces consumers to choose a \u201cbest-fit\u201d model that isn\u2019t easily extensible. Also, the forks are expensive for model creators to maintain.</li> <li>The ability to contribute ideas is limited by a lack of AI/ML expertise. One has to learn how to fork, train, and refine models to see their idea move forward. This is a high barrier to entry.</li> <li>There is no direct community governance or best practice around review, curation, and distribution of forked models.</li> </ul> <p>InstructLab is here to solve these problems.</p> <p>The project enables community contributors to add additional \"skills\" or \"knowledge\" to a particular model.</p> <p>InstructLab's model-agnostic technology gives model upstreams with sufficient infrastructure resources the ability to create regular builds of their open source licensed models not by rebuilding and retraining the entire model but by composing new skills into it.</p> <p>Take a look at \"lab-enhanced\" models on the InstructLab Hugging Face page.</p>"},{"location":"#requirements","title":"\ud83d\udccb Requirements","text":"<ul> <li>\ud83c\udf4e Apple M1/M2/M3 Mac or \ud83d\udc27 Linux system (tested on Fedora).   We anticipate support for more operating systems in the future.</li> <li>C++ compiler</li> <li>Python 3.10 or Python 3.11</li> <li>Approximately 60GB disk space (entire process)</li> </ul> <p>Note</p> <p>Python 3.12 is currently not supported, because some dependencies don't work on Python 3.12, yet. As of now, we only support Python 3.10 or Python 3.11 (preferred). Any other Python version would install an older version of InstructLab (e.g., 0.17).</p> <p>Tip</p> <p>When installing the <code>ilab</code> CLI on macOS, you may have to run the <code>xcode-select --install</code> command, installing the required packages previously listed.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/","title":"Creating New Knowledge or Skills","text":""},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#generate-a-model-with-new-skills-and-knowledge","title":"\ud83d\udcbb Generate a model with new skills and knowledge","text":""},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#list-and-validate-your-new-data","title":"\ud83d\udcdc List and validate your new data","text":"<p>You can use the <code>ilab taxonomy diff</code> command to ensure <code>ilab</code> is registering your new knowledge or skills and your contributions are properly formatted. </p> <pre><code>(venv) $ ilab taxonomy diff\ncompositional_skills/writing/freeform/foo-lang/qna.yaml\nTaxonomy in $HOME/.local/share/instructlab/taxonomy is valid :)\n</code></pre> <p>This command displays any new or modified YAML files within your taxonomy tree. For example, the following is the expected result of a valid compositional skill contribution after adding a new skill called <code>foo-lang</code> to the freeform writing subdirectory.</p> <p>You can also validate your entire taxonomy by performing a diff against an empty base by using the <code>--taxonomy-base=empty</code> argument:</p> <pre><code>(venv) $ ilab taxonomy diff --taxonomy-base=empty\ncompositional_skills/general/tables/empty/qna.yaml\ncompositional_skills/general/tables/editing/add_remove/qna.yaml\n...\nTaxonomy in $HOME/.local/share/instructlab/taxonomy is valid :)\n</code></pre>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#generate-a-synthetic-dataset","title":"\ud83d\ude80 Generate a synthetic dataset","text":"<p>Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, <code>ilab data generate</code> can start a server for you if you provide a fully qualified model path via <code>--model</code>.</p> <p>1) To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:</p> <p>Default SDG</p> <pre><code>ilab data generate\n</code></pre> <p>With GPU acceleration</p> <pre><code>ilab data generate --pipeline full --gpus &lt;NUM_OF_GPUS&gt;\n</code></pre> <p>Without GPU acceleration:</p> <pre><code>ilab data generate --pipeline simple\n</code></pre> <p>Using a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1) to generate data:</p> <pre><code>ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n</code></pre> <p>\u23f3 This can take from 15 minutes to 1+ hours to complete, depending on your computing resources.</p> <p>Example output of <code>ilab data generate</code></p> <pre><code>(venv) $ ilab data generate\nINFO 2024-07-30 19:57:44,093 numexpr.utils:161: NumExpr defaulting to 8 threads.\nINFO 2024-07-30 19:57:44,452 datasets:58: PyTorch version 2.3.1 available.\nGenerating synthetic data using 'full' pipeline, 'mistral-7b-instruct-v0.2.Q4_K_M.gguf' model, './taxonomy' taxonomy, against http://localhost:8000/v1 server\nINFO 2024-07-30 19:57:45,084 instructlab.sdg:375: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.\nINFO 2024-07-30 19:57:45,090 instructlab.sdg.pipeline:153: Running pipeline single-threaded\nINFO 2024-07-30 19:57:47,820 instructlab.sdg.llmblock:51: LLM server supports batched inputs: False\nINFO 2024-07-30 19:57:47,820 instructlab.sdg.pipeline:197: Running block: gen_skill_freeform\nINFO 2024-07-30 19:57:47,820 instructlab.sdg.pipeline:198: Dataset({\nfeatures: ['task_description', 'seed_question', 'seed_response'],\nnum_rows: 5\n})\nINFO 2024-07-30 20:02:16,455 instructlab.sdg:411: Generated 1 samples\n...\n</code></pre> <p>The synthetic data set will be two files in the newly created in the datasets directory: <code>~/.local/share/instructlab/datasets</code>. These files will be named <code>skills_train_msgs_*.jsonl</code> and <code>knowledge_train_msgs_*.jsonl</code>.</p> <p>2) Verify the files have been created by running the <code>ls datasets</code> command in the <code>.local/share/instructlab</code> directory.</p> <pre><code>(venv) $ ls datasets/\n\u251c\u2500\u2500 node_datasets_2024-08-12T20_31_15\n\u251c\u2500\u2500 node_datasets_2024-08-13T19_51_48\n\u251c\u2500\u2500 knowledge_recipe_2024-08-12T20_31_15.yaml\n\u251c\u2500\u2500 knowledge_recipe_2024-08-13T19_51_48.yaml\n\u251c\u2500\u2500 knowledge_train_msgs_2024-08-12T20_31_15.jsonl\n\u251c\u2500\u2500 knowledge_train_msgs_2024-08-13T19_51_48.jsonl\n\u251c\u2500\u2500 skills_recipe_2024-08-12T20_31_15.yaml\n\u251c\u2500\u2500 skills_recipe_2024-08-13T19_51_48.yaml\n\u251c\u2500\u2500 skills_train_msgs_2024-08-12T20_31_15.jsonl\n\u251c\u2500\u2500 skills_train_msgs_2024-08-13T19_51_48.jsonl\n\u251c\u2500\u2500 messages_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl\n\u251c\u2500\u2500 messages_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl\n\u251c\u2500\u2500 test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_13_21.jsonl\n\u251c\u2500\u2500 test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_23_06.jsonl\n\u251c\u2500\u2500 test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl\n\u251c\u2500\u2500 test_mixtral-8x7b-instruct-v0-1_2024-08-13T19_47_59.jsonl\n\u251c\u2500\u2500 test_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl\n\u251c\u2500\u2500 train_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl\n\u2514\u2500\u2500 train_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl\n</code></pre> <p>Generating synthetic data on a different model: It is also possible to run the generate step against a different model via an OpenAI-compatible API. For example, the one spawned by <code>ilab model serve</code> or any remote or locally hosted LLM (e.g. via <code>ollama</code>, <code>LM Studio</code>, etc.). Run the following command:</p> <pre><code>ilab data generate --endpoint-url http://localhost:8000/v1\n</code></pre> <p>Generating synthetic data on the entire taxonomy repo: You can generate a synthetic dataset based on the entire contents of the taxonomy repo using the <code>--taxonomy-base=empty</code> option:</p> <pre><code>ilab data generate --taxonomy-base=empty\n</code></pre>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#training-the-model","title":"\ud83d\udc69\u200d\ud83c\udfeb Training the model","text":"<p>There are many options for training the model with your synthetic data-enhanced dataset.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#before-you-begin-training","title":"\u270b Before you begin training","text":"<p>There are a few models you need to download before running the InstructLab end-to-end workflow locally.</p> <p>Download the <code>granite-7b-lab</code> model for training: <pre><code>ilab model download --repository instructlab/granite-7b-lab \n</code></pre></p> <p>Download the <code>prometheus-8x7b-v2.0</code> for multi-phase training and benchmark evaluation. This model is not required for <code>simple</code> or <code>full</code> training. <pre><code>ilab model download --repository prometheus-eval/prometheus-8x7b-v2.0 --hf-token &lt;your-huggingface-token&gt; \n</code></pre></p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#instructlab-model-training-pipelines","title":"InstructLab model training pipelines","text":"<p><code>ilab model train</code> has three pipelines: <code>simple</code>, <code>full</code>, and <code>accelerated</code>. The default is <code>full</code>.</p> <ol> <li><code>simple</code> uses an SFT Trainer on Linux and MLX on MacOS. This type of training takes roughly an hour and produces the lowest fidelity model but should indicate if your data is being picked up by the training process.</li> <li><code>full</code> uses a custom training loop and data processing functions for the granite family of models. This loop is optimized for CPU and MPS functionality. Please use <code>--pipeline=full</code> in combination with <code>--device=cpu</code> (Linux) or <code>--device=mps</code> (MacOS). You can also use <code>--device=cpu</code> on a MacOS machine. However, MPS is optimized for better performance on these systems.</li> <li><code>accelerated</code> uses the instructlab-training library which supports GPU accelerated and distributed training. The <code>full</code> loop and data processing functions are either pulled directly from or based off of the work in this library.</li> </ol> <p>After running <code>ilab model train</code>, the output locations depend on the chosen pipeline or strategy:</p> Pipeline/Strategy Operating System Output Location/Details <code>simple</code> Linux Model saved in <code>models</code> directory as <code>ggml-model-f16.gguf</code>. <code>simple</code> MacOS Model saved in <code>&lt;model_name&gt;-mlx-q</code> directory. <code>full</code> Linux &amp; MacOS <code>.bin</code> and <code>.gguf</code> models saved in <code>~/.local/share/instructlab/checkpoints/hf_format</code> directory. Two models in each <code>sample_*</code> directory: one quantized (<code>Q4-M-K</code> suffix) and one full precision. <code>accelerated</code> Linux Models saved in <code>~/.local/share/instructlab/checkpoints</code>. Can be evaluated with <code>ilab model evaluate</code> to choose the best one. <code>lab-multiphase</code> Linux Phase 1 models saved in <code>~/.local/share/instructlab/phased/phase1/checkpoints</code> (Knowledge training). Phase 2 models saved in <code>~/.local/share/instructlab/phased/phase2/checkpoints</code> (Skills training). Evaluation is run for both phases to identify the best checkpoint. <p>To limit training time, you can adjust the <code>num_epoch</code> paramater in the <code>config.yaml</code> file. The maximum number of epochs for running the InstructLab end-to-end workkflow is 10.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#train-the-model-locally","title":"Train the model locally","text":"<p>Train the model with your synthetic data with the <code>ilab model train</code> command:</p> <pre><code>ilab model train\n</code></pre> <p>\u23f3 This step can potentially take several hours to complete depending on your computing resources. Please stop <code>ilab model chat</code> and <code>ilab model serve</code> first to free resources.</p> <p>When running multiphase training evaluation is run on each phase, we will tell you which checkpoint in this folder performs the best.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#train-the-model-locally-on-an-m-series-mac-or-on-linux-using-the-full-pipeline","title":"Train the model locally on an M-series Mac or on Linux using the full pipeline","text":"<p>To train the model locally on your M-Series Mac using our full pipeline and MPS or on your Linux laptop/desktop using CPU:</p> <pre><code>ilab model train --pipeline full --device mps --data-path &lt;path-to-sdg-dataset&gt;\n</code></pre> <pre><code>ilab model train --pipeline full --device cpu --data-path &lt;path-to-sdg-dataset&gt;\n</code></pre> <p>Example command</p> <pre><code>ilab model train --pipeline full --device cpu --data-path ~/.local/share/instructlab/datasets/knowledge_train_msgs_2024-10-23T09_14_44.jsonl\n</code></pre> <p>\u23f3 This process will take a while to complete. If you run for ~8 epochs it will take several hours.</p> <p><code>ilab model train</code> outputs a directory for each epoch that resembles the following structure:</p> <pre><code>$ ls ~/.local/share/instructlab/checkpoints/hf_format/samples_0/\n\u251c\u2500\u2500 added_tokens.json\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 pytorch_model.bin\n\u251c\u2500\u2500 pytorch_model.gguf\n\u251c\u2500\u2500 pytorch_model-Q4_K_M.gguf\n\u251c\u2500\u2500 special_tokens_map.json\n\u251c\u2500\u2500 tokenizer.json\n\u251c\u2500\u2500 tokenizer_config.json\n\u2514\u2500\u2500 tokenizer.model\n</code></pre> <p>This entire folder can be served on a system that supports vLLM using the <code>.bin</code> model. However, on most laptops you can serve either the full precision gguf: <code>pytorch_model.gguf</code> or the 4-bit-quantized one: <code>pytorch_model-Q4_K_M.gguf</code>.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#train-the-model-locally-on-an-m-series-mac-or-on-linux-using-the-simple-pipeline","title":"Train the model locally on an M-series Mac or on Linux using the simple pipeline","text":"<p>To train the model locally on your M-Series Mac using our simple pipeline and MLX or on your Linux laptop/desktop using an SFT Trainer:</p> <pre><code>ilab model train --pipeline simple\n</code></pre> <p>\u23f3 This process will take a little while to complete (time can vary based on hardware and output of <code>ilab data generate</code> but on the order of 5 to 15 minutes)</p> <p>On a Mac <code>ilab model train</code> outputs a brand-new model that is saved in the <code>&lt;model_name&gt;-mlx-q</code> directory called <code>adapters.npz</code> (in <code>Numpy</code> compressed array format). For example:</p> <pre><code>(venv) $ ls instructlab-granite-7b-lab-mlx-q/\n\u251c\u2500\u2500 adapters-010.npz\n\u251c\u2500\u2500 adapters-020.npz\n\u251c\u2500\u2500 adapters-030.npz\n\u251c\u2500\u2500 adapters-040.npz\n\u251c\u2500\u2500 adapters-050.npz\n\u251c\u2500\u2500 adapters-060.npz\n\u251c\u2500\u2500 adapters-070.npz\n\u251c\u2500\u2500 adapters-080.npz\n\u251c\u2500\u2500 adapters-090.npz\n\u251c\u2500\u2500 adapters-100.npz\n\u251c\u2500\u2500 adapters.npz\n\u251c\u2500\u2500 added_tokens.json\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 model.safetensors\n\u251c\u2500\u2500 special_tokens_map.json\n\u251c\u2500\u2500 tokenizer.json\n\u251c\u2500\u2500 tokenizer.model\n\u2514\u2500\u2500 tokenizer_config.json\n</code></pre>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#train-the-model-locally-with-gpu-acceleration","title":"Train the model locally with GPU acceleration","text":"<p>Training has support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see the GPU acceleration documentation for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.</p> <pre><code>ilab model train --pipeline accelerated --device cuda --data-path &lt;path-to-sdg-data&gt;\n</code></pre> <p>Example command</p> <pre><code>ilab model train --pipeline full --device cpu --data-path ~/.local/share/instructlab/datasets/knowledge_train_msgs_2024-10-23T09_14_44.jsonl\n</code></pre> <p>This version of <code>ilab model train</code> outputs brand-new models that can be served in the <code>~/.local/share/instructlab/checkpoints</code> directory.  These models can be run through <code>ilab model evaluate</code> to choose the best one.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#train-the-model-locally-with-multiphase-training-and-gpu-acceleration","title":"Train the model locally with multiphase training and GPU acceleration","text":"<p><code>ilab model train</code> supports multiphase training. This results in the following workflow:</p> <ol> <li>We train the model on knowledge</li> <li>Evaluate the trained model to find the best checkpoint</li> <li>We train the model on skills</li> <li>We evaluate the model to find the best overall checkpoint</li> </ol> <pre><code>ilab model train --strategy lab-multiphase --phased-phase1-data &lt;knowledge train messages jsonl&gt; --phased-phase2-data &lt;skills train messages jsonl&gt; -y\n</code></pre> <p>This command takes in two <code>.jsonl</code> files from your <code>datasets</code> directory, one is the knowledge jsonl and the other is a skills jsonl. The <code>-y</code> flag skips an interactive prompt asking the user if they are sure they want to run multiphase training.</p> <p>\u23f3 This command may take 3 or more hours depending on the size of the data and number of training epochs you run.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#train-the-model-in-the-cloud","title":"Train the model in the cloud","text":"<p>Follow the instructions in Training.</p> <p>\u23f3 Approximate amount of time taken on each platform:</p> <ul> <li>Google Colab: 5-10 minutes with a T4 GPU</li> <li>Kaggle: ~30 minutes with a P100 GPU.</li> </ul> <p>After that's done, you can play with your model directly in the Google Colab or Kaggle notebook. Model trained on the cloud will be saved on the cloud. The model can also be downloaded and served locally.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#test-the-newly-trained-model","title":"\ud83d\udcdc Test the newly trained model","text":"<ul> <li>Run the following command to test the model:</li> </ul> <pre><code>ilab model test\n</code></pre> <p>The output from the command will consist of a series of outputs from the model before and after training.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#evaluate-the-newly-trained-model","title":"\ud83e\uddea Evaluate the newly trained model","text":"<p>You can use the <code>ilab model evaluate</code> command to evaluate the models you are training with several benchmarks. Currently, four benchmarks are supported.</p> Benchmark Measures Full Name Description Reference MMLU Knowledge Massive Multitask Language Understanding Tests a model against a standardized set of knowledge data and produces a score based on the model's performance Measuring Massive Multitask Language Understanding MMLUBranch Knowledge N/A Tests your knowledge contributions against a base model and produces a score based on the difference in performance N/A MTBench Skills Multi-turn Benchmark Tests a model's skill at applying its knowledge against a judge model and produces a score based on the model's performance MT-Bench (Multi-turn Benchmark) MTBenchBranch Skills N/A Tests your skill contributions against a judge model and produces a score based on the difference in performance N/A <p>[!NOTE] Evaluation must be used with local models (safetensors or GGUF format). Using models directly from Hugging Face without downloading them is unsupported. GGUF models are not yet supported for mmlu and mmlu_branch evaluations MTBench and MTBenchBranch use prometheus-8x7b-v2.0 as the judge model by default. While you do not need to use this model as your judge, it is strongly recommended to do so if you have the necessary hardware resources. You can download it via <code>ilab model download</code>.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#running-mmlu","title":"Running MMLU","text":"<p>Example of running MMLU with a local safetensors model directory:</p> <pre><code>$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n$ ilab model evaluate --benchmark mmlu --model $ILAB_MODELS_DIR/instructlab/granite-7b-test\n...\n# KNOWLEDGE EVALUATION REPORT\n\n## MODEL (SCORE)\n/home/user/.cache/instructlab/models/instructlab/granite-7b-test (0.52/1.0)\n\n### SCORES (0.0 to 1.0):\nmmlu_abstract_algebra - 0.31\nmmlu_anatomy - 0.46\nmmlu_astronomy - 0.52\nmmlu_business_ethics - 0.55\nmmlu_clinical_knowledge - 0.57\nmmlu_college_biology - 0.56\nmmlu_college_chemistry - 0.38\nmmlu_college_computer_science - 0.46\nmmlu_college_mathematics - 0.34\nmmlu_college_medicine - 0.49\nmmlu_college_physics - 0.27\nmmlu_computer_security - 0.66\nmmlu_conceptual_physics - 0.38\nmmlu_econometrics - 0.39\nmmlu_electrical_engineering - 0.48\nmmlu_elementary_mathematics - 0.3\n...\n</code></pre> <p>The output of MMLU displays a much longer list of subjects.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#running-mmlubranch","title":"Running MMLUBranch","text":"<p>Example of running MMLUBranch with a local safetensors model directory:</p> <pre><code>$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n$ ilab model evaluate --benchmark mmlu_branch --model $ILAB_MODELS_DIR/instructlab/granite-7b-test --base-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab\n...\n# KNOWLEDGE EVALUATION REPORT\n\n## BASE MODEL (SCORE)\n/home/user/.cache/instructlab/models/instructlab/granite-7b-lab (0.74/1.0)\n\n## MODEL (SCORE)\n/home/user/.cache/instructlab/models/instructlab/granite-7b-test (0.78/1.0)\n\n### IMPROVEMENTS (0.0 to 1.0):\n1. tonsils: 0.74 -&gt; 0.78 (+0.04)\n</code></pre>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#running-mtbench","title":"Running MTBench","text":"<p>Example of running MTBench with a local safetensors model directory:</p> <pre><code>$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n$ ilab model evaluate --benchmark mt_bench --model $ILAB_MODELS_DIR/instructlab/granite-7b-test\n...\n# SKILL EVALUATION REPORT\n\n## MODEL (SCORE)\n/home/user/.cache/instructlab/models/instructlab/granite-7b-test (7.27/10.0)\n\n### TURN ONE (0.0 to 10.0):\n7.48\n\n### TURN TWO (0.0 to 10.0):\n7.05\n</code></pre>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#running-mtbenchbranch","title":"Running MTBenchBranch","text":"<p>Below is an example of running MTBenchBranch with a local safetensors model directory:</p> <pre><code>$ export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n$ export ILAB_TAXONOMY_DIR=$HOME/.local/share/instructlab/taxonomy\n$ ilab model evaluate --benchmark mt_bench_branch \\\n   --model $ILAB_MODELS_DIR/instructlab/granite-7b-test \\\n   --base-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab \\\n   --taxonomy-path $ILAB_TAXONOMY_DIR \\\n   --branch rc \\\n   --base-branch main\n...\n# SKILL EVALUATION REPORT\n\n## BASE MODEL (SCORE)\n/home/user/.cache/instructlab/models/instructlab/granite-7b-lab (5.78/10.0)\n\n## MODEL (SCORE)\n/home/user/.cache/instructlab/models/instructlab/granite-7b-test (6.00/10.0)\n\n### IMPROVEMENTS (0.0 to 10.0):\n1. foundational_skills/reasoning/linguistics_reasoning/object_identification/qna.yaml: 4.0 -&gt; 6.67 (+2.67)\n2. foundational_skills/reasoning/theory_of_mind/qna.yaml: 3.12 -&gt; 4.0 (+0.88)\n3. foundational_skills/reasoning/linguistics_reasoning/logical_sequence_of_words/qna.yaml: 9.33 -&gt; 10.0 (+0.67)\n\n### REGRESSIONS (0.0 to 10.0):\n1. foundational_skills/reasoning/unconventional_reasoning/lower_score_wins/qna.yaml: 5.67 -&gt; 4.0 (-1.67)\n2. foundational_skills/reasoning/mathematical_reasoning/qna.yaml: 7.33 -&gt; 6.0 (-1.33)\n3. foundational_skills/reasoning/temporal_reasoning/qna.yaml: 5.67 -&gt; 4.67 (-1.0)\n\n### NO CHANGE (0.0 to 10.0):\n1. foundational_skills/reasoning/linguistics_reasoning/odd_one_out/qna.yaml (9.33)\n2. compositional_skills/grounded/linguistics/inclusion/qna.yaml (6.5)\n</code></pre>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#serve-the-newly-trained-model","title":"\ud83c\udf74 Serve the newly trained model","text":"<p>If you have a server running, stop the server you have running by entering <code>ctrl+c</code> keys in the terminal running the server.</p> <p>\ud83c\udf4e Mac only: Before serving the newly trained model you must convert it to work with the <code>ilab</code> cli. The <code>ilab model convert</code> command converts the new model into quantized GGUF format which is required by the server to host the model in the <code>ilab model serve</code> command.</p> <p>1) Convert the newly trained model by running the following command:</p> <pre><code>```shell\n</code></pre> <p>ilab model convert    ```</p> <p>2) Serve the newly trained model locally via <code>ilab model serve</code> command with the <code>--model-path</code> argument to specify your new model:</p> <pre><code>ilab model serve --model-path &lt;new model path&gt;\n</code></pre> <p>Which model should you select to serve? After running the <code>ilab model convert</code> command, some files and a directory are generated. The model you will want to serve ends with an extension of <code>.gguf</code>    and exists in a directory with the suffix <code>trained</code>. For example:    <code>instructlab-granite-7b-lab-trained/instructlab-granite-7b-lab-Q4_K_M.gguf</code>.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#chat-with-the-new-model-not-optional-this-time","title":"\ud83d\udce3 Chat with the new model (not optional this time)","text":"<p>1) Try the fine-tuned model out live using the chat interface, and see if the results are better than the untrained version of the model with chat by running the following command:</p> <pre><code>ilab model chat -m &lt;New model path&gt;\n</code></pre> <p>If you are interested in optimizing the quality of the model's responses, please see <code>TROUBLESHOOTING.md</code></p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#submit-your-new-knowledge-or-skills","title":"\ud83c\udf81 Submit your new knowledge or skills","text":"<p>Of course, the final step is, if you've improved the model, to open a pull-request in the taxonomy repository that includes the files (e.g. <code>qna.yaml</code>) with your improved data.</p>"},{"location":"adding-data-to-model/creating_new_knowledge_or_skills/#contributing","title":"\ud83d\udcec Contributing","text":"<p>Check out our contributing guide to learn how to contribute.</p>"},{"location":"adding-data-to-model/creating_new_wikipedia_based_qna/","title":"Creating a new Wikipedia based qna.yaml","text":"<p>In this tutorial we will walk you through building out a new <code>qna.yaml</code> for adding new or updated knowledge to the <code>granite-7b-lab</code> model. Let's get started!</p> <p>The first thing we need to do is create a new directory to have a clean place to work and pull down some software. Most of the time, the easiest thing to update in the model is the Wikipedia entry, so luckily, <code>erictherobot</code> has written a helpful tool to pull down markdown versions of the articles for us.</p> <pre><code>mkdir instructlab\ncd instructlab\ngit clone git@github.com:erictherobot/wikipedia-markdown-generator.git\n</code></pre> <p>After this, clone down your instructlab knowledge docs repository. It can be named whatever you'd like, but if you use our https://ui.instructlab.ai, you'll notice you already have <code>instructlab-knowledge-docs</code>.</p> <pre><code>git clone git@github.com:&lt;USERNAME&gt;/instructlab-knowledge-docs.git\n</code></pre> <p>Next, we need to build a Python virtual environment and install the dependencies to get it to work. These commands cd into the directory, create the virtual environment with python3.11 (you may need to change the version of Python on your machine), activate the virtual environment, and then do the pip install the dependencies. You'll notice the <code>Texas_Longhorns_football</code> there, a Wikipedia article I wanted to pull down and create the <code>qna.yaml</code> against. You should choose whatever new knowledge you want to do here.</p> <pre><code>cd wikipedia-markdown-generator\npython3.11 -m venv venv-md-gen\nsource venv-md-gen/bin/activate\npip install -r requirements.txt\npython3 wiki-to-md.py Texas_Longhorns_football\n</code></pre> <p>Next, we go ahead and copy the markdown into the knowledge repository, and commit it to our repository and push it up to GitHub.</p> <pre><code>cp md_output/Texas_Longhorns_football.md ../instructlab-knowledge-docs/\ncd ../instructlab-knowledge-docs\ngit add .\ngit commit -m \"added markdown doc\"\ngit push origin main\ncd ..\n</code></pre> <p>Next we pull down the upstream public taxonomy directory, and <code>cd</code> into that directory.</p> <pre><code>git clone git@github.com:instructlab/taxonomy\ncd taxonomy\n</code></pre> <p>This next step is a \"best effort\" for you. As the taxonomy grows, there will be some obvious choices, but if you select a tree that hasn't been flushed out yet, you'll have to do your best to think about where you'd find the <code>qna.yaml</code>. In this case, the Dewey Decimal System says sports should be under arts; this is American Football, college level with the University of Texas. Also, notice the underscores for the spaces; this is important.</p> <pre><code>mkdir -p knowledge/arts/sports/american_football/college/university_of_texas/\n</code></pre> <p>Finally, you can pull down the <code>template_qna.yaml</code> and fill it out for the needed questions and answers. Be sure to put the context at a maximum of about 500 Tokens and questions and answers around 250 Tokens.</p> <pre><code>wget https://raw.githubusercontent.com/instructlab/taxonomy/main/docs/template_qna.yaml\nmv template_qna.yaml knowledge/arts/sports/american_football/college/university_of_texas/qna.yaml\n</code></pre> <pre><code>vim knowledge/arts/sports/american_football/college/university_of_texas/qna.yaml\n</code></pre>"},{"location":"cmb/","title":"About Community Model Build","text":"<p>The primary goal of the InstructLab project is to democratize AI. One of the key ways our community seeks to do that is to enable collaborative contributions to improve an open source-licensed LLM based on the Granite models.</p> <p>We are currently in a worldwide GPU shortage, and the InstructLab community project is actively pursuing hardware resources to enable regular builds of our collaborative community dataset into new versions of our model via industry and academic partnerships. </p>"},{"location":"cmb/build_process/","title":"Community Model Build Process","text":"<p>Note</p> <p>This document is the Community Build Process, these are the general steps to get the cmb built. If you are looking for the config.yaml that worked for <code>granite-3.0-8b-base</code> there it is.</p>"},{"location":"cmb/build_process/#community-model-build-diagram","title":"Community Model Build diagram","text":"<p>We have created a default <code>build.sh</code> script, which will live in a repository (soon). The actual commands are explained here, and this should be considered the source of truth.</p>"},{"location":"cmb/build_process/#add-the-prs-to-the-build-machines-taxonomy-tree","title":"Add the PRs to the build machine's taxonomy tree","text":"<p>Add the PRs you want to be built into the run. Tag the PRs with \"cmb-running.\"</p> <p>Example: <pre><code>mkdir -p compositional_skills/general/synonyms\nvi compositional_skills/general/synonyms/attribution.txt\nvi compositional_skills/general/synonyms/qna.yaml\n</code></pre> Or if you are pulling from GitHub: <pre><code>cd ~/.local/share/instructlab/taxonomy\ngit fetch origin pull/ID/head:BRANCH_NAME\ngit checkout BRANCHNAME\n</code></pre></p>"},{"location":"cmb/build_process/#verify-changes","title":"Verify changes","text":"<pre><code>ilab taxonomy diff\n</code></pre> <p>Warning</p> <p><code>~/.local/share/instructlab/datasets</code> -- should be empty before starting  Every gpu should be \"empty\", or <code>0%</code> check with <code>nvidia-smi</code></p> <p>Note</p> <p>These steps were tested on the <code>a100</code> x8 machine that was given to the team as of Dec 3<sup>rd</sup>, 2024. If you have different hardware you'll need a different profile, and different options.</p>"},{"location":"cmb/build_process/#reset-the-build-directories","title":"Reset the build directories","text":"<p>Move the old build directories away, or save them. Something along these lines: <pre><code>mv /home/instructlab/.local/share/instructlab/phased/journalfile.yaml /home/instructlab/.local/share/instructlab/phased/journalfile.yaml_$DATE\nmv /home/instructlab/.local/share/instructlab/datasets /home/instructlab/.local/share/instructlab/datasets_$DATE\nmv /home/instructlab/.local/share/instructlab/phased /home/instructlab/.local/share/instructlab/phased_$DATE\n</code></pre></p> <p>Create the directories you moved away: <pre><code>mkdir /home/instructlab/.local/share/instructlab/phased\nmkdir /home/instructlab/.local/share/instructlab/datasets\n</code></pre></p>"},{"location":"cmb/build_process/#add-the-instructlab_community-mixin","title":"Add the <code>instructlab_community</code> mixin","text":"<p>For the community build, off the <code>base</code> model, you should add the community data set, these are the steps: <pre><code>cd ~/.local/share/instructlab/datasets/\nwget https://huggingface.co/datasets/instructlab/InstructLabCommunity/resolve/main/instructlab_community.jsonl\ncd ~\n</code></pre></p>"},{"location":"cmb/build_process/#modify-your-config","title":"Modify your config","text":"<p><code>ilab config edit</code></p> <p>find the general section of your config and ensure it matches the following:</p> <pre><code>general:\n  # Debug level for logging.\n  # Default: 0\n  debug_level: 0\n  # Log format. https://docs.python.org/3/library/logging.html#logrecord-attributes\n  # Default: %(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s\n  log_format: '%(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s'\n  # Log level for logging.\n  # Default: INFO\n  log_level: INFO\n  # Use legacy IBM Granite chat template (default uses 3.0 Instruct template)\n  # Default: False\n  use_legacy_tmpl: true \n</code></pre> <p>use_legacy_tmpl must be true in order to generate data for and train the granite-3.0-8b-base model</p>"},{"location":"cmb/build_process/#create-the-data","title":"Create the data","text":"<pre><code># annouce the start of the SDG\nilab data generate --pipeline full --gpus 8\n# annouce the completion of the SDG\n</code></pre>"},{"location":"cmb/build_process/#run-the-training-after-the-generate-is-complete","title":"Run the training after the generate is complete","text":"<pre><code># annouce the start of the training\nilab model train --strategy lab-multiphase --phased-phase1-data /home/instructlab/.local/share/instructlab/datasets/knowledge_train_msgs_*.jsonl --phased-phase2-data /home/instructlab/.local/share/instructlab/datasets/skills_train_msgs_*.jsonl --skip-user-confirm --force-clear-phased-cache\n# annouce the completion of the training\n</code></pre>"},{"location":"cmb/build_process/#optional-post-training-evaluation-steps","title":"(optional) Post training evaluation steps","text":"<p>If you want to send a sanity check, you can set these two variables to do a subset of the training: <pre><code>export INSTRUCTLAB_EVAL_FIRST_N_QUESTIONS=10 # mtbench\nexport INSTRUCTLAB_EVAL_MMLU_MIN_TASKS=true # mmlu\n</code></pre></p> <p>(In case of sanity of a specific Sample Model creation) <pre><code>ilab model evaluate --benchmark mt_bench --model ~/.local/share/instructlab/checkpoints/hf_format/samples_XXXXXX\n</code></pre></p> <p>Tip</p> <p>We should do the revaluation because we want to reverify the numbers before going any farther.</p>"},{"location":"cmb/build_process/#general-benchmarking","title":"General Benchmarking","text":"<ul> <li><code>mmlu</code>: general model knowledge, general facts, it's a knowledge number out of 100</li> <li><code>mt_bench</code>: is a skill based, extraction, etc, out of 10</li> </ul> <p>Note</p> <p>we want around 7.1 for <code>mt_bench</code> average for a model candidate</p>"},{"location":"cmb/build_process/#specific-benchmarking","title":"Specific Benchmarking","text":"<p><code>mmlu_branch</code>: these are specific to the general knowledge</p> <pre><code>ilab model evaluate --benchmark mmlu_branch --model ~/.local/share/checkpoints/hf_format/&lt;checkpoint&gt; --tasks-dir ~/.local/share/instructlab/datasets/&lt;node-dataset&gt; --base-model ~/.cache/instructlab/models/granite-7b-redhat-lab\n</code></pre> <p><code>mt_bench_branch</code>:  these are specific for the skills <pre><code>ilab model evaluate --benchmark mt_bench_branch --model ~/.local/share/checkpoints/hf_format/&lt;checkpoint&gt; --taxonomy-path ~/.local/share/instructlab/taxonomy --judge-model ~/.cache/instructlab/models/prometheus-8x7b-v2-0 --base-model ~/.cache/instructlab/models/granite-7b-redhat-lab --base-branch main --branch main\n</code></pre></p>"},{"location":"cmb/build_process/#publish-to-huggingface","title":"Publish to Huggingface","text":"<p>Sanity check the model to make sure it does what you are expecting: <pre><code>ilab model chat --model /home/instructlab/.local/share/instructlab/phased/phase2/checkpoints/hf_format/samples_XXXXX\n</code></pre></p> <p>Copy the checkpoint to the repository directory: <pre><code>cp /home/instructlab/.local/share/instructlab/phased/phase2/checkpoints/hf_format/samples_XXXX/* ~/huggingface_repos/granite-3.0-8b-lab-community/\n</code></pre></p> <p>Add and commit the changes to the repository: <pre><code>cd ~/huggingface_repos/granite-3.0-8b-lab-community/\ngit add .\ngit commit -s\ngit push origin main\n</code></pre></p> <p>Congratulations, this is the core steps to building out the safe-tensors to publish to hugging face.</p>"},{"location":"community/CODE_OF_CONDUCT/","title":"InstructLab - Code of Conduct and Covenant","text":""},{"location":"community/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"community/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or  advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic  address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a  professional setting</li> </ul>"},{"location":"community/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"community/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"community/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the Code of Conduct Committee. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p> <p>You can find the Code of Conduct Committee member email addresses listed here.</p>"},{"location":"community/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"community/CODE_OF_CONDUCT_COMMITTEE/","title":"Filing a Code of Conduct Violation Report","text":"<p>We take reports of violations to our project Code of Conduct with the utmost seriousness and will act upon them as quickly as possible.</p> <p>To report a Code of Conduct violation to our Code of Conduct Committee, you may reach out to us by email at coc@instructlab.ai. The email will be read and acted upon by our Code of Conduct Committee members.</p> <p>If you experience a Code of Conduct violation in our InstructLab Discord workspace, please follow the instructions in our moderation guide to get immediate help in Discord.</p> <p>If you experience a Code of Conduct violation in our InstructLab Slack workspace, please follow the instructions in our moderation guide to get immediate help in Slack.</p> <p>As part of our follow up on your report, we would like to contact you for further discussion. If you would prefer not to engage beyond reporting the matter to the committee, please let us know that as part of your submission. We will respect your request.</p>"},{"location":"community/CODE_OF_CONDUCT_COMMITTEE/#code-of-conduct-committee-members","title":"Code of Conduct Committee Members","text":"<p>The current members of the CoCC are:</p> <ul> <li>Cara Delia, Red Hat</li> <li>Carol Chen, Red Hat</li> <li>Leslie Hawthorn, Red Hat</li> <li>JJ Ashgar, IBM</li> <li>Joe Sepi, IBM</li> <li>Maureen McElaney, IBM</li> </ul>"},{"location":"community/CONTRIBUTING/","title":"Contributing","text":"<p>\ud83d\udc4d\ud83c\udf89 First off, thank you for taking the time to contribute! \ud83c\udf89\ud83d\udc4d</p> <p>The following is a set of guidelines for contributing. These are just guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.</p>"},{"location":"community/CONTRIBUTING/#what-to-know-before-getting-started","title":"What to know before getting started","text":""},{"location":"community/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to a Code of Conduct. By participating, you are expected to uphold this code.</p> <p>Please report unacceptable behavior to one of the Code of Conduct Committee members.</p>"},{"location":"community/CONTRIBUTING/#related-repositories","title":"Related repositories","text":"<p>In addition to this repository, InstructLab has two related repositories:</p> <ul> <li><code>ilab</code> command-line interface (CLI) tool. This repository is responsible for the <code>ilab</code> command-line interface (CLI) tool.</li> <li>taxonomy tree. This repository is responsible for the taxonomy tree that allows you to create models tuned with your data.</li> </ul> <p>The following sections provide a general overview for contributing to any of the InstructLab repositories.</p>"},{"location":"community/CONTRIBUTING/#contributing-overview","title":"Contributing overview","text":"<p>Participating in the InstructLab project can come by contributing to any one of the repositories. The following workflow is designed to help you understand contribution best practices, and to help you begin your contribution journey. It will guide you through creating and picking up issues, working through them, having your work reviewed, and then getting your pull request merged.</p> <p>Help on open source projects is always welcome and there is always something that can be improved. For example, documentation (like the text you are reading now) can always use improvement, code can always be clarified, variables or functions can always be renamed or commented on, and there is always a need for more test coverage. If you see something that you think should be fixed, take ownership!</p> <p>To contribute code or documentation, please submit a pull request to the relevant repository. Note that contribution to any repository has its own set of requirements and expectations, and users should familiarize themselves with those expectations before contributing.</p>"},{"location":"community/CONTRIBUTING/#ilab-command-line-interface-cli-tool-repository","title":"ilab command-line interface (CLI) tool repository","text":"<p>We welcome contributions in the form of pull requests for documentation updates, code contributions and more. Prior to contribution, users should acquaint themselves with the <code>ilab</code> CLI repository contribution guide.</p> <p>To submit a pull request to the <code>ilab</code> CLI tool repository, see the pull request page.</p>"},{"location":"community/CONTRIBUTING/#taxonomy-repository","title":"Taxonomy repository","text":"<p>We welcome contributions in the form of pull requests for documentation updates, skills contributions, knowledge contributions and more. Prior to contribution, users should acquaint themselves with the taxonomy repository contribution guide.</p> <p>To submit a pull request to the taxonomy repository, see the pull request page.</p>"},{"location":"community/CONTRIBUTING/#community","title":"Community","text":"<p>We welcome contributions in the form of pull requests for documentation. To submit a pull request to the community repository, see the pull request page.</p>"},{"location":"community/CONTRIBUTING/#getting-started-with-contribution","title":"Getting started with contribution","text":"<p>The InstructLab project uses the Fork and Pull model for contribution that is common in many open source repositories; this entails multiple steps, including forking and cloning the repository, creating a pull request (PR), and more. For details on this process, check out The GitHub Workflow Guide from Kubernetes.</p> <p>After you have forked and cloned the repository, you can start the contribution process by looking at the issue trackers of the community repository, CLI repository, or the taxonomy repository. You can then pick up an issue by leaving a comment on said issue, and address the issue in a pull request (PR). Prior to submission, make sure that your changes pass formatting, linting, and unit tests. Additionally, all PRs must agree to the terms of Developer Certificate of Origin (DCO) by signing off your commits. Only PRs with commits signed off are accepted. If you didn't sign off your commits before creating the pull request, no worries, you can fix that after the fact. For more information about this process, see Developer Certificate of Origin (DCO).</p> <p>Then, you can submit the PR to be reviewed. In general, we follow the standard GitHub pull request process. Follow the provided template on your PR to include details about your pull request for the maintainers.</p> <p>[!IMPORTANT] If you are seeking to make a larger contribution, such as introducing a new feature or functionality, or refactoring a significant portion of the codebase to improve performance, readability, or maintainability, get in touch with us prior to starting. This helps ensure that your time is not wasted working on a change that the project developers will not accept into the codebase.</p>"},{"location":"community/CONTRIBUTING/#pull-request-review","title":"Pull request review","text":"<p>Once you've created a pull request (PR), maintainers will review your code and may make suggestions to fix before merging. It will be easier for your PR to receive reviews if you consider the criteria the reviewers follow while working. Remember to:</p> <ul> <li>Run tests locally and ensure that they pass</li> <li>Follow the project coding conventions</li> <li>Write detailed commit messages</li> <li>Break large changes into a logical series of smaller patches, which are easy to understand individually and combine to solve a broader issue</li> </ul> <p>For a list of the maintainers and triagers, see the MAINTAINERS.md page.</p>"},{"location":"community/CONTRIBUTING/#proposing-new-features","title":"Proposing new features","text":"<p>To propose a new feature, it's best to raise an issue in the appropriate repository:</p> <ul> <li>InstructLab CLI repository</li> <li>Taxonomy repository</li> </ul> <p>This way, features can be discussed with the project maintainers, ensuring that your time is not wasted working on a feature that the project developers will not accept into the codebase.</p>"},{"location":"community/CONTRIBUTING/#submitting-or-fixing-bugs","title":"Submitting or fixing bugs","text":"<p>To submit a new bug, raise an issue in the appropriate repository before creating a pull request. This ensures that the issue is appropriately tracked.</p> <p>To fix an existing bug, leave a comment on the issue that you are working on. Then, create a pull request and submit the pull request for review.</p>"},{"location":"community/CONTRIBUTING/#legal","title":"Legal","text":"<p>The following sections detail important legal information that should be viewed prior to contribution.</p>"},{"location":"community/CONTRIBUTING/#license-and-copyright","title":"License and Copyright","text":"<p>Distributed under the Apache License, Version 2.0.</p> <p>SPDX-License-Identifier: Apache-2.0</p> <p>If you would like to see the detailed LICENSE click here.</p>"},{"location":"community/CONTRIBUTING/#developer-certificate-of-origin-dco","title":"Developer Certificate of Origin (DCO)","text":"<p>We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO) - that the Linux\u00ae Kernel community uses to manage code contributions.</p> <p>We ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message. If you set your <code>user.name</code> and <code>user.email</code> in your <code>git config</code> file, you can sign your commit automatically by using the following command:</p> <pre><code>git commit -s\n</code></pre> <p>The following example includes a <code>Signed-off-by:</code> line, which indicates that the submitter has accepted the DCO:</p> <pre><code>Signed-off-by: John Doe &lt;john.doe@example.com&gt;\n</code></pre> <p>We automatically verify that all commit messages contain a <code>Signed-off-by:</code> line with your email address.</p>"},{"location":"community/CONTRIBUTING/#useful-tools-for-doing-dco-signoffs","title":"Useful tools for doing DCO signoffs","text":"<p>There are a number of tools that make it easier for developers to manage DCO signoffs.</p> <ul> <li>DCO command line tool, which lets you do a single signoff for an entire repo ( https://github.com/coderanger/dco )</li> <li>GitHub UI integrations for adding the signoff automatically ( https://github.com/scottrigby/dco-gh-ui )</li> <li>Chrome - https://chrome.google.com/webstore/detail/dco-github-ui/onhgmjhnaeipfgacbglaphlmllkpoijo</li> <li>Firefox - https://addons.mozilla.org/en-US/firefox/addon/scott-rigby/?src=search</li> </ul>"},{"location":"community/CONTRIBUTING/#communication","title":"Communication","text":"<p>You can join the InstructLab Slack workspace to communicate with project maintainers and your fellow users.</p>"},{"location":"community/CONTRIBUTING/#additional-resources","title":"Additional resources","text":"<p>The following resources include additional information about each repository, such as setting up the environment, testing the environment, coding styles, etc.</p>"},{"location":"community/CONTRIBUTING/#ilab-cli-tool-additional-resources","title":"ilab CLI tool additional resources","text":"<ul> <li> <p><code>ilab</code> CLI tool README.md. This resource provides information about the <code>ilab</code> CLI tool, including an overview, getting started, training the model, submitting a pull request, etc.</p> </li> <li> <p><code>ilab</code> CLI tool CONTRIBUTING.md. This resource provides information about contributing to the <code>ilab</code> CLI tool repository, reporting bugs, testing, coding styles, etc.</p> </li> </ul>"},{"location":"community/CONTRIBUTING/#taxonomy-additional-resources","title":"Taxonomy additional resources","text":"<ul> <li> <p>Taxonomy README.md. This resource provides information about the taxonomy repository, including getting started, YAML examples for skills and knowledge pull requests, how to contribute, etc.</p> </li> <li> <p>Taxonomy CONTRIBUTING.md. This resource contains information and best practices for contributing to the taxonomy repository.</p> </li> </ul>"},{"location":"community/FAQ/","title":"InstructLab FAQ","text":"<p>Last updated: October 2024</p> <p>[!TIP] AI is a rapidly-developing field with a lot of specialized terminology. You may wish read through the glossary before getting started with the documentation.</p>"},{"location":"community/FAQ/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Document summary</li> <li>General FAQ</li> <li>What is InstructLab?</li> <li>What is LAB?</li> <li>How does InstructLab work?</li> <li>What are the goals of the InstructLab project?</li> <li>How can I contribute?</li> <li>I'm having problems with the <code>ilab</code> CLI tool. What should I do?</li> <li>Why should I contribute?</li> <li>What large language models (LLMs) am I contributing to through the InstructLab project?</li> <li>What is Merlinite-7b?</li> <li>What is Granite-7b-lab?</li> <li>What is a \u201cskill\u201d?</li> <li>What is \u201cknowledge\u201d?</li> <li>Is the project looking for certain types of skill contributions?</li> <li>What are the acceptance criteria for a skills submission?</li> <li>What are the acceptance criteria for a knowledge submission?</li> <li>How can I submit a skill or knowledge?</li> <li>What happens after you submit a pull request?</li> <li>How are submissions reviewed?</li> <li>How long will it take for my pull request to be reviewed?</li> <li>If my pull request is accepted, how long will it take for my changes to appear in the next model update?</li> <li>What is the software license for InstructLab?</li> <li>Am I required to license code submissions to InstructLab under the Apache 2.0 license?</li> <li>My contribution requires submitting data along with code. What data is permissible to include?</li> <li>Where can I download updated models of InstructLab?</li> <li>I have a question about the project. Where should I go?</li> <li>What are the software and hardware requirements for using InstructLab?</li> <li>Glossary</li> <li>Additional Resources</li> </ul>"},{"location":"community/FAQ/#document-summary","title":"Document summary","text":"<p>This page serves as a comprehensive FAQ for the InstructLab project, detailing how it works, how to begin contribution, and the goals behind the project. Key information includes:</p> <ul> <li>InstructLab Overview: This open source project allows users to interact with and train the Granite-7b community AI Large Language Model (LLM) by contributing skills and knowledge.</li> <li>LAB Method: A synthetic data-based tuning method for LLMs consisting of a taxonomy-driven data curation process, a synthetic data generator, and two-phased training with replay buffers. Learn more in the Large-Scale Alignment for ChatBots paper outlining the methodology.</li> <li>Contribution Process: Contributors can add skills or knowledge to the LLM by creating YAML files and testing changes locally before submitting a pull request to InstructLab\u2019s GitHub taxonomy repository. Contributors may also contribute to the InstructLab tooling and library codebases.</li> <li>Project Goal: To democratize contributions to AI and LLMs.</li> </ul>"},{"location":"community/FAQ/#documentation-disclaimer","title":"Documentation disclaimer","text":"<p>There are currently three repositories that contain documentation crucial to getting users starting with the project:</p> <ul> <li>Community This repository shares InstructLab's activity and collaboration details across the community and include the most current information about the project, communication channels, and people processes.</li> <li><code>ilab</code> command-line interface (CLI) tool. This repository is responsible for the <code>ilab</code> CLI tool. It provides information about how to download the <code>ilab</code> CLI, how to contribute to the <code>ilab</code> CLI tool, among others.</li> <li>Taxonomy Tree. This repository is responsible for the taxonomy tree that allows you to create models tuned with your data. It provides information about what skills and knowledge are, how to create a pull request to contribute to the AI model, and expectations for pull request review.</li> </ul> <p>As this project grows, documentation and its organization will change. Members of this project will be made aware of significant changes and updates made to documentation.</p> <p>Unless otherwise noted, all documentation for the InstructLab project is licensed under the CC-BY-4.0 license.</p>"},{"location":"community/FAQ/#general-faq","title":"General FAQ","text":""},{"location":"community/FAQ/#what-is-instructlab","title":"What is InstructLab?","text":"<p>InstructLab (Large-scale Alignment for chatBots) is an open source initiative that provides a platform for easy engagement with AI Large Language Models (LLM) by using the <code>ilab</code> command-line interface (CLI) tool. You can use the CLI to work with Granite-7b to test new skills and knowledge, for example, asking it to write a meeting notes summary or answer a question about a particular subject. Users can then augment the LLM\u2019s capabilities by submitting the skills and knowledge they have tested to the project\u2019s taxonomy repository on GitHub by creating a pull request. This approach encourages community-driven enhancements without the need for complex model forking or fine-tuning of the model, promoting rapid development through collaborative contributions.</p> <p>[!IMPORTANT] Building models locally on consumer-grade hardware using quantized models with the <code>ilab</code> CLI is not meant for production-grade model creation. The <code>ilab</code> desktop configuration is meant for testing single knowledge or skill contributions on top of an already trained and quantized model. It is not for building a complete, production-grade model. For the full InstructLab production-grade model build process, multi-GPU hardware configurations are required, and the student model must be an untrained, unquantized base model.</p>"},{"location":"community/FAQ/#what-is-lab","title":"What is LAB?","text":"<p>LAB (Large-scale Alignment for chatBots) is a novel synthetic data-based align tuning method for LLMs from IBM Research. It consists of three components:</p> <ol> <li>A taxonomy-drive data curation process</li> <li>A large-scale synthetic data generator</li> <li>Multi-phased-training with replay buffers</li> </ol> <p>The LAB approach allows incrementally adding new knowledge and skills to an already pre-trained model without catastrophic forgetting.</p> <p>More information about the LAB method can be found on the Hugging Face project page.</p>"},{"location":"community/FAQ/#how-does-instructlab-work","title":"How does InstructLab work?","text":"<p>InstructLab is driven by taxonomies and works by empowering users to add new skills and knowledge to a pre-trained LLM.</p>"},{"location":"community/FAQ/#what-are-the-goals-of-the-instructlab-project","title":"What are the goals of the InstructLab project?","text":"<p>The goal on the InstructLab project is to democratize contributions to AI and LLMs. There are two approaches to achieving this goal in our community:</p> <ul> <li> <p>Enabling collaborative contribution to a large language model (LLM) through the project's taxonomy repository. When users contribute to this repository, the project resynthesizes its open source training data. Our community Granite-based model is then retrained, ensuring that community contributions are integrated while enriching the model\u2019s capabilities over time.</p> </li> <li> <p>Providing open source tooling to enable the InstructLab methodology and enabling community contributions to this toolset in accordance with open source project principles. This tooling includes the InstructLab core engine &amp; CLI as well as libraries such as the sdg, training, and evaluation libraries.</p> </li> </ul>"},{"location":"community/FAQ/#how-can-i-contribute","title":"How can I contribute?","text":"<p>You can begin your contribution journey by reading over the Contributing guide and joining the Community Discord Server or the Community Slack Channel.</p> <p>When you're ready to start contributing, you can follow the Getting Started guide. This guide shows you how to</p> <ul> <li>Install the <code>ilab</code> CLI.</li> <li>Deploy the LLM locally.</li> <li>Add skills or knowledge and train to the local LLM with your data.</li> <li>Create a pull request and add your information to the InstructLab taxonomy.</li> <li>Get reviews on your pull requests</li> </ul>"},{"location":"community/FAQ/#im-having-problems-with-the-ilab-cli-tool-what-should-i-do","title":"I'm having problems with the <code>ilab</code> CLI tool. What should I do?","text":"<p>A list of common problems associated with downloading the <code>ilab</code> CLI tool can be found in the CLI repository's discussion board.</p>"},{"location":"community/FAQ/#why-should-i-contribute","title":"Why should I contribute?","text":"<p>InstructLab is designed to enable collaboration around the InstructLab Granite models, open source licensed LLMs that contributors can access through Hugging Face. Participating is an opportunity to contribute to open source AI regardless of technical background.</p> <p>When contributors write an addition to the existing taxonomy, make a pull request, and get it reviewed and merged, their changes are rolled out in the next build. This update strategy expedites the model\u2019s capabilities and allows contributors to see the impact that they have made on the model much sooner than other LLMs.</p>"},{"location":"community/FAQ/#what-large-language-models-llms-am-i-contributing-to-through-the-instructlab-project","title":"What large language models (LLMs) am I contributing to through the InstructLab project?","text":"<p>Contributions to the InstructLab project include fine-tuning Granite-7b, an open-source licensed LLM. Contributors have direct access to the model they are improving through Hugging Face.</p>"},{"location":"community/FAQ/#what-is-merlinite-7b","title":"What is Merlinite-7b?","text":"<p>Merlinite-7b is a Mistral-7b derivative model fine-tuned with the LAB (Large-scale Alignment for chatBots) method using Mixtral-8x7b-Instruct as a teacher model.</p> <p>More information about the Merlinite-7b can be found on the Hugging Face project page.</p>"},{"location":"community/FAQ/#what-is-granite-7-lab","title":"What is Granite-7-lab?","text":"<p>Granite-7b-lab is a model that was built from scratch by IBM and fine tuned with the LAB (Large-scale Alignment for chatBots) method.</p> <p>More information about the Granite-7b can be found on the Hugging Face project page.</p>"},{"location":"community/FAQ/#what-is-a-skill","title":"What is a \u201cskill\u201d?","text":"<p>In the context of InstructLab, a skill is a capability domain submitted by a contributor intending to train the AI model on the submitted information. In other words, when you submit a skill, you teach the AI model how to do something.</p> <p>InstructLab skills are broken down into two main categories, compositional and foundational:</p> <ul> <li>Compositional skills. Composition or performative skills allow AI models to perform specific tasks or functions. With InstructLab, there are two types of composition skills:</li> <li>Freeform compositional skills are performative skills that do not require additional context. For example, to train an AI model to write a poem, you would provide examples of poems.</li> <li>Grounded compositional skills are performative skills that require additional context. One example is how an AI model reads the value of a cell in a table layout. To create the grounded skill to read a table formatted in Markdown, the additional context might be an example table layout.</li> <li>Foundational skills. Foundational skills are skills like math, reasoning, and coding. Note: Foundational skills are not currently being accepted.</li> </ul> <p>Skills are written in a YAML file and submitted to the InstructLab upstream project for review. See the Skills: YAML examples for different types of examples.</p>"},{"location":"community/FAQ/#what-is-knowledge","title":"What is \u201cknowledge\u201d?","text":"<p>Knowledge consists of data and facts. When creating knowledge for an AI model, you are providing it with additional data and information to answer questions more accurately. Whereas skills are the information that trains an AI model on how to do something, knowledge is based on the AI model\u2019s ability to answer questions that involve facts, data, or references.</p> <p>Like skills, knowledge submissions are submitted in YAML format to the InstructLab upstream project for review. See the Knowledge: YAML examples for different types of examples.</p>"},{"location":"community/FAQ/#is-the-project-looking-for-certain-types-of-skill-contributions","title":"Is the project looking for certain types of skill contributions?","text":"<p>Currently, InstructLab only accepts compositional (freeform and grounded) skills and knowledge. However, any type of freeform or grounded skill can be submitted. Some skills might not be added to the taxonomy repository for reasons such as duplication, submitting a skill that the model already does well, or submitting a controversial skill.</p> <p>Foundational skills are not currently being accepted.</p> <p>For a list of accepted skills, see Accepted Skills.</p>"},{"location":"community/FAQ/#what-are-the-acceptance-criteria-for-a-skills-submission","title":"What are the acceptance criteria for a skills submission?","text":"<p>Skills should seek to add capabilities or a knowledge domain to the AI model; in other words, a skills submission should teach the AI model how to do something instead of providing information about something. A good skills submission might address something that the AI model does poorly and seek to enhance its ability to execute that capability better. For a list of commonly accepted skills, see Accepted Skills.</p> <p>Skills submissions that are unlikely to be accepted include submitting a knowledge request instead of a skills request, submitting a skill that the model already does well, submitting a controversial skill, or submitting skills that do not execute pure math or coding. For a list of skills to avoid submitting, see Skills to Avoid.</p>"},{"location":"community/FAQ/#what-are-the-acceptance-criteria-for-a-knowledge-submission","title":"What are the acceptance criteria for a knowledge submission?","text":"<p>Requirements for knowledge submissions can be found in the Getting Started with Knowledge Contributions guide.</p>"},{"location":"community/FAQ/#how-can-i-submit-a-skill-or-knowledge","title":"How can I submit a skill or knowledge?","text":"<p>For information about submitting a skill after you have identified a gap, see the Ways to contribute guide.</p>"},{"location":"community/FAQ/#what-happens-after-you-submit-a-pull-request","title":"What happens after you submit a pull request?","text":"<p>After a pull request is submitted, a review is conducted by both the Taxonomy Triage team and the Taxonomy Approvers team to ensure that they are relevant, actionable, and have all of the required information needed to be a valuable addition to the AI model. Triagers might provide feedback and use labels to manage the state of the submitted pull request. Triagers also might provide informative feedback and helpful comments to improve the submission. After the pull request is approved, a Taxonomy Approver merges the skill.</p> <p>More information regarding basic review questions, subjective review questions, labels, and the reasons for approval, further review requirements, or rejection can be found on the Triaging contributions page of the GitHub repository.</p>"},{"location":"community/FAQ/#how-are-submissions-reviewed","title":"How are submissions reviewed?","text":"<p>For code review, the project maintainers use LGTM (Looks Good to Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers.</p> <p>For skills and knowledge PRs, your PR will be checked to ensure it is relevant, actionable, and has all the information necessary for the approval team to review and merge the PR. The Triage team will use labels to manage the state and action of PRs as well as provide feedback to contributors based upon the following review guidelines:</p> <ul> <li>Does the PR have the pull request template information filled out?</li> <li>Did all the PR checks pass?</li> <li>Does the skill have three or more examples?</li> <li>Are the YAML fields correct?</li> <li>No PII in content</li> <li>Does this content include anything documented in the project's Avoid these Topics guidelines?</li> <li>Does it adhere to the Code of Conduct guidelines?</li> <li>Was a response clearly generated by the LLM?</li> </ul>"},{"location":"community/FAQ/#how-long-will-it-take-for-my-pull-request-to-be-reviewed","title":"How long will it take for my pull request to be reviewed?","text":"<p>Due to the large number of contributions currently being received, it is difficult to provide an exact timeline for reviewing your pull request.</p>"},{"location":"community/FAQ/#if-my-pull-request-is-accepted-how-long-will-it-take-for-my-changes-to-appear-in-the-next-model-update","title":"If my pull request is accepted, how long will it take for my changes to appear in the next model update?","text":"<p>After a pull request is accepted, the changes are regularly incorporated into InstructLab.</p>"},{"location":"community/FAQ/#what-is-the-software-license-for-instructlab","title":"What is the software license for InstructLab?","text":"<p>The InstructLab project as well as the Granite-7b models are distributed under Apache License, Version 2.0.</p>"},{"location":"community/FAQ/#what-is-the-content-license-for-instructlab-documentation","title":"What is the content license for InstructLab documentation?","text":"<p>Unless otherwise specified, all documentation for InstructLab is licensed under the CC-BY-4.0 license from Creative Commons.</p>"},{"location":"community/FAQ/#am-i-required-to-license-code-submissions-to-instructlab-under-the-apache-20-license","title":"Am I required to license code submissions to InstructLab under the Apache 2.0 license?","text":"<p>Yes. Code contributions to the InstructLab project are subject to the terms and conditions under the Apache 2.0 license.</p>"},{"location":"community/FAQ/#my-contribution-requires-submitting-data-along-with-code-what-data-is-permissible-to-include","title":"My contribution requires submitting data along with code. What data is permissible to include?","text":"<p>It is recommended that third-party content be licensed with an open data license that does not restrict commercial use or the creation of derivative works, including the following licenses:</p> <ul> <li>CC0</li> <li>CDLA-Permissive</li> <li>CC-BY-4.0</li> <li>CC-BY-4.0 SA</li> <li>Apache 2.0</li> <li>MIT</li> </ul>"},{"location":"community/FAQ/#do-submissions-to-the-project-require-a-contributor-license-agreement-of-some-kind","title":"Do submissions to the project require a contributor license agreement of some kind?","text":"<p>The InstructLab project follows the same approach (the Developer's Certificate of Origin 1.1 (DCO)) that the Linux Kernel community uses to manage code contributions. Unless the file says otherwise for this project, the relevant open source license is the Apache License, Version 2.0.  When submitting a patch for review, you must include a sign-off statement in the commit message. See the \"Legal\" section of the Contributing document.</p> <p>You can find more information about useful tools for managing DCO sign-off in our Community Contributions Guide.</p>"},{"location":"community/FAQ/#where-can-i-download-updated-models-of-instructlab","title":"Where can I download updated models of InstructLab?","text":"<p>The latest version of InstructLab can be downloaded using the <code>ilab download</code> CLI command, as well as from InstructLab on Hugging Face.</p>"},{"location":"community/FAQ/#i-have-a-question-about-the-project-where-should-i-go","title":"I have a question about the project. Where should I go?","text":"<p>Currently, the best method for communicating with peers and project maintainers is in the Community Discord/Slack servers. Visit our InstructLab Slack Workspace Guide, InstructLab Slack Workspace Guide for information on how to join.</p> <p>See our community collaboration page, including information on our mailing list, meetings, and other ways of interacting with the community.</p>"},{"location":"community/FAQ/#what-are-the-software-and-hardware-requirements-for-using-instructlab","title":"What are the software and hardware requirements for using InstructLab?","text":"<p>The local training is the most hardware intensive part of this process. Your hardware determines how fast/slow training the model locally will take.</p> <p>To run and train InstructLab locally, you must meet the following requirements:</p> <ul> <li>A supported operating system</li> <li>A Linux-based operating system</li> <li>An Apple Silicon M1, M2, or M3 system</li> <li>A Windows system with WSL (Windows Subsystem for Linux)</li> <li>Python 3.9 or later, including the development headers</li> <li>Approximately 10GB of free disk space to get through the <code>ilab generate</code> step</li> <li>Approximately 60GB of free disk space is needed to run the entire process locally on Apple hardware</li> <li>About 32 GB RAM</li> </ul> <p>[!IMPORTANT] Some of our community members have reported challenges in working with Windows and WSL for InstructLab support. If possible, you may want to work with Linux or Mac for the smoothest experience. We are continuing to work on improvements across our supported operating systems for the local desktop InstructLab tooling experience.</p>"},{"location":"community/FAQ/#glossary","title":"Glossary","text":"Term Explanation Additional Reference Checkpoints Snapshots during training. They are scored individually and the best is selected. N/A CUDA \u201cCompute Unified Device Architecture\u201d - A parallel computing platform and API for general computing on GPUs by NVIDIA. Ref DeepSpeed Deep learning optimization library for PyTorch Ref Granite Open source licensed LLM released by IBM Ref FSDP \u201cFull Sharded Data Parallel\u201d - A wrapper for sharding module parameters across data parallel workers, used within PyTorch Ref LAB \u201cLarge-Scale Alignment for ChatBots\u201d Ref Labradorite LAB-enhanced Llama2 model N/A Llama LLM released by Meta N/A Llama CPP A C++ library for inference of Llama models, similar to vLLM Ref LoRA \u201cLow Rank Adapter\u201d - Fine-tuning algorithm used within PyTorch Ref Merlinite LAB-enhanced Mistral model developed by IBM N/A Mistral LLM released by Mistral AI N/A Mixtral LLM using Mixture of Experts by Mistral AI N/A MMLU \u201cMassive Multitask Language Understanding\u201d - An evaluation scheme used for knowledge benchmarking Ref MLX An array framework for machine learning research on Apple Silicon chips Ref MPS \u201cMetal Performance Shaders\u201d - A MacOS hardware accelerator, similar to CUDA kernels N/A MT-Bench \u201cMulti-turn benchmark\u201d - An evaluation scheme used for skills benchmarking Ref PEFT \u201cParameter Efficient Fine-Tuning\u201d N/A PR-bench Evaluation scheme used for skills PR benchmarking N/A PR-mmlu Evaluation scheme used for knowledge PR benchmarking N/A PyTorch Library supporting tensors and dynamic neural networks in Python with strong GPU acceleration Ref QLoRA \"\u201cQuantized Low Rank Adapter\u201d - Fine tuning algorithm used within PyTorch Ref Quantization Process of reducing resource needs for a model by decreasing the range of the data type Ref SDG \u201cSynthetic Data Generation\u201d - The process where a model artificially generates data based on provided examples. N/A vLLM A library for LLM inference and serving, similar to Llama CPP. Provides an OpenAI-compatible API. Ref"},{"location":"community/FAQ/#additional-resources","title":"Additional Resources","text":"<p>Additional resources, including the Code of Conduct, Code of Conduct Committee members, how to contribute, how to join the Discord or Slack server, and more, can be found in the following repositories:</p> <p>InstructLab Taxonomy Repository</p> <p>InstructLab CLI Repository</p> <p>InstructLab Community Repository</p> <p>Discord and communication</p> <ul> <li>Joining the Discord Server</li> <li>Discord Moderation</li> </ul> <p>Slack and communication</p> <ul> <li>Joining the Slack Channel</li> <li>Slack Moderation</li> </ul>"},{"location":"community/GOVERNANCE/","title":"InstructLab Governance","text":"<p>The following document outlines how the InstructLab project governance operates.</p>"},{"location":"community/GOVERNANCE/#the-instructlab-project","title":"The InstructLab Project","text":"<p>InstructLab is made up of several projects that are defined as codebases and services with different release cycles. Collectively, these enable large-model development. Currently, these projects include the following:</p> <ul> <li><code>ilab</code> command-line interface (CLI) tool. This repository is responsible for the <code>ilab</code> command-line interface (CLI) tool.</li> <li>taxonomy tree. This repository is responsible for the taxonomy tree that allows you to create models tuned with your data.</li> </ul>"},{"location":"community/GOVERNANCE/#governance-structure-and-roadmap","title":"Governance Structure and Roadmap","text":"<p>The InstructLab Project has a two-level governance structure with an Oversight Committee and Project Maintainers.</p> <p>Except where otherwise noted, decisions should always start at the most local level of project governance. For example, decisions that affect only one project, such as the taxonomy repository and not the <code>ilab</code> CLI tool, can happen within the taxonomy project. While communication between the different project teams is important as they are all interconnected, minor decisions do not need organization-wide consensus and can be moved forward at the project level.</p> <p>Changes in maintainership and other governance are currently announced on the InstructLab community Discord and Slack servers. Directions to join the Discord server can be found here, and instructions for the Slack server can be found here. Changes are also announced to the announce mailing list.</p>"},{"location":"community/GOVERNANCE/#project-maintainers-overview","title":"Project Maintainers overview","text":"<p>Project Maintainers focus on a single codebase, a group of related codebases, a service (for example, a website), or a project to support other projects (such as marketing or community management).</p> <p>Project Maintainers are responsible for activities surrounding the development and release of code, the operation of any services that they own, or the tasks needed to execute their project (for example, community management or setting up an event booth). Technical decisions for code reside with the project Maintainers unless there is a decision related to multiple maintainer groups that cannot be resolved by those groups. Those cases can be escalated to the Oversight Committee.</p> <p>To be considered an active project Maintainer, it is required to be associated with at least one active, non-archived project. If only listed on archived projects, they become emeritus Maintainers and are no longer eligible to become an organization Maintainer.</p> <p>Project Maintainers do not need to be software developers; however, they must be substantial contributors. For example, if a repository is for documentation it would be appropriate for a project Maintainer to be an editor or technical writer.</p> <p>Advancement to the project Maintainer position, removal or stepping down, and duties are detailed in the Contributor Roles. The list of current maintainers can be found here.</p>"},{"location":"community/GOVERNANCE/#instructlab-oversight-committee-overview","title":"InstructLab Oversight Committee Overview","text":"<p>The initial Oversight Committee at the launch of the project was appointed by the founding sponsors of the InstructLab project. This bootstrap committee will serve until the first election of the Oversight Committee using processes and timing as determined by this group.</p> <p>The list of Oversight Committee members can be found in MAINTAINERS.md.</p> <p>The Oversight Committee consists of 3 to 7 leaders on the InstructLab project.  These members will serve to supervise the overall project and its health. It will also consist of a selected Chair member who will set agendas and call meetings. These meetings can be public or private at the discretion of the Oversight Committee.</p> <p>The Oversight Committee is responsible for the following duties:</p> <ul> <li>Maintaining the mission, vision, values, and scope of the project</li> <li>Refining the governance and charter as needed</li> <li>Making project-level decisions, including setting technical policies that apply across all components</li> <li>Resolving escalated project decisions when the team responsible is blocked</li> <li>Managing the InstructLab brand</li> <li>Controlling access to InstructLab assets such as source repositories and hosting</li> <li>Appointing members to the Code of Conduct Committee</li> <li>Deciding what projects are part of the InstructLab project</li> <li>Overseeing the resolution and disclosure of security issues</li> <li>Managing financial decisions related to the project</li> </ul>"},{"location":"community/GOVERNANCE/#draft-oversight-committee-selection-process","title":"Draft Oversight Committee selection process","text":"<p>Note</p> <p>This section is a draft. It is a responsibility of the initial Oversight Committee to finalize this process.</p> <p>The Oversight Committee will be selected and maintained using the following process:</p> <p>A project Maintainer of any active (non-archived) InstructLab organization project is eligible for a position as an organization Maintainer. Once a year, the Oversight Committee will be re-elected. The election will consist of a nomination period followed by an election period. Any person who has made a contribution to any repository under the InstructLab GitHub organization may nominate a suitable project Maintainer of an active project.</p> <p>The election will proceed according to the following process:</p> <ol> <li> <p>The nomination period will be three weeks. This period starts from the day after an organization Maintainer opening becomes available.</p> </li> <li> <p>The nomination must be made on the InstructLab community mailing list.</p> </li> <li> <p>After a nominated individual agrees to be a candidate for the Oversight Committee, project Maintainers will vote. The voting period will be open for a minimum of three business days. It will remain open until a supermajority of project Maintainers have voted. Only current Maintainers of active projects are eligible to vote.</p> </li> <li> <p>When the number of nominated individuals matches the number of openings, each individual must have a Yes vote from a supermajority of those who voted.</p> </li> <li> <p>When there are more individuals than open positions, voting will use a Ranked Choice voting method, such as Condorcet.</p> </li> </ol>"},{"location":"community/GOVERNANCE/#resignation-or-departure-from-the-maintainer-or-the-oversight-committee-role","title":"Resignation or Departure from the Maintainer or the Oversight Committee role","text":"<p>Project Maintainers or Oversight Committee members may resign or could be expelled as follows:</p> <ul> <li> <p>Maintainers or an Oversight Committee member may step down through email. Within 7 calendar days, organization contributors and Maintainers will be notified on the InstructLab community mailing list.</p> </li> <li> <p>After an Oversight Committee member steps down, they become an emeritus Maintainer.</p> </li> <li> <p>Maintainers and Committee members MUST remain active on the project. In the event that an Oversight Committee member or a Maintainer is unresponsive or inactive for more than 3 months, they may be removed by a supermajority vote.</p> </li> <li> <p>Maintainers and Oversight Committee members who have violated the Code of Conduct may be removed by a supermajority vote of the remaining Oversight Committee members.</p> </li> </ul>"},{"location":"community/GOVERNANCE/#decision-making-at-the-instructlab-organization-level","title":"Decision making at the InstructLab organization level","text":"<p>Generally, there are methods for decision making for the InstructLab project: by lazy consensus or by voting.</p> <p>The default decision-making process is lazy-consensus. This means that any decision is considered supported by the team making it so long as no one objects. Silence on any consensus decision is implicit agreement, and equivalent to explicit agreement. An explicit agreement may be stated at will.</p> <p>When a consensus cannot be met, a Maintainer can call for a majority vote on a decision.</p> <p>Many of the day-to-day project maintenance can be done through the lazy consensus model.</p> <p>The secondary decision-making process is done by voting. The following items are examples that must be called to a vote and conducted by the appropriate body:</p> <ul> <li>Appointing or removing a member of the Code of Conduct Committee (supermajority of the Oversight Committee)</li> <li>Carrying out Code of Conduct decisions requiring severe censure (majority of the Code of Conduct committee)</li> <li>Removing a Maintainer for any reason other than inactivity (supermajority of the Oversight Committee)</li> <li>Non-trivial changes to the governance (this document) (supermajority of the Oversight Committee)</li> <li>Licensing and intellectual property changes such as new logos or wordmarks (majority of the Oversight Committee)</li> <li>Adding, archiving, or removing projects (majority of the Oversight Committee)</li> </ul> <p>Other decisions may be called out and put up for decision on the InstructLab community mailing list. This can be done by anyone at any time. By default, any decisions called to a vote will be for a simple majority vote of the Oversight Committee.</p>"},{"location":"community/GOVERNANCE/#code-of-conduct","title":"Code of Conduct","text":"<p>InstructLab's Code of Conduct is enforced by the Code of Conduct Committee (CoCC). This committee will be appointed and removed by the Oversight Committee using a supermajority vote.</p> <p>The CoCC is responsible for investigating, evaluating, and recommending remedies for substantiated Code of Conduct incidents to the appropriate body. The CoCC will judge possible violations around principles of restorative justice rather than punishment. All teams within InstructLab are obligated to support the CoCC's recommendations on remedies.</p> <p>Current CoCC members can be found on the Code of Conduct Committee page.</p> <p>Possible Code of Conduct violations should be reported to the Code of Conduct Committee via the Code of Conduct email alias.</p>"},{"location":"community/GOVERNANCE/#developer-certificate-of-origin-dco-and-licenses","title":"Developer Certificate of Origin (DCO) and Licenses","text":"<p>The following licenses and contributor agreements will be used for InstructLab projects:</p> <ul> <li>Apache 2.0 for code</li> <li>Creative Commons Attribution 4.0 International Public License for documentation</li> <li>Developer Certificate of Origin for new contributions</li> </ul>"},{"location":"community/GOVERNANCE/#modifications-to-this-governance","title":"Modifications to this Governance","text":"<p>This governance may be modified by a supermajority vote of the Oversight Committee.</p> <p>Trivial changes that do not introduce policy changes may be approved by two members of the Oversight Committee.</p>"},{"location":"community/InstructLab_DISCORD_GUIDE/","title":"InstructLab Discord Server Guide","text":"<p>The purpose of this document is to inform members about how to join the InstructLab Discord Server and outline the channels available. We look forward to meeting everyone and welcoming you to Discord!</p>"},{"location":"community/InstructLab_DISCORD_GUIDE/#overview","title":"Overview","text":"<p>The InstructLab Discord server can be accessed via this invitation link.</p> <p>Upon joining, you will automatically be assigned the <code>@Labs</code> role and gain access to the <code>#announcements</code> channel, as well as a number of the other default channels. You are welcome and encouraged to join other channels that may exist.</p> <p>All discussions in the InstructLab Discord server are governed by our project code of conduct.</p>"},{"location":"community/InstructLab_DISCORD_GUIDE/#opt-in-channels","title":"Opt-in Channels","text":"<p>To ensure that users are not overwhelmed by notifications, we have a few automated channels as opt-in only. In order to join them, visit the <code>#welcome-and-rules</code> channels and react with the emoji corresponding to the channel you wish to join.</p> <p>Currently, these channels are:</p> <ul> <li><code>#receive-e2e-logs</code>: Pushes updates about E2E runs that take place in our GitHub CI, allowing users to watch for failures.</li> </ul>"},{"location":"community/InstructLab_DISCORD_GUIDE/#moderation-and-reporting-abuse","title":"Moderation and Reporting Abuse","text":"<p>We are an open, welcoming, and inclusive community and expect our members to be kind and respectful in all discourse.</p> <p>We take reports of harassment very seriously and will address any reports of inappropriate behavior as quickly as possible.</p> <p>To learn how to report abuse \u2013 and to whom you will be reporting \u2013 please see our InstructLab Discord Moderation Guide.</p>"},{"location":"community/InstructLab_DISCORD_GUIDE/#having-trouble-joining","title":"Having Trouble Joining?","text":"<p>If you are having trouble joining the InstructLab Discord server, please file an issue in the community repo so we can help you.</p> <p>TODO: Update with email address to get help once these are set up.</p>"},{"location":"community/InstructLab_DISCORD_GUIDE/#private-channels","title":"Private Channels","text":"<p>InstructLab is an open-source project and we value defaulting to open in all of our community communications. However, there are some cases where discussions must happen in private. For the sake of transparency, we are documenting these private channels and their purposes.</p> <ul> <li><code>#code-of-conduct-committee</code> \u2013 Space for the InstructLab Code of Conduct Committee to discuss any reports of harassment or other violations of the project Code of Conduct and how to respond to them.</li> <li><code>#admin</code> \u2013 Space for the InstructLab Server Administrators to confer privately only when necessary. We default to open channels and hold each other accountable to do so.</li> </ul>"},{"location":"community/InstructLab_DISCORD_MODERATION_GUIDE/","title":"InstructLab Discord Moderation Guide","text":"<p>The purpose of this document is to describe how users of the InstructLab Discord server can report abuse within the Discord server and to provide server administrators with an easy-to-use guide for channel moderation.</p>"},{"location":"community/InstructLab_DISCORD_MODERATION_GUIDE/#reporting-abuse","title":"Reporting Abuse","text":"<p>Should any community members using the InstructLab Discord server feel that they have experienced behavior that violates our project Code of Conduct, they are welcome and encouraged to contact the members of the Code of Conduct Committee for help. Mentioning <code>@dog-watch</code> will notify all members of the moderation team so that they can assist you.</p> <p>In the event that you do not receive help within a timely fashion \u2013 and we will do our very best to respond right away \u2013 you can ask for help from the server admins by either joining the <code>#ask-an-admin</code> channel or mentioning <code>@admins</code>. If you feel that it is a personal matter, you can also ping one of the people with the role of <code>#dog-watch</code> directly.</p>"},{"location":"community/InstructLab_DISCORD_MODERATION_GUIDE/#moderation-guide","title":"Moderation Guide","text":"<p>Moderation activities can only be performed by users who are designated as server administrators (i.e. <code>@dog-watch</code>/ <code>@lead-retriever</code>).</p>"},{"location":"community/InstructLab_DISCORD_MODERATION_GUIDE/#server-administrators","title":"Server Administrators","text":"<p>At the time of writing this, anyone with either the <code>@dog-watch</code> or <code>@lead-retriever</code> role is considered to be a moderator or administrator respectively.</p> <p>+ Members of the Code of Conduct Committee</p>"},{"location":"community/InstructLab_DISCORD_MODERATION_GUIDE/#how-we-moderate","title":"How We Moderate","text":""},{"location":"community/InstructLab_DISCORD_MODERATION_GUIDE/#deleting-inappropriate-comments","title":"Deleting Inappropriate Comments","text":"<p>Upon a report of abuse to the Code of Conduct Committee or, alternatively, if needed to the server administrators due to a coverage gap, the appropriate parties will assess the situation.</p> <p>The first step will be to remind individuals to abide by the project Code of Conduct.</p> <p>Inappropriate or offensive messages will be deleted. To delete a message, simply click on the three vertical dots that appear when you hover over a message, or on mobile simply tap and hold on a message, and then click the Delete button:</p> <p></p> <p>Deleting a message shall be done at the sole discretion of the Code of Conduct Committee and/or server administrators.</p>"},{"location":"community/InstructLab_DISCORD_MODERATION_GUIDE/#removing-server-members","title":"Removing Server Members","text":"<p>Admins should consider first removing the offending person's messages from the channel in which the unacceptable behavior occurred and having a conversation with them via Direct Message (DM) to remind them of their responsibilities to abide by the project Code of Conduct as part of their participation in the InstructLab community.</p> <p>If a user is a repeat offender, after being warned, their account can be kicked/banned by an admin, depending on the situation.</p> <p>For more details on moderating a Discord server, please refer to the following guides:</p> <ul> <li>How to kick someone from a Discord server</li> <li>Auto Moderation in Discord</li> </ul>"},{"location":"community/InstructLab_DISCORD_MODERATION_GUIDE/#banning-a-members-account","title":"Banning a Member's Account","text":"<p>If you feel that a community member is violating the InstructLab Code of Conduct, please reach out to the server moderators or the Code of Conduct Committee to receive further assistance.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/","title":"InstructLab Slack Workspace Guide","text":"<p>The purpose of this document is to inform folks about how to join the InstructLab Slack Workspace and document the channels therein. We look forward to meeting everyone and welcoming you on Slack!</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#overview","title":"Overview","text":"<p>The InstructLab Slack workspace resides at https://instruct-lab.slack.com. You must join via this invitation link</p> <p>Upon joining, you will automatically be added to our <code>#announce</code> channel. You are welcome and encouraged to join other channels.</p> <p>All discussions in the InstructLab Slack are governed by our project code of conduct.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#channel-overview","title":"Channel Overview","text":"<ul> <li><code>#dev</code> Cross-project coordination discussion, such as topics that cover both frontend and backend development for InstructLab</li> <li><code>#admin</code> Place to get non-technical help. If you don't know where to go after reading this guide, join this channel for air traffic control.</li> <li><code>#announce</code> Project wide announcements such as releases, reminders about community calls, and celebrating new maintainers. This channel is moderated (only Workspace Administrators can post) and low-traffic.</li> <li><code>#backend</code> Backend work for the InstructLab project, including pipeline for synthetic data generation, training, model evaluation, and publishing.</li> <li><code>#community</code> Place to discuss community matters such as improving the contributor experience, getting help reviewing a presentation about InstructLab you want to give at a meetup, or learning how you can contribute to InstructLab beyond software development.</li> <li><code>#contribhelp</code> General questions about getting started as an InstructLab contributor. This channel is the place to go if you need help with your first pull request.</li> <li><code>#docs</code> Documentation team discussions and questions about documentation.</li> <li><code>#infra</code> Topics related to project infrastructure, such as repo maintenance, planned outages, or who has the keys to the social media accounts.</li> <li><code>#frontend</code> Frontend work for the InstructLab project, including the CLI tool and User Interface</li> <li><code>#social</code> Place to chat and enjoy camaraderie with fellow community members.</li> <li><code>#triage</code> Triage team discussions.</li> <li><code>#users</code> InstructLab users forum for troubleshooting and sharing tips and tricks.</li> <li><code>#github-bot</code> Place to discuss, brainstorm and hack code for InstructLab GitHub Bot.</li> </ul>"},{"location":"community/InstructLab_SLACK_GUIDE/#usings-threaded-replies-in-slack","title":"Usings Threaded Replies in Slack","text":"<p>By default, we use threaded messages in Slack so as to keep all responses to a particular topic grouped together. Please reply to specific messages by replying in thread.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#moderation-and-reporting-abuse","title":"Moderation and Reporting Abuse","text":"<p>We are an open, welcoming, and inclusive community and expect our members to be kind and respectful in all discourse.</p> <p>We take reports of harassment very seriously and will action any reports of inappropriate behavior as quickly as possible.</p> <p>To learn how to report abuse - and to whom you will be reporting - please see our InstructLab slack Moderation Guide.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#having-trouble-joining","title":"Having Trouble Joining?","text":"<p>If you are having trouble joining the InstructLab Slack, please file an issue in the community repo so we can help you.</p> <p>TODO: Update with email address to get help once these are set up.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#private-channels","title":"Private Channels","text":"<p>InstructLab is an open source project and we value defaulting to open in all of our community communications. There are some cases where discussions must happen in private. For the sake of transparency, we are documenting these private channels and what they are used for.</p> <ul> <li><code>#code-of-conduct-committee</code> Space for the InstructLab Code of Conduct Committee to discuss any reports of harassment or other violations of the project Code of Conduct and how to respond to them.</li> <li><code>#mods</code> Space for the InstructLab Workspace Administrators to confer privately only when necessary. We default to open and hold each other accountable to do so.</li> </ul>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/","title":"InstructLab Slack Moderation Guide","text":"<p>The purpose of this document is both describe how users of the InstructLab's Slack workspace can report abuse in the Slack workspace and to provide space administrators with an easy to use how to guide for channel moderation.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#reporting-abuse","title":"Reporting Abuse","text":"<p>Should any community members using the InstructLab Slack workspace feel that they have experienced behavior that violates our project Code of Conduct, they are welcome and encouraged to contact the members of the Code of Conduct Committee for help. Mentioning <code>@cocc</code> will page all members of the committee so that they can assist you.</p> <p>In the event that you do not receive help within a timely fashion - and we will do our very best to respond right away - you can ask for help from the workspace admins by either joining channel <code>#admin</code> or mentioning <code>@admins</code>.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#moderation-guide","title":"Moderation Guide","text":"<p>Moderation activities can only be performed by users who are designated as workspace administrators.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#workspace-administrators","title":"Workspace Administrators","text":"<p>At time of writing, our workspace administrators/moderators are as follows:</p> <ul> <li>Aakanksha Duggal</li> <li>Ali Maredia</li> <li>Alina Ryan</li> <li>Cara Delia +</li> <li>Carol Chen +</li> <li>Charlie Doern</li> <li>Dan McPherson</li> <li>Jaideep Rao</li> <li>James Kunstle</li> <li>Jason Greene</li> <li>Jeremy Eder</li> <li>JJ Asghar +</li> <li>Joe Sepi +</li> <li>Kelly Brown</li> <li>Leslie Hawthorn +</li> <li>Mo McElaney +</li> <li>M\u00e1ir\u00edn Duffy</li> <li>Nathan Weinberg</li> <li>Oleg Silkin</li> <li>Russell Bryant</li> </ul> <p>+ Members of the Code of Conduct Committee</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#how-we-moderate","title":"How We Moderate","text":""},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#deleting-inappropriate-comments","title":"Deleting Inappropriate Comments","text":"<p>Upon report of abuse to the Code of Conduct Committee or, alternatively if needed to the workspace administrators due to a coverage gap, the appropriate parties will assess the situation.</p> <p>The first step will be to remind folks to abide by the project Code of Conduct.</p> <p>Inappropriate or offensive messages will be deleted.</p> <p>Deleting a message shall be done at the sole discretion of the Code of Conduct Committee and/or workspace administrators.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#how-to-delete-a-message","title":"How to delete a message","text":""},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#removing-workspace-members","title":"Removing Workspace Members","text":"<p>Admins should consider first removing the offending person from the channel in which the unacceptable behavior occurred and having a conversation with them as a DM to remind them of their responsibilities to abide by the project code of conduct as part of their participation in the InstructLab community.</p> <p>If a user is a repeat offender, after being warned, their account can be deactivated by an admin.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#removing-someone-from-a-channel","title":"Removing someone from a channel","text":"<ul> <li>By default, Workspace Owners and Admins can remove people from public channels, and members can remove people from private channels.</li> <li>Anyone can be removed from a channel by those with permission.</li> <li>All members and guests need to be added back to a private channel to rejoin it, and guests also need to be added back to a public channel to rejoin it.</li> <li>It's not possible to remove people from the #announce channel. However, posting in this channel is restricted to workspace administrators by default.</li> </ul>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#deactivating-a-members-account","title":"Deactivating a member's account","text":""},{"location":"getting-started/download_models/","title":"\ud83d\udce5 Download the model","text":"<p>1) Run the ilab model download command to download a compact pre-trained version of the <code>granite-7b-lab-GGUF</code>, <code>merlinite-7b-lab-GGUF</code>, and <code>Mistral-7B-Instruct-v0.2-GGUF</code> models (~4.4G each) from HuggingFace.</p> <pre><code>ilab model download\n</code></pre> <p><code>ilab model download</code> downloads a compact pre-trained version of the model (~4.4G) from HuggingFace:</p> <p>Example output of the models downloading</p> <pre><code>Downloading model from Hugging Face:\n    Model: instructlab/granite-7b-lab-GGUF@main\n    Destination: /Users/&lt;user&gt;/.cache/instructlab/models\nDownloading model from Hugging Face:\n    Model: instructlab/merlinite-7b-lab-GGUF@main\n    Destination: /Users/&lt;user&gt;/.cache/instructlab/models\nDownloading model from Hugging Face:\n    Model: TheBloke/Mistral-7B-Instruct-v0.2-GGUF@main\n    Destination: /Users/&lt;user&gt;/.cache/instructlab/models\n\nTheBloke/Mistral-7B-Instruct-v0.2-GGUF requires a HF Token to be set.\nPlease use '--hf-token' or 'export HF_TOKEN' to download all necessary models.\n</code></pre> <p>a) You may be prompted to use your Hugging Face token to download the <code>Mistral-7B-Instruct-v0.2-GGUF</code> model.</p> <pre><code>ilab model download --hf-token &lt;your-huggingface-token&gt;\n</code></pre> <p>Note</p> <p>\u23f3 This command can take few minutes to run, or it can finish immediately. The speed depends on your internet connection and whether or not the model is cached. If you have issues connecting to Hugging Face, refer to the Hugging Face discussion forum for more details.</p>"},{"location":"getting-started/download_models/#downloading-an-entire-hugging-face-repository-safetensors-model","title":"Downloading an entire Hugging Face repository (Safetensors Model)","text":"<p>1) Specify repository, and a Hugging Face token if necessary. For example:</p> <pre><code>ilab model download --repository instructlab/granite-7b-lab-GGUF --filename granite-7b-lab-Q4_K_M.gguf --hf-token &lt;your-huggingface-token&gt;\n</code></pre> <p>These types of models are useful for GPU-enabled systems or anyone looking to serve a model using vLLM. InstructLab provides Safetensor versions of our Granite models on HuggingFace.</p>"},{"location":"getting-started/download_models/#listing-downloaded-models","title":"Listing downloaded models","text":"<p>All downloaded models can be seen with the <code>ilab model list</code> command.</p> <pre><code>ilab model list\n</code></pre> <p>Example output of <code>ilab model list</code> after <code>ilab model download</code></p> <pre><code>(venv) $ ilab model list\n+-------------------------------------+---------------------+--------+\n| Model Name                          | Last Modified       | Size   |\n+-------------------------------------+---------------------+--------+\n| granite-7b-lab-Q4_K_M.gguf          | 2024-08-01 15:05:48 | 4.1 GB |\n| merlinite-7b-lab-Q4_K_M.gguf        | 2024-08-01 15:05:48 | 4.1 GB |\n| mistral-7b-instruct-v0.2.Q4_K_M.gguf| 2024-08-01 15:05:48 | 4.1 GB |\n+-------------------------------------+---------------------+--------+\n</code></pre>"},{"location":"getting-started/initilize_ilab/","title":"\ud83c\udfd7\ufe0f Initialize <code>ilab</code>","text":"<p>1) Initialize <code>ilab</code> by running the following command:</p> <pre><code>ilab config init\n</code></pre> <p>2) When prompted, clone the <code>https://github.com/instructlab/taxonomy.git</code> repository into the current directory by typing enter</p> <p>Optional: If you want to point to an existing local clone of the <code>taxonomy</code> repository, you can pass the path interactively or alternatively with the <code>--taxonomy-path</code> flag.</p> <p><code>ilab</code> will use the default configuration file unless otherwise specified. You can override this behavior with the <code>--config</code> parameter for any <code>ilab</code> command.</p> <p>3) When prompted, provide the path to your default model. Otherwise, the default of a quantized Merlinite model is used.</p> <p>Example output of steps 1 - 3 <pre><code>----------------------------------------------------\n         Welcome to the InstructLab CLI\nThis guide will help you to setup your environment\n----------------------------------------------------\n\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [/Users/&lt;user&gt;/.local/share/instructlab/taxonomy]:\nPath to your model [/Users/&lt;user&gt;/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:\n</code></pre></p> <p>You can download this model with <code>ilab model download</code> command as well.</p> <p>4) The InstructLab CLI auto-detects your hardware and select the exact system profile that matches your machine. System profiles populate the <code>config.yaml</code> file with the proper parameter values based on your detected GPU types and avaiible vRAM.</p> <p>Example output of profile auto-detection</p> <pre><code>Generating config file and profiles:\n/home/user/.config/instructlab/config.yaml\n/home/user/.local/share/instructlab/internal/train_configuration/profiles\n\nWe have detected the AMD CPU profile as an exact match for your system.\n\n--------------------------------------------\n   Initialization completed successfully!\n   You're ready to start using `ilab`. Enjoy!\n--------------------------------------------\n</code></pre> <p>5) If there is not an exact match for your system, you can manually select a system profile when prompted. There are various flags you can utilize with individual <code>ilab</code> commands that allow you to utilize your GPU if applicable.</p> <p>Example output of selecting a system profile</p> <pre><code>Please choose a system profile to use.\nSystem profiles apply to all parts of the config file and set hardware specific defaults for each command.\nFirst, please select the hardware vendor your system falls into\n[1] APPLE\n[2] INTEL\n[3] AMD\n[4] NVIDIA\nEnter the number of your choice [0]: 1\nYou selected: APPLE\nNext, please select the specific hardware configuration that most closely matches your system.\n[0] No system profile\n[1] APPLE M1 ULTRA\n[2] APPLE M1 MAX\n[3] APPLE M2 MAX\n[4] APPLE M2 ULTRA\n[5] APPLE M2 PRO\n[6] APPLE M2\n[7] APPLE M3 MAX\n[8] APPLE M3 PRO\n[9] APPLE M3\nEnter the number of your choice [hit enter for hardware defaults] [0]: 8\nYou selected: /Users/kellybrown/.local/share/instructlab/internal/system_profiles/apple/m3/m3_pro.yaml\n\n--------------------------------------------\n   Initialization completed successfully!\nYou're ready to start using `ilab`. Enjoy!\n--------------------------------------------\n</code></pre> <p>The GPU profiles are listed by GPU type and number of GPUs present. If you happen to have a GPU configuration with a similar amount of vRAM as any of the above profiles, feel free to try them out!</p>"},{"location":"getting-started/initilize_ilab/#ilab-directory-layout-after-initializing-your-system","title":"<code>ilab</code> directory layout after initializing your system","text":"<p>After running <code>ilab config init</code> your directories will look like the following on a Linux system:</p> <pre><code>\u251c\u2500 ~/.cache/instructlab/models/ (1)\n\u251c\u2500 ~/.local/share/instructlab/datasets (2)\n\u251c\u2500 ~/.local/share/instructlab/taxonomy (3)\n\u251c\u2500 ~/.local/share/instructlab/checkpoints (4)\n</code></pre> <p>1) <code>~/.cache/instructlab/models/</code>: Contains all downloaded large language models, including the saved output of ones you generate with ilab.</p> <p>2) <code>~/.local/share/instructlab/datasets/</code>: Contains data output from the SDG phase, built on modifications to the taxonomy repository.</p> <p>3) <code>~/.local/share/instructlab/taxonomy/</code>: Contains the skill and knowledge data.</p> <p>4) <code>~/.local/share/instructlab/checkpoints/</code>: Contains the output of the training process</p>"},{"location":"getting-started/linux_amd/","title":"Install InstructLab on Linux AMD","text":""},{"location":"getting-started/linux_amd/#tldr","title":"tl;dr","text":"<p>Note</p> <p>These steps will pull down a premade <code>qna.yaml</code> so you can do a local build. Skip the <code>wget</code>, <code>mv</code>, and <code>ilab taxonomy diff</code> if you don't want to do this.</p> <pre><code>python3.11 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on \\\n   -DAMDGPU_TARGETS=all \\\n   -DCMAKE_C_COMPILER=/opt/rocm/llvm/bin/clang \\\n   -DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++ \\\n   -DCMAKE_PREFIX_PATH=/opt/rocm \\\n   -DLLAMA_NATIVE=off\" \\\n   pip install 'instructlab[rocm]' \\\n   --extra-index-url https://download.pytorch.org/whl/rocm6.0\nwhich ilab\nilab config init\ncd ~/.local/share/instructlab\nmkdir -p taxonomy/knowledge/astronomy/constellations/Phoenix/\nwget https://raw.githubusercontent.com/instructlab/taxonomy/26b3fe21ccbb95adc06fe8ce76c7c18559e8dd05/knowledge/science/astronomy/constellations/phoenix/qna.yaml\nmv qna.yaml taxonomy/knowledge/astronomy/constellations/Phoenix/\nilab taxonomy diff\nilab data generate\nilab model train\nilab model convert --model-dir checkpoints/instructlab-granite-7b-lab-mlx-q\nilab model serve --model-path instructlab-granite-7b-lab-trained/instructlab-granite-7b-lab-Q4_K_M.gguf\n</code></pre>"},{"location":"getting-started/linux_amd/#installing-ilab","title":"Installing <code>ilab</code>","text":"<p>The following steps in this document use Python venv for virtual environments. However, if you use another tool such as pyenv or Conda Miniforge for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in <code>venv</code>.</p> <p>Note</p> <p>\u23f3 <code>pip install</code> may take some time, depending on your internet connection. In case installation fails with error <code>unsupported instruction `vpdpbusd'</code>, append <code>-C cmake.args=\"-DLLAMA_NATIVE=off\"</code> to <code>pip install</code> command.</p> <p>1) Install with AMD ROCm</p> <pre><code>python3 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\npip install 'instructlab[rocm]' \\\n--extra-index-url https://download.pytorch.org/whl/rocm6.0 \\\n-C cmake.args=\"-DLLAMA_HIPBLAS=on\" \\\n-C cmake.args=\"-DAMDGPU_TARGETS=all\" \\\n-C cmake.args=\"-DCMAKE_C_COMPILER=/opt/rocm/llvm/bin/clang\" \\\n-C cmake.args=\"-DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++\" \\\n-C cmake.args=\"-DCMAKE_PREFIX_PATH=/opt/rocm\" \\\n-C cmake.args=\"-DLLAMA_NATIVE=off\"\n</code></pre> <p>On Fedora 40+, use <code>-DCMAKE_C_COMPILER=clang-17</code> and <code>-DCMAKE_CXX_COMPILER=clang++-17.</code></p> <p>2) From your <code>venv</code> environment, verify <code>ilab</code> is installed correctly, by running the <code>ilab</code> command.</p> <pre><code>ilab\n</code></pre> <p>Example output of the <code>ilab</code> command</p> <pre><code>(venv) $ ilab\nUsage: ilab [OPTIONS] COMMAND [ARGS]...\n\nCLI for interacting with InstructLab.\n\nIf this is your first time running ilab, it's best to start with `ilab\nconfig init` to create the environment.\n\nOptions:\n--config PATH  Path to a configuration file.  [default:\n                    /Users/kellybrown/.config/instructlab/config.yaml]\n-v, --verbose  Enable debug logging (repeat for even more verbosity)\n--version      Show the version and exit.\n--help         Show this message and exit.\n\nCommands:\nconfig    Command Group for Interacting with the Config of InstructLab.\ndata      Command Group for Interacting with the Data generated by...\nmodel     Command Group for Interacting with the Models in InstructLab.\nsystem    Command group for all system-related command calls\ntaxonomy  Command Group for Interacting with the Taxonomy of InstructLab.\n\nAliases:\nchat      model chat\ngenerate  data generate\nserve     model serve\ntrain     model train\n</code></pre> <p>Important</p> <p>Every <code>ilab</code> command needs to be run from within your Python virtual environment. You can enter the Python environment by running the <code>source venv/bin/activate</code> command.</p>"},{"location":"getting-started/linux_amd/#optional-enabling-tab-completion-for-the-ilab-command","title":"Optional: Enabling tab completion for the <code>ilab</code> command","text":""},{"location":"getting-started/linux_amd/#bash-version-44-or-newer","title":"Bash (version 4.4 or newer)","text":"<p>Enable tab completion in <code>bash</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=bash_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=bash_source ilab &gt; ~/.ilab-complete.bash\necho \". ~/.ilab-complete.bash\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"getting-started/linux_amd/#zsh","title":"Zsh","text":"<p>Enable tab completion in <code>zsh</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=zsh_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.zshrc</code>:</p> <pre><code>_ILAB_COMPLETE=zsh_source ilab &gt; ~/.ilab-complete.zsh\necho \". ~/.ilab-complete.zsh\" &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"getting-started/linux_amd/#fish","title":"Fish","text":"<p>Enable tab completion in <code>fish</code> with the following command:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab | source\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab &gt; ~/.config/fish/completions/ilab.fish\n</code></pre>"},{"location":"getting-started/linux_amd/#next-steps","title":"Next Steps","text":"<p>Now that you have InstructLab installed, the next step is to initialize your environment.</p> <p>Initialize InstructLab</p>"},{"location":"getting-started/linux_nvidia/","title":"Install InstructLab on Linux NVIDIA","text":""},{"location":"getting-started/linux_nvidia/#tldr","title":"tl;dr","text":"<p>Note</p> <p>These steps will pull down a premade <code>qna.yaml</code> so you can do a local build. Skip the <code>wget</code>, <code>mv</code>, and <code>ilab taxonomy diff</code> if you don't want to do this.</p> <pre><code>python3.11 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\npip install 'instructlab[cuda]' \\\n   -C cmake.args=\"-DLLAMA_CUDA=on\" \\\n   -C cmake.args=\"-DLLAMA_NATIVE=off\"\npip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01\nwhich ilab\nilab config init\ncd ~/.local/share/instructlab\nmkdir -p taxonomy/knowledge/astronomy/constellations/Phoenix/\nwget https://raw.githubusercontent.com/instructlab/taxonomy/26b3fe21ccbb95adc06fe8ce76c7c18559e8dd05/knowledge/science/astronomy/constellations/phoenix/qna.yaml\nmv qna.yaml taxonomy/knowledge/astronomy/constellations/Phoenix/\nilab taxonomy diff\nilab data generate\nilab model train\nilab model convert --model-dir checkpoints/instructlab-granite-7b-lab-mlx-q\nilab model serve --model-path instructlab-granite-7b-lab-trained/instructlab-granite-7b-lab-Q4_K_M.gguf\n</code></pre>"},{"location":"getting-started/linux_nvidia/#installing-ilab","title":"Installing <code>ilab</code>","text":"<p>The following steps in this document use Python venv for virtual environments. However, if you use another tool such as pyenv or Conda Miniforge for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in <code>venv</code>.</p> <p>Note</p> <p>\u23f3 <code>pip install</code> may take some time, depending on your internet connection. In case installation fails with error <code>unsupported instruction `vpdpbusd'</code>, append <code>-C cmake.args=\"-DLLAMA_NATIVE=off\"</code> to <code>pip install</code> command.</p> <p>1) Install with Nvidia CUDA</p> <p>For the best CUDA experience, installing vLLM is necessary to serve Safetensors format models.</p> <pre><code>python3 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\npip install 'instructlab[cuda]' \\\n-C cmake.args=\"-DLLAMA_CUDA=on\" \\\n-C cmake.args=\"-DLLAMA_NATIVE=off\"\npip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01\n</code></pre> <p>2) From your <code>venv</code> environment, verify <code>ilab</code> is installed correctly, by running the <code>ilab</code> command.</p> <pre><code>ilab\n</code></pre> <p>Example output of the <code>ilab</code> command</p> <pre><code>(venv) $ ilab\nUsage: ilab [OPTIONS] COMMAND [ARGS]...\n\nCLI for interacting with InstructLab.\n\nIf this is your first time running ilab, it's best to start with `ilab\nconfig init` to create the environment.\n\nOptions:\n--config PATH  Path to a configuration file.  [default:\n                    /Users/kellybrown/.config/instructlab/config.yaml]\n-v, --verbose  Enable debug logging (repeat for even more verbosity)\n--version      Show the version and exit.\n--help         Show this message and exit.\n\nCommands:\nconfig    Command Group for Interacting with the Config of InstructLab.\ndata      Command Group for Interacting with the Data generated by...\nmodel     Command Group for Interacting with the Models in InstructLab.\nsystem    Command group for all system-related command calls\ntaxonomy  Command Group for Interacting with the Taxonomy of InstructLab.\n\nAliases:\nchat      model chat\ngenerate  data generate\nserve     model serve\ntrain     model train\n</code></pre> <p>Important</p> <p>Every <code>ilab</code> command needs to be run from within your Python virtual environment. You can enter the Python environment by running the <code>source venv/bin/activate</code> command.</p>"},{"location":"getting-started/linux_nvidia/#optional-enabling-tab-completion-for-the-ilab-command","title":"Optional: Enabling tab completion for the <code>ilab</code> command","text":""},{"location":"getting-started/linux_nvidia/#bash-version-44-or-newer","title":"Bash (version 4.4 or newer)","text":"<p>Enable tab completion in <code>bash</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=bash_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=bash_source ilab &gt; ~/.ilab-complete.bash\necho \". ~/.ilab-complete.bash\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"getting-started/linux_nvidia/#zsh","title":"Zsh","text":"<p>Enable tab completion in <code>zsh</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=zsh_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.zshrc</code>:</p> <pre><code>_ILAB_COMPLETE=zsh_source ilab &gt; ~/.ilab-complete.zsh\necho \". ~/.ilab-complete.zsh\" &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"getting-started/linux_nvidia/#fish","title":"Fish","text":"<p>Enable tab completion in <code>fish</code> with the following command:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab | source\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab &gt; ~/.config/fish/completions/ilab.fish\n</code></pre>"},{"location":"getting-started/linux_nvidia/#next-steps","title":"Next Steps","text":"<p>Now that you have InstructLab installed, the next step is to initialize your environment.</p> <p>Initialize InstructLab</p>"},{"location":"getting-started/mac_metal/","title":"Install InstructLab on Mac Metal","text":""},{"location":"getting-started/mac_metal/#tldr","title":"tl;dr","text":"<p>Note</p> <p>These steps will pull down a premade <code>qna.yaml</code> so you can do a local build. Skip the <code>wget</code>, <code>mv</code>, and <code>ilab taxonomy diff</code> if you don't want to do this.</p> <pre><code>python3.11 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip install 'instructlab[mps]'\nwhich ilab\nilab config init\ncd ~/Library/Application\\ Support/instructlab/\nmkdir -p taxonomy/knowledge/astronomy/constellations/Phoenix/\nwget https://raw.githubusercontent.com/instructlab/taxonomy/26b3fe21ccbb95adc06fe8ce76c7c18559e8dd05/knowledge/science/astronomy/constellations/phoenix/qna.yaml\nmv qna.yaml taxonomy/knowledge/astronomy/constellations/Phoenix/\nilab taxonomy diff\nilab data generate\nilab model train\nilab model convert --model-dir checkpoints/instructlab-granite-7b-lab-mlx-q\nilab model serve --model-path instructlab-granite-7b-lab-trained/instructlab-granite-7b-lab-Q4_K_M.gguf\n</code></pre>"},{"location":"getting-started/mac_metal/#installing-ilab","title":"Installing <code>ilab</code>","text":"<p>The following steps in this document use Python venv for virtual environments. However, if you use another tool such as pyenv or Conda Miniforge for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in <code>venv</code>.</p> <p>Note</p> <p>\u23f3 <code>pip install</code> may take some time, depending on your internet connection. In case installation fails with error <code>unsupported instruction `vpdpbusd'</code>, append <code>-C cmake.args=\"-DLLAMA_NATIVE=off\"</code> to the <code>pip install</code> command.</p> <p>1) Install with Apple Metal on M1/M2/M3 Macs:</p> <pre><code>python3.11 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\npip install instructlab\n</code></pre> <p>Note</p> <p>Make sure your system Python build is <code>Mach-O 64-bit executable arm64</code> by using <code>file -b $(command -v python)</code>, or if your system is setup with pyenv by using the <code>file -b $(pyenv which python)</code> command.</p> <p>2) From your <code>venv</code> environment, verify <code>ilab</code> is installed correctly, by running the <code>ilab</code> command.</p> <pre><code>ilab\n</code></pre> <p>Example output of the <code>ilab</code> command</p> <pre><code>(venv) $ ilab\nUsage: ilab [OPTIONS] COMMAND [ARGS]...\n\nCLI for interacting with InstructLab.\n\nIf this is your first time running ilab, it's best to start with `ilab\nconfig init` to create the environment.\n\nOptions:\n--config PATH  Path to a configuration file.  [default:\n                    /Users/kellybrown/.config/instructlab/config.yaml]\n-v, --verbose  Enable debug logging (repeat for even more verbosity)\n--version      Show the version and exit.\n--help         Show this message and exit.\n\nCommands:\nconfig    Command Group for Interacting with the Config of InstructLab.\ndata      Command Group for Interacting with the Data generated by...\nmodel     Command Group for Interacting with the Models in InstructLab.\nsystem    Command group for all system-related command calls\ntaxonomy  Command Group for Interacting with the Taxonomy of InstructLab.\n\nAliases:\nchat      model chat\ngenerate  data generate\nserve     model serve\ntrain     model train\n</code></pre> <p>Important</p> <p>Every <code>ilab</code> command needs to be run from within your Python virtual environment. You can enter the Python environment by running the <code>source venv/bin/activate</code> command.</p>"},{"location":"getting-started/mac_metal/#optional-enabling-tab-completion-for-the-ilab-command","title":"Optional: Enabling tab completion for the <code>ilab</code> command","text":""},{"location":"getting-started/mac_metal/#bash-version-44-or-newer","title":"Bash (version 4.4 or newer)","text":"<p>Enable tab completion in <code>bash</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=bash_source ilab)\"\n</code></pre> <p>To enable tab completion automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=bash_source ilab &gt; ~/.ilab-complete.bash\necho \". ~/.ilab-complete.bash\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"getting-started/mac_metal/#zsh","title":"Zsh","text":"<p>Enable tab completion in <code>zsh</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=zsh_source ilab)\"\n</code></pre> <p>Note</p> <p>If you receive an error message about <code>compdef</code>, add the following lines to your <code>~/.zshrc</code> and reload your configuration: <pre><code>autoload -Uz compinit\ncompinit\n</code></pre></p> <p>To enable tab completion automatically every time you open a new shell, you can save the completion script and source it from <code>~/.zshrc</code>:</p> <pre><code>_ILAB_COMPLETE=zsh_source ilab &gt; ~/.ilab-complete.zsh\necho \". ~/.ilab-complete.zsh\" &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"getting-started/mac_metal/#fish","title":"Fish","text":"<p>Enable tab completion in <code>fish</code> with the following command:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab | source\n</code></pre> <p>To enable tab completion automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab &gt; ~/.config/fish/completions/ilab.fish\n</code></pre>"},{"location":"getting-started/mac_metal/#next-steps","title":"Next Steps","text":"<p>Now that you have InstructLab installed, the next step is to initialize your environment.</p> <p>Initialize InstructLab</p>"},{"location":"getting-started/serve_and_chat/","title":"\ud83c\udf74 Serving the model","text":"<p>Serve the model by running the following command:</p> <pre><code>ilab model serve\n</code></pre> <p>Serve a non-default model with the following command:</p> <pre><code>ilab model serve --model-path models/granite-7b-instruct.GGUF\n</code></pre> <p>Example output of a model that is served and ready</p> <pre><code>(venv) $ ilab model serve\nINFO 2024-03-02 02:21:11,352 lab.py:201 Using model 'models/ggml-merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.\nStarting server process\nAfter application startup complete see http://127.0.0.1:8000/docs for API.\nPress CTRL+C to shut down the server.\n</code></pre> <p>Note</p> <p>If multiple <code>ilab</code> clients try to connect to the same InstructLab server at the same time, the 1<sup>st</sup> will connect to the server while the others will start their own temporary server. This will require additional resources on the host machine.</p> <p>Serve a non-default Safetensors model (e.g. granite-7b-lab). NOTE: this requires a GPU.</p> <p>a. Ensure vllm is installed:</p> <pre><code>pip show vllm\n</code></pre> <p>b. If it is not, please run:</p> <pre><code>pip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01\n</code></pre> <pre><code>ilab model serve --model-path ~/.cache/instructlab/models/instructlab/granite-7b-lab\n</code></pre>"},{"location":"getting-started/serve_and_chat/#chat-with-the-model-optional","title":"\ud83d\udce3 Chat with the model (Optional)","text":"<p>Because you're serving the model in one terminal window, you will have to create a new window and re-activate your Python virtual environment to run <code>ilab model chat</code> command:</p> <pre><code>source venv/bin/activate\nilab model chat\n</code></pre> <p>Chat with a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):</p> <pre><code>source venv/bin/activate\nilab model chat --model models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>Please note that usage of <code>--model</code> necessitates that the existing server has that model. If not, you must exit the server. <code>--model</code> in <code>ilab model chat</code> has the ability to start a server on your behalf with the specified model if one is not already running on the port.</p> <p>Before you start adding new skills and knowledge to your model, you can check its baseline performance by asking it a question such as <code>what is the capital of Canada?</code>.</p> <p>Note</p> <p>The model needs to be trained with the generated synthetic data to use any new skills or knowledge</p> <pre><code>(venv) $ ilab model chat\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 system \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Welcome to InstructLab Chat w/ GGML-MERLINITE-7B-lab-Q4_K_M (type /h for help)                                                                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n&gt;&gt;&gt; what is the capital of Canada                                                                                                                                                                                                 [S][default]\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ggml-merlinite-7b-lab-Q4_K_M \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The capital city of Canada is Ottawa. It is located in the province of Ontario, on the southern banks of the Ottawa River in the eastern portion of southern Ontario. The city serves as the political center for Canada, as it is home to \u2502\n\u2502 Parliament Hill, which houses the House of Commons, Senate, Supreme Court, and Cabinet of Canada. Ottawa has a rich history and cultural significance, making it an essential part of Canada's identity.                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 elapsed 12.008 seconds \u2500\u256f\n</code></pre>"},{"location":"resources/CONTRIBUTORS/","title":"Contributors","text":"<p>Let's celebrate our friends who help out with this site here!</p> <p> <ul> <li>Laura Santamaria</li> </ul>"},{"location":"resources/MKDOCS/","title":"mkdocs examples","text":"<p>This page includes a few neat tricks that you can do with <code>mkdocs</code>. For a complete list of examples visit the mkdocs documentation.</p>"},{"location":"resources/MKDOCS/#code","title":"Code","text":"<pre><code>print(\"hello world!\")\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-line-numbers","title":"Code with line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-highlights","title":"Code with highlights","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-tabs","title":"Code with tabs","text":"Tab HeaderAnother Tab Header <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"resources/MKDOCS/#more-tabs","title":"More tabs","text":"WindowsMacOSLinux <p>If on windows download the <code>Win32.zip</code> file and install it.</p> <p>Run <code>brew install foo</code>.</p> <p>Run <code>apt-get install foo</code>.</p>"},{"location":"resources/MKDOCS/#checklists","title":"Checklists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> In hac habitasse platea dictumst</li> </ul>"},{"location":"resources/MKDOCS/#add-a-button","title":"Add a button","text":"<p>Launch the lab</p> <p>Visit IBM Developer</p> <p>Sign up! </p>"},{"location":"resources/MKDOCS/#call-outs","title":"Call outs","text":"<p>Tip</p> <p>You can use <code>note</code>, <code>abstract</code>, <code>info</code>, <code>tip</code>, <code>success</code>, <code>question</code> <code>warning</code>, <code>failure</code>, <code>danger</code>, <code>bug</code>, <code>quote</code> or <code>example</code>.</p> <p>Note</p> <p>A note.</p> <p>Abstract</p> <p>An abstract.</p> <p>Info</p> <p>Some info.</p> <p>Success</p> <p>A success.</p> <p>Question</p> <p>A question.</p> <p>Warning</p> <p>A warning.</p> <p>Danger</p> <p>A danger.</p> <p>Example</p> <p>A example.</p> <p>Bug</p> <p>A bug.</p>"},{"location":"resources/MKDOCS/#call-outs-with-code","title":"Call outs with code","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.</p>"},{"location":"resources/MKDOCS/#formatting","title":"Formatting","text":"<p>In addition to the usual italics, and bold there is now support for:</p> <ul> <li>highlighted</li> <li>underlined</li> <li>strike-through</li> </ul>"},{"location":"resources/MKDOCS/#tables","title":"Tables","text":"OS or Application Username Password Windows VM <code>Administrator</code> <code>foo</code> Linux VM <code>root</code> <code>bar</code>"},{"location":"resources/MKDOCS/#emojis","title":"Emojis","text":"<p>Yes, these work.  </p>"},{"location":"resources/MKDOCS/#images","title":"Images","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/MKDOCS/#right-align-image","title":"right align image","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/RESOURCES/","title":"Additional resources","text":"<ul> <li>TechXchange 2024 Open Source AI workshop</li> </ul>"},{"location":"taxonomy/","title":"About the InstructLab Taxonomy","text":"<p>InstructLab \ud83d\udc36 uses a novel synthetic data-based alignment tuning method for Large Language Models (LLMs.) The \"lab\" in InstructLab \ud83d\udc36 stands for Large-Scale Alignment for ChatBots<sup>1</sup>.</p> <p>The LAB method is driven by taxonomies, which are largely created manually and with care. Taxonomies allow you to create models tuned with your data (enhanced via synthetic data generation) using the LAB \ud83d\udc36 method.</p> <p>The instructlab/taxonomy repository contains a comprehensive taxonomy tree that we use to build the overall model. You are welcome to contribute to it!</p>"},{"location":"taxonomy/#skills-and-knowledge","title":"Skills and Knowledge","text":"<p>In a taxonomy, we have \"skills,\" or performative actions, and \"knowledge,\" or based more on answering questions that involve facts, data, or references. Learn more about knowledge in the knowledge guide, and learn more about skills in the skills guide.</p> <p></p>"},{"location":"taxonomy/#taxonomy-trees","title":"Taxonomy trees","text":"<p>The overall structure of a taxonomy tree for InstructLab is a cascading file structure. The top-level directory is called the \"root\" of the tree. The resulting subdirectories are \"branches\" of the tree. Along a branch are more nested directories until we reach the final directory. That final directory along a branch is called the \"leaf node,\" and it contains a YAML file named <code>qna.yaml</code> and, in the case of the upstream taxonomy, an <code>attribution.txt</code> file that holds citation information. The only required file in a leaf node directory for the InstructLab process is the <code>qna.yaml</code> file. You can learn more about the structure of the <code>qna.yaml</code> file in the knowledge and skill guides.</p>"},{"location":"taxonomy/#tree-structure","title":"Tree structure","text":"<p>The tree structure is important because it is used by the synthetic data generation process to relate chunks of knowledge together. Without it, training would not be as accurate.</p> <p>The root of a taxonomy tree does not need to be the root of all knowledge. The only requirements for a taxonomy tree are that</p> <ul> <li>knowledge is within a <code>knowledge</code> directory</li> <li>ungrounded compositional skills are within a <code>compositional_skills</code> directory</li> <li>grounded compositional skills are within a <code>compositional_skills/grounded/</code> directory structure</li> <li>the <code>knowledge</code> and <code>compositional_skills</code> directories are within a single root directory</li> </ul> <p>This helps the synthetic data generation process and training process parse the files in the right order.</p>"},{"location":"taxonomy/#sorting-knowledge-and-skills","title":"Sorting knowledge and skills","text":"<p>For each piece of knowledge, you should have a single <code>qna.yaml</code> file. For example, if you are fine-tuning a model to talk about cloud formations, you would make a leaf node directory for each type of cloud formation (e.g., <code>cumulonimbus</code>, <code>cirrus</code>, <code>cumulus</code>, <code>incus</code>, <code>lenticular</code>) and then have a <code>qna.yaml</code> file dedicated to each formation with a document for each one. You would not lump all the cloud formations together into one YAML file with five or six documents as sources as the synthetic data generation process would not group the resulting data based on cloud formation, thereby making the resulting model possibly provide information about one cloud formation when asked about another.</p> <p>The same thought applies to skills. A single skill should be in one leaf node directory, even if it is related to another skill. Do not create a <code>qna.yaml</code> file that has multiple skills in it.</p>"},{"location":"taxonomy/#the-upstream-taxonomy","title":"The upstream taxonomy","text":"<p>We have a comprehensive taxonomy tree in the InstructLab project used to build the community model. We welcome contributions to that taxonomy. Here's more information on how that tree is structured.</p>"},{"location":"taxonomy/#upstream-tree-domain-classification","title":"Upstream tree domain classification","text":"<p>In general, we use the Dewey Decimal Classification (DDC) System to determine our domains (and subdomains) in the overall taxonomy. This DDC SUMMARIES document is a great resource for determining where a topic might be classified.</p> <p>Generally, expect that there may be several layers you need to add to the taxonomy tree when adding knowledge or skills. For example, if you were to write a knowledge submission about a constellation, you would need to add directories to the tree, primarily <code>astronomy/constellations/</code> inside the <code>knowledge/science/</code> branch, before you would add your submission. These are \"branches\", with your submission as a \"leaf node.\" The taxonomy is very much like a tree.</p> <p>If you are unsure where to put your knowledge or compositional skill, create a folder in the <code>miscellaneous_unknown</code> folder under the <code>knowledge</code> or <code>compositional_skills</code> folders.</p>"},{"location":"taxonomy/#upstream-taxonomy-tree-layout","title":"Upstream taxonomy tree layout","text":"<p>The taxonomy tree is organized in a cascading directory structure. At the end of each branch, there is a YAML file (<code>qna.yaml</code>) that contains the examples for that domain along with any attribution files (<code>attribution.txt</code>). Maintainers can decide to change the names of the existing branches or to add new branches.</p> <p>Important</p> <p>Folder names do not have spaces. Use underscores between words.</p>"},{"location":"taxonomy/#taxonomy-diagram","title":"Taxonomy diagram","text":"<p>Note</p> <p>These diagrams show subsets of the taxonomy. They are not a complete representation.</p> <p>In this diagram, a subset of a taxonomy for InstructLab demonstrates the branch-and-leaf-node structure.</p> <pre><code> flowchart TD;\n   na[not accepting contributions\\n at this time]:::na\n   taxonomy --&gt; foundational_skill &amp; compositional_skills &amp; knowledge\n\n   foundational_skill:::na --&gt; reasoning:::na\n   reasoning:::na --&gt; common_sense_reasoning:::na\n   reasoning:::na --&gt; mathematical_reasoning:::na\n   reasoning:::na --&gt; theory_of_mind:::na\n\n   compositional_skills --&gt; engineering\n   compositional_skills --&gt; grounded\n   compositional_skills --&gt; lingustics\n\n   grounded --&gt; grounded/arts\n   grounded --&gt; grounded/geography\n   grounded --&gt; grounded/history\n   grounded --&gt; grounded/science\n\n   knowledge --&gt; knowledge/arts\n\n   knowledge --&gt; knowledge/miscellaneous_unknown\n   knowledge --&gt; knowledge/science\n   knowledge --&gt; knowledge/technology\n   knowledge/science --&gt; animals --&gt; birds --&gt; black_capped_chickadee --&gt; black_capped_chickadee-a &amp; black_capped_chickadee-q\n   knowledge/science --&gt; astronomy --&gt; constellations --&gt; phoenix --&gt; phoenix-a &amp; phoenix-q\n\n   black_capped_chickadee-a{attribution.txt}\n   black_capped_chickadee-q{qna.yaml}\n   phoenix-a{attribution.txt}\n   phoenix-q{qna.yaml}\n   classDef na fill:#EEE</code></pre> <p>Here is an illustrative directory structure to show how the taxonomy is laid out in the practical sense:</p> <pre><code>.\n\u2514\u2500\u2500 linguistics\n    \u251c\u2500\u2500 writing\n    \u2502   \u251c\u2500\u2500 brainstorming\n    \u2502   \u2502   \u251c\u2500\u2500 idea_generation\n    |   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2502   \u2502   \u251c\u2500\u2500 refute_claim\n    |   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2502   \u251c\u2500\u2500 prose\n    \u2502   \u2502   \u251c\u2500\u2500 articles\n    \u2502   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2514\u2500\u2500 grammar\n        \u2514\u2500\u2500 qna.yaml\n        \u2502   attribution.txt\n        \u2514\u2500\u2500 spelling\n            \u2514\u2500\u2500 qna.yaml\n                attribution.txt\n</code></pre>"},{"location":"taxonomy/#contribute-knowledge-and-skills-to-the-taxonomy","title":"Contribute knowledge and skills to the taxonomy","text":"<p>The ability to contribute to a Large Language Model (LLM) has been difficult in no small part because it is difficult to get access to the necessary compute infrastructure.</p> <p>The taxonomy repository will be used as the seed to synthesize the training data for InstructLab-trained models. We intend to retrain the model(s) using the main branch following InstructLab's progressive training on a regular basis. This enables fast iteration of the model(s), for the benefit of the open source community.</p> <p>By contributing your skills and knowledge to this repository, you will see your changes built into an LLM within days of your contribution rather than months or years! If you are working with a model and notice its knowledge or ability lacking, you can correct it by contributing knowledge or skills and check if it's improved after your changes are built.</p> <p>While public contributions are welcome to help drive community progress, you can also fork this repository under the Apache License, Version 2.0, add your own internal skills, and train your own models internally. However, you might need your own access to significant compute infrastructure to perform sufficient retraining.</p>"},{"location":"taxonomy/#ways-to-contribute","title":"Ways to contribute","text":"<p>You can contribute to the taxonomy in the following two ways:</p> <ol> <li>Adding new examples to existing leaf nodes:</li> <li>Adding new branches/skills corresponding to the existing domain:</li> </ol> <p>For more information, see the Ways of contributing to the taxonomy repository documentation.</p>"},{"location":"taxonomy/#how-to-contribute-skills-and-knowledge","title":"How to contribute skills and knowledge","text":"<p>To contribute to the repo, you'll use the Fork and Pull model common in many open source repositories. You can add your skills and knowledge to the taxonomy in multiple ways; for additional information on how to make a contribution, see the Documentation on contributing. You can also use the following guides to help with contributing:</p> <ul> <li>Contributing using the GitHub webpage UI.</li> <li>Contributing knowledge to the taxonomy in the Knowledge contribution guidelines.</li> </ul>"},{"location":"taxonomy/#why-should-i-contribute","title":"Why should I contribute?","text":"<p>This taxonomy repository will be used as the seed to synthesize the training data for InstructLab-trained models. We intend to retrain the model(s) using the main branch as often as possible (at least weekly). Fast iteration of the model(s) benefits the open source community and enables model developers who do not have access to the necessary compute infrastructure.</p> <ol> <li> <p>Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, Akash Srivastava. \"LAB: Large-Scale Alignment for ChatBots\", arXiv preprint arXiv: 2403.01081, 2024. (* denotes equal contributions)\u00a0\u21a9</p> </li> </ol>"},{"location":"taxonomy/knowledge/","title":"What is \"Knowledge\"?","text":"<p>In the InstructLab world, knowledge consists of data and facts and is backed by documents. When you create knowledge for a model, you're giving it additional data to more accurately answer questions.</p> <p>Knowledge contributions in this project contain a few things:</p> <ul> <li>A Markdown or PDF file in a git repository that holds your information. For example, these repositories can include Markdown or PDF versions of information on Oscar 2024 winners, Law books, Shakespeare, Sports, Chemistry, etc. Note that sources must be licensed using a compatible license. Learn more from our list of accepted sources.</li> <li>A <code>qna.yaml</code> file that asks and answers questions about the information in the git repository.</li> <li>An <code>attribution.txt</code> file that includes the sources for the information used in the <code>qna.yaml</code>. Note that this file is not required if you are not submitting to the upstream taxonomy repository at https://github.com/instructlab/taxonomy.</li> </ul> <p>You can learn more about the knowledge structure in the documentation for the file structure for knowledge.</p>"},{"location":"taxonomy/knowledge/#getting-started-with-knowledge","title":"Getting Started with Knowledge","text":"<p>While skills are foundational or performative, knowledge is based more on answering questions that involve facts, data, or references.</p> <p>Knowledge is supported by documents, such as a textbook, technical manual, encyclopedia, journal, or magazine.</p> <p>Important</p> <p>If you are using InstructLab version <code>0.21.0</code> or above, you can specify PDF files in your knowledge <code>qna.yaml</code> file as a valid document type. Any previous version of InstructLab still only consumes knowledge documents in Markdown format.</p> <p>Knowledge in a taxonomy tree consists of a few more elements than skills:</p> <ul> <li>Each knowledge node in the tree has a <code>qna.yaml</code>, similar to the format of the <code>qna.yaml</code> for skills.</li> <li>\u2b50 Currently, for the synthetic data generation (SDG) process to work, knowledge submissions require you to create a Git repository, which can be with GitHub, that contains the source files of your knowledge contributions. These contributions in your repository must either be in Markdown (<code>.md</code>) format or as a PDF (<code>.pdf</code>).</li> <li>The <code>qna.yaml</code> includes parameters that contain information from your repository.</li> </ul> <p>Tip</p> <p>Guidelines for Knowledge contributions</p> <ul> <li>Submit the most up-to-date version of the document</li> <li>All submissions must be text; images will be ignored</li> </ul>"},{"location":"taxonomy/knowledge/#structure-of-the-qnayaml-file","title":"Structure of the <code>qna.yaml</code> file","text":"<p>Explore the file structure.</p> <p>For the upstream taxonomy specifically, reference the structure provided in the contribution details guide to understand the required keys and values.</p>"},{"location":"taxonomy/knowledge/#the-knowledge-documents-repository","title":"The knowledge documents repository","text":"<p>Currently, all knowledge documents need to be stored in a git repository. You can use a local git repository, initialized from a local directory, if you so choose.</p>"},{"location":"taxonomy/knowledge/file_structure/","title":"The knowledge files","text":"<p>Taxonomy trees in InstructLab have leaf-node directories. These leaf nodes contain at least one file, and usually two:</p> <ul> <li>A <code>qna.yaml</code> file that asks and answers questions about the information in the git repository where you have stored a knowledge document.</li> <li>An <code>attribution.txt</code> file that includes the sources for the information used in the <code>qna.yaml</code>. This file is only required when submitting knowledge to the InstructLab taxonomy repository.</li> </ul>"},{"location":"taxonomy/knowledge/file_structure/#the-qnayaml-file","title":"The <code>qna.yaml</code> file","text":"<p>Note</p> <p>Tokens in the case of context, questions, and answers can fit to \"words,\" but it's specifically tokens, and not words, that are the limitations. Learn more.</p> <p>In general, here are the fields in the YAML file:</p> Key Type Required Constraints Value Notes <code>version</code> integer Y - <code>3</code> The taxonomy schema version used in the <code>qna.yaml</code> file. Defined in instructlab/schema <code>created_by</code> string Y no spaces Your GitHub username (for the upstream taxonomy) or your name with no spaces (for general intructlab use) - <code>domain</code> string Y - Knowledge sub-category The knowledge domain which is used in prompts to the teacher model during synthetic data generation. The domain should be brief such as the title to a textbook chapter or section. <code>seed_examples</code> Y array at least 5 sets null This is a collection of questions and answers with context from the knowledge document that InstructLab uses to generate data synthetically. <code>context</code> string Y &lt; 500 tokens A chunk of the knowledge document showing off the different unique content to help guide the teacher model. If the knowledge documents have only text, all context would be text. If the knowledge documnets have tables or other content formats, ensure samples of those formats are all used. This should be a copy-paste from the Markdown version of your document <code>questions_and_answers</code> Y array at least 3 pairs per context null This is a collection of questions and answers. <code>question</code> Y string &gt; 250 tokens A question related to and grounded in the relevant context Questions are things you'd expect someone to ask the model based on the context given. This will be used for synthetic data generation. <code>answer</code> Y string &gt; 250 tokens An answer for the question, longer than a one-word or one-number answer Answers are what you'd like the model to give as an answer. It will not be an exact answer the model always gives. <code>document_outline</code> Y string - A brief title-like summary of the document. This provides the context specific for each document chunk. It should be as specific as you possibly can get. For example, \"Acme Company 2023 Financial Report\" rather than \"Financial report.\" - <code>document</code> Y object - null The collection of data for the knowledge document. <code>repo</code> Y string a git URL The URL (with a <code>.git</code> suffix) that identifies your git repo where you've stored your knowledge documents - <code>commit</code> Y string full commit hash A SHA1 full commit hash that corresponds to the document in the repo This hash must be exactly where the system can find the document. <code>patterns</code> Y array <code>*.md</code>, <code>*.pdf</code> A list of glob patterns specifying the files in the repo. Any glob pattern that starts with <code>*</code> must be quoted due to YAML rules. Currently, the system accepts <code>.md</code> and <code>.pdf</code> files."},{"location":"taxonomy/knowledge/file_structure/#version","title":"<code>version</code>","text":"<p>The <code>version</code> field is the version of the schema that is in use. Currently, the value here should be <code>3</code>.</p> qna.yaml<pre><code>version: 3\ndomain: # ...\ncreated_by: # ...\nseed_examples:\n  # ...\ndocument_outline: # ...\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#domain","title":"<code>domain</code>","text":"<p>The <code>domain</code> field helps the synthetic data generation (SDG) process by identifying what specialized area the knowledge provided covers.</p> qna.yaml<pre><code>version: # ...\ndomain: astronomy\ncreated_by: # ...\nseed_examples:\n  # ...\ndocument_outline: # ...\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#created_by","title":"<code>created_by</code>","text":"<p>The <code>created_by</code> field defines the user who submitted the knowledge. If you're working upstream, it would be your GitHub username. If you're working on your own taxonomy, it would be some kind of identifier with no spaces.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: juliadenham\nseed_examples:\n  # ...\ndocument_outline: # ...\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#seed_examples","title":"<code>seed_examples</code>","text":"<p>The <code>seed_examples</code> field does not have anything next to it because it is an array. An array in YAML is a collection of other values, and those values are indicated through indentation on subsequent lines.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples:\n  - context: # ...\n    questions_and_answers:\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\ndocument_outline: # ...\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#context","title":"<code>context</code>","text":"<p>The <code>context</code> field is a snippet from the knowledge document that is used to answer the questions and answers that follow. It should be directly copied and pasted from the knowledge document as found in your knowledge repository. </p> <p>There will be more than one <code>context</code> field in the document.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples:\n  - context: |\n      **Phoenix** is a minor constellation in the southern sky. Named after the mythical\n      phoenix, it was first depicted on a celestial atlas by Johann Bayer in his 1603\n      *Uranometria*. The French explorer and astronomer Nicolas Louis de\n      Lacaille charted the brighter stars and gave their Bayer designations\n      in 1756. The constellation stretches from roughly \u221239 degrees to \u221257 degrees\n      declination, and from 23.5h to 2.5h of right ascension. The constellations Phoenix,\n      Grus, Pavo, are known as the Southern Birds.\n    questions_and_answers:\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\ndocument_outline: # ...\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#questions_and_answers","title":"<code>questions_and_answers</code>","text":"<p>The <code>questions_and_answers</code> field starts another array.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples:\n  - context: # ...\n    questions_and_answers:\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\n      - question: # ...\n        answer: # ...\ndocument_outline: # ...\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#question","title":"<code>question</code>","text":"<p>The <code>question</code> field is a sample question that the teacher model can use to train the student model during the synthetic data generation process.</p> <p>There will be more than one <code>question</code> field in the document.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples:\n  - context: # ...\n    questions_and_answers:\n      - question: |\n          What is the Phoenix constellation?\n        answer: # ...\n      - question: |\n          Who charted the Phoenix constellation?\n        answer: # ...\n      - question: |\n          How far does the Phoenix constellation stretch?\n        answer: # ...\ndocument_outline: # ...\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#answer","title":"<code>answer</code>","text":"<p>The <code>answer</code> field is a sample expected answer for a question that the teacher model can use to train the student model during the synthetic data generation process.</p> <p>There will be more than one <code>answer</code> field in the document.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples:\n  - context: # ...\n    questions_and_answers:\n      - question: # ...\n        answer: |\n          Phoenix is a minor constellation in the southern sky.\n      - question: # ...\n        answer: |\n          The Phoenix constellation was charted by french explorer and\n          astronomer Nicolas Louis de Lacaille.\n      - question: # ...\n        answer: |\n          The phoenix constellation stretches from roughly \u221239\u00b0 to \u221257\u00b0\n          declination, and from 23.5h to 2.5h of right ascension.\ndocument_outline: # ...\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#document_outline","title":"<code>document_outline</code>","text":"<p>The <code>document_outline</code> field is a short description of the knowledge document's topic.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples: # ...\ndocument_outline:  |\n  Information about the Phoenix Constellation including the\n  history, characteristics, and features of the stars in the constellation.\ndocument:\n  # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#document","title":"<code>document</code>","text":"<p>The <code>document</code> field starts another array.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples: # ...\ndocument_outline: # ...\ndocument:\n    repo: # ...\n    commit: # ...\n    patterns: # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#repo","title":"<code>repo</code>","text":"<p>As InstructLab currently requires a git repository, you would provide an address to a git repository here in the <code>repo</code> field. That repository may be a local one or one hosted on version control providers like GitHub or GitLab.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples: # ...\ndocument_outline: # ...\ndocument:\n    repo: https://github.com/juliadenham/Summit_knowledge\n    commit: # ...\n    patterns: # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#commit","title":"<code>commit</code>","text":"<p>The <code>commit</code> field holds the git-based commit hash where the exact version of your knowledge document can be found. This field allows you to pin a document version.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples: # ...\ndocument_outline: # ...\ndocument:\n    repo: # ...\n    commit: 0a1f2672b9b90582e6115333e3ed62fd628f1c0f\n    patterns: # ...\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#patterns","title":"<code>patterns</code>","text":"<p>The <code>patterns</code> field indicates what kind of files the SDG process should use. You can give an exact name, like <code>phoenix_constellation.md</code>, or a glob pattern, like <code>*.md</code>. This is a list of patterns, so there may be more than one.</p> qna.yaml<pre><code>version: # ...\ndomain: # ...\ncreated_by: # ...\nseed_examples: # ...\ndocument_outline: # ...\ndocument:\n    repo: # ...\n    commit: # ...\n    patterns:\n      - phoenix_constellation.md\n</code></pre>"},{"location":"taxonomy/knowledge/file_structure/#example-of-a-knowledge-submission","title":"Example of a knowledge submission","text":"<p>You can review a full example of a knowledge submission for the upstream taxonomy in this commit in the taxonomy repository.</p> The submissionThe <code>qna.yaml</code> fileThe <code>attribution.txt</code> file <ul> <li>the <code>qna.yaml</code> file</li> <li>the <code>attribution.txt</code> file</li> </ul> <pre><code>version: 3\ndomain: astronomy\ncreated_by: juliadenham\nseed_examples:\n  - context: |\n      **Phoenix** is a minor constellation in the southern sky. Named after the mythical\n      phoenix, it was first depicted on a celestial atlas by Johann Bayer in his 1603\n      *Uranometria*. The French explorer and astronomer Nicolas Louis de\n      Lacaille charted the brighter stars and gave their Bayer designations\n      in 1756. The constellation stretches from roughly \u221239 degrees to \u221257 degrees\n      declination, and from 23.5h to 2.5h of right ascension. The constellations Phoenix,\n      Grus, Pavo, are known as the Southern Birds.\n    questions_and_answers:\n      - question: |\n          What is the Phoenix constellation?\n        answer: |\n          Phoenix is a minor constellation in the southern sky.\n      - question: |\n          Who charted the Phoenix constellation?\n        answer: |\n          The Phoenix constellation was charted by french explorer and\n          astronomer Nicolas Louis de Lacaille.\n      - question: |\n          How far does the Phoenix constellation stretch?\n        answer: |\n          The phoenix constellation stretches from roughly \u221239\u00b0 to \u221257\u00b0\n          declination, and from 23.5h to 2.5h of right ascension.\n  - context: |\n      Phoenix was the largest of the 12 constellations established by Petrus\n      Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de\n      Houtman. It first appeared on a 35cm diameter celestial globe published\n      in 1597 (or 1598) in Amsterdam by Plancius with Jodocus Hondius. The first\n      depiction of this constellation in a celestial atlas was in Johann Bayer's\n      *Uranometria* of 1603. De Houtman included it in his southern star catalog\n      the same year under the Dutch name *Den voghel Fenicx*, \"The Bird Phoenix\",\n      symbolising the phoenix of classical mythology. One name of the brightest star Alpha\n      Phoenicis\u2014Ankaa\u2014is derived from the Arabic: \u0627\u0644\u0639\u0646\u0642\u0627\u0621, romanized: al-\u2018anq\u0101\u2019,\n      lit.\u2009'the phoenix', and was coined sometime after 1800 in relation to the constellation.\n    questions_and_answers:\n      - question: |\n          What is the brightest star in the Phoenix constellation\n          called?\n        answer: |\n          Alpha Phoenicis or Ankaa is the brightest star in the Phoenix\n          Constellation.\n      - question: Where did the Phoenix constellation first appear?\n        answer: |\n          The Phoenix constellation first appeared on a 35-cm diameter\n          celestial globe published in 1597 (or 1598) in Amsterdam by\n          Plancius with Jodocus Hondius.\n      - question: |\n          What does \"The Bird Phoenix\" symbolize?\n        answer: |\n          \"The Bird Phoenix\" symbolizes the phoenix of classical mythology.\n  - context: |\n      Phoenix is a small constellation bordered by Fornax and Sculptor to the north,\n      Grus to the west, Tucana to the south, touching on the corner of Hydrus to the\n      south, and Eridanus to the east and southeast. The bright star Achernar is\n      nearby. The three-letter abbreviation for the constellation, as adopted by the\n      International Astronomical Union in 1922, is \"Phe\". The official constellation\n      boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930,\n      are defined by a polygon of 10 segments. In the equatorial coordinate system, the right\n      ascension coordinates of these borders lie between 23h 26.5m and 02h 25.0m,\n      while the declination coordinates are between \u221239.31\u00b0 and \u221257.84\u00b0. This means it remains\n      below the horizon to anyone living north of the 40th parallel in the Northern\n      Hemisphere, and remains low in the sky for anyone living north of the equator.\n      It is most visible from locations such as Australia and South Africa during\n      late Southern Hemisphere spring. Most of the constellation lies within, and\n      can be located by, forming a triangle of the bright stars Achernar, Fomalhaut\n      and Beta Ceti\u2014Ankaa lies roughly in the centre of this.\n    questions_and_answers:\n      - question: What are the characteristics of the Phoenix constellation?\n        answer: |\n          Phoenix is a small constellation bordered by Fornax and Sculptor to\n          the north, Grus to the west, Tucana to the south, touching on the\n          corner of Hydrus to the south, and Eridanus to the east and southeast.\n          The bright star Achernar is nearby.\n      - question: |\n          When is the phoenix constellation most visible?\n        answer: |\n          Phoenix is most visible from locations such as Australia and\n          South Africa during late Southern Hemisphere spring.\n      - question: |\n          What are the Phoenix Constellation boundaries?\n        answer: |\n          The official constellation boundaries for Phoenix, as set by Belgian\n          astronomer Eug\u00e8ne Delporte in 1930, are defined by a polygon of 10\n          segments.\n  - context: |\n      Ten stars have been found to have planets to date, and four planetary\n      systems have been discovered with the SuperWASP project. HD 142 is a yellow\n      giant that has an apparent magnitude of 5.7, and has a planet HD 142b 1.36\n      times the mass of Jupiter which orbits every 328 days.  HD 2039 is a yellow\n      subgiant with an apparent magnitude of 9.0 around 330 light years away which\n      has a planet HD 2039 b six times the mass of Jupiter. WASP-18 is a star of\n      magnitude 9.29 which was discovered to have a hot Jupiter-like planet taking\n      less than a day to orbit the star. The planet is suspected to be causing WASP-18 to\n      appear older than it really is. WASP-4 and WASP-5 are solar-type yellow stars around 1000\n      light years distant and of 13th magnitude, each with a single planet larger\n      than Jupiter. WASP-29 is an orange dwarf of spectral type K4V and visual magnitude\n      11.3, which has a planetary companion of similar size and mass to Saturn. The planet\n      completes an orbit every 3.9 days.\n    questions_and_answers:\n      - question: In the Phoenix constellation, how many stars have planets?\n        answer: |\n          In the Phoenix constellation, ten stars have been found to have\n          planets to date, and four planetary systems have been discovered\n          with the SuperWASP project.\n      - question: |\n          What is HD 142?\n        answer: |\n          HD 142 is a yellow giant that has an apparent magnitude of 5.7, and\n          has a planet (HD 142 b) 1.36 times the mass of Jupiter which\n          orbits every 328 days.\n      - question: |\n          Are WASP-4 and WASP-5 solar-type yellow stars?\n        answer: |\n          Yes, WASP-4 and WASP-5 are solar-type yellow stars around 1000 light\n          years distant and of 13th magnitude, each with a single planet\n          larger than Jupiter.\n  - context: |\n      The constellation does not lie on the galactic plane of the Milky Way, and there\n      are no prominent star clusters. NGC 625 is a dwarf irregular galaxy of apparent magnitude 11.0\n      and lying some 12.7 million light years distant. Only 24000 light years in\n      diameter, it is an outlying member of the Sculptor Group. NGC 625 is\n      thought to have been involved in a collision and is experiencing a burst\n      of active star formation. NGC 37 is a lenticular galaxy of apparent magnitude\n      14.66. It is approximately 42 kiloparsecs in diameter and about 12.9 billion years old.\n      Robert's Quartet , and three spiral galaxies NGC 88 and NGC 92) is a group of\n      four galaxies located around 160 million light-years away which are in the process of colliding\n      and merging. They are within a circle of radius of 1.6 arcmin, corresponding to about\n      75,000 light-years. Located in the galaxy ESO 243-49 is HLX-1, an intermediate-mass\n      black hole intermediate-mass_black_hole \u2014the first one of its kind identified.\n      It is thought to be a remnant of a dwarf galaxy that was absorbed in a collision\n      with ESO 243-49. Before its discovery, this class of black hole was only hypothesized.\n    questions_and_answers:\n      - question: |\n          Is the Phoenix Constellation part of the Milky Way?\n        answer: |\n          The Phoenix constellation does not lie on the galactic plane of\n          the Milky Way, and there are no prominent star clusters.\n      - question: |\n          How many light years away is NGC 625?\n        answer: |\n          NGC 625 is 24000 light years in diameter and is an outlying\n          member of the Sculptor Group.\n      - question: |\n          What is Robert's Quartet composed of?\n        answer: |\n          Robert's Quartet is composed of the irregular galaxy NGC 87,\n          and three spiral galaxies NGC 88, NGC 89 and NGC 92.\ndocument_outline: |\n  Information about the Phoenix Constellation including the\n  history, characteristics, and features of the stars in the constellation.\ndocument:\n    repo: https://github.com/juliadenham/Summit_knowledge\n    commit: 0a1f2672b9b90582e6115333e3ed62fd628f1c0f\n    patterns:\n      - phoenix_constellation.md\n</code></pre> <pre><code>Title of work: Phoenix (constellation)\nLink to work: https://en.wikipedia.org/wiki/Phoenix_(constellation)\nRevision: https://en.wikipedia.org/w/index.php?title=Phoenix_(constellation)&amp;oldid=1237187773\nLicense of the work: CC-BY-SA-4.0\nCreator names: Wikipedia Authors\n</code></pre> <p>For more information on what to include in your <code>attribution.txt</code> file for an upstream submission, reference the upstream contribution document on knowledge.</p>"},{"location":"taxonomy/knowledge/file_structure/#example-of-a-knowledge-document-file","title":"Example of a knowledge document file","text":"<p>The previous knowledge example references one Markdown file: <code>phoenix_constellation.md</code>. You can also add multiple Markdown files or PDFs for knowledge contributions.</p> <p>Here's what a snippet of <code>phoenix_constellation.md</code> might look like in your Git repository.</p> <p>Example of a <code>.md</code> file</p> <pre><code># Phoenix (constellation)\n\n**Phoenix** is a minor constellation in the southern sky. Named after the mythical\nphoenix, it was first depicted on a celestial atlas by Johann Bayer in his 1603\n*Uranometria*. The French explorer and astronomer Nicolas Louis de\nLacaille charted the brighter stars and gave their Bayer designations\nin 1756. The constellation stretches from roughly \u221239 degrees to \u221257 degrees\ndeclination, and from 23.5h to 2.5h of right ascension. The constellations Phoenix,\nGrus, Pavo, are known as the Southern Birds.\n\nThe brightest star, Alpha Phoenicis, is named Ankaa, an Arabic word meaning 'the Phoenix'.\nIt is an orange giant of apparent magnitude 2.4. Next is Beta Phoenicis, actually a\nbinary system composed of two yellow giants with a combined apparent magnitude of 3.3. Nu\nPhoenicis has a dust disk, while the constellation has ten star systems with known planets and the recently\ndiscovered galaxy clusters El Gordo and the Phoenix\nCluster\u2014located 7.2 and 5.7 billion light years away respectively, two of the largest objects in the visible\nuniverse. Phoenix is the radiant of two annual meteor showers: the Phoenicids in December, and the July\nPhoenicids.\n</code></pre> <p>You can organize the knowledge files in your repository however you want. You need to ensure the <code>document</code> section of the YAML file is pointing to the correct file.</p>"},{"location":"taxonomy/skills/","title":"Skills Overview","text":""},{"location":"taxonomy/skills/#what-is-a-skill","title":"What is a \"skill\"?","text":"<p>There are various types of skills that you can contribute to the taxonomy.</p>"},{"location":"taxonomy/skills/#compositional-skills","title":"Compositional skills","text":"<p>Skills are performative. When you create a skill for the model, you're teaching it how to do something such as \"write me a song,\" \"rearrange words in a sentence\" or \"summarize an email.\"</p> <p>There are two types of compositional skills, freeform and grounded.</p>"},{"location":"taxonomy/skills/#freeform-compositional-skills","title":"Freeform compositional skills","text":"<p>Freeform compositional skills are performative and do not require additional context. An example of a freeform skill is teaching the model words that rhyme. You could provide examples of \"words that rhyme with 'tool'\". By providing those examples, you're essentially using the latent knowledge of the LLM. In our example, you're enabling the LLM to be able to identify words that rhyme in its latent knowledge.</p> <p>Freeform skills include things like:</p> <ul> <li>Speak like Yoda</li> <li>Convert to camel case</li> <li>Write me a limerick</li> <li>Generate StableDiffusion prompts</li> </ul>"},{"location":"taxonomy/skills/#grounded-compositional-skills","title":"Grounded compositional skills","text":"<p>Grounded skills are performative and do require additional context. Examples of a grounded skill would be to read the value of a cell in a table layout or to parse a JSON file. To create a grounded skill to read a Markdown-formatted table layout, the additional context could be an example table layout. This additional context is including in the YAML for the skill and not external to it.</p> <p>Note</p> <p>The content of the table layout will not be used in training or aligning the model; only the table layout format itself will be used.</p> <p>Grounded skills include things like:</p> <ul> <li>Game creation like Sudoku or tic-tac-toe</li> <li>Summarizing or extracting from a piece of text</li> <li>Find unresolved items in a meeting transcript</li> </ul> <p>Example of a grounded compositional skill pull request in the upstream repository</p>"},{"location":"taxonomy/skills/#core-or-foundational-skills","title":"Core or foundational skills","text":"<p>Core skills are foundational skills like math, reasoning, and coding. In the taxonomy repository, core skills are found in the <code>foundational_skills</code> directory.</p> <p>Note</p> <p>Unlike knowledge and compositional skills, core skills are not something you can contribute to the tree. So references to contributing \"skills\" to the taxonomy from this point forward are to compositional skills.</p>"},{"location":"taxonomy/skills/file_structure/","title":"The skills files","text":"<p>Taxonomy trees in InstructLab have leaf-node directories. These leaf nodes contain at least one file, and usually two:</p> <ul> <li>A <code>qna.yaml</code> file that asks and answers questions about the information in the git repository where you have stored a knowledge document.</li> <li> <p>An <code>attribution.txt</code> file that includes the sources for the information used in the <code>qna.yaml</code>. This file is only required when submitting knowledge to the InstructLab taxonomy repository. Learn more about this file in the upstream skill contribution guidelines.</p> </li> <li> <p>Skills require a much smaller volume of content than knowledge contributions. An entire skill contribution can be just a few lines of YAML in the <code>qna.yaml</code> file.</p> </li> </ul> <p>Compositional skills can either be grounded (includes a context) or ungrounded (does not include a context).  Grounded or ungrounded is declared by placement in a taxonomy tree, such as <code>linguistics/writing/poetry/haiku/</code> for an ungrounded skill or <code>grounded/linguistics/grammar</code> for a grounded skill. The <code>qna.yaml</code> is in the final node.</p>"},{"location":"taxonomy/skills/file_structure/#the-structure-of-the-qnayaml-file","title":"The structure of the <code>qna.yaml</code> file","text":"<p>Taxonomy skill files must be a valid YAML file named <code>qna.yaml</code>. Each <code>qna.yaml</code> file contains a set of key/value entries with the following keys:</p> Field Type Required? Constraints Content <code>version</code> integer yes -- The taxonomy schema version used in the <code>qna.yaml</code> file. Defined in instructlab/schema <code>task_description</code> string yes A description of the task which is used in prompts to the teacher model during synthetic data generation. The description should be detailed and prescriptive to improve the teacher model's responses. <code>created_by</code> string yes The GitHub username of the contributor. <code>seed_examples</code> array yes at least 5 sets A collection of key/value entries. New submissions should have at least five entries, although older files may have fewer.Note collections are nested lists, like sub-entries in a bulleted list. <code>context</code> string only for grounded skills &lt; 500 tokens Part of the <code>seed_examples</code> collection.Grounded skills require the user to provide context containing information that the model is expected to take into account during processing. This is different from knowledge, where the model is expected to gain facts and background knowledge from the tuning process.A chunk of the knowledge document showing off the different unique content to help guide the teacher model. It should be related to the skill. This should be a copy-paste from the Markdown version of your document. Note: The context key should not be used for ungrounded skills. <code>question</code> string yes &gt; 250 tokens Part of the <code>seed_examples</code> collection.A question for the model. Questions are things you'd expect someone to ask the model based on the context given. This will be used for synthetic data generation. Note that if this is a grounded skill, questions should be grounded in the context provided. <code>answer</code> string yes &gt; 250 tokens Part of the <code>seed_examples</code> collection.The desired response from the model. Longer than a one-word or one-number answer. <p>Other keys at any level are currently ignored.</p> <p>Important</p> <p>Each <code>qna.yaml</code> file requires a minimum of five question and answer pairs.</p>"},{"location":"taxonomy/skills/file_structure/#version","title":"<code>version</code>","text":"<p>The <code>version</code> field is the version of the schema that is in use. Currently, the value here should be <code>3</code>.</p> UngroundedGrounded qna.yaml<pre><code>version: 3\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  # ...\n</code></pre> qna.yaml<pre><code>version: 3\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  # ...\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#task_description","title":"<code>task_description</code>","text":"<p>The <code>task_description</code> field is</p> UngroundedGrounded qna.yaml<pre><code>version: # ...\ntask_description: 'Teach the model how to rhyme.'\ncreated_by: # ...\nseed_examples:\n  # ...\n</code></pre> qna.yaml<pre><code>version: # ...\ntask_description: |\n    This skill provides the ability to read a markdown-formatted table.\ncreated_by: # ...\nseed_examples:\n  # ...\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#created_by","title":"<code>created_by</code>","text":"<p>The <code>created_by</code> field defines the user who submitted the skill. If you're working upstream, it would be your GitHub username. If you're working on your own taxonomy, it would be some kind of identifier with no spaces.</p> UngroundedGrounded qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: juliadenham\nseed_examples:\n  # ...\n</code></pre> qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: mairin\nseed_examples:\n  # ...\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#seed_examples","title":"<code>seed_examples</code>","text":"<p>The <code>seed_examples</code> field does not have anything next to it because it is an array. An array in YAML is a collection of other values, and those values are indicated through indentation on subsequent lines.</p> UngroundedGrounded qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  - question: # ...\n    answer: # ...\n  - question: # ...\n    answer: # ...\n  - question: # ...\n    answer: # ...\n  - question: # ...\n    answer: # ...\n  - question: # ...\n    answer: # ...\n</code></pre> qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  - context: # ...\n    question: # ...\n    answer: # ...\n  - context: # ...\n    question: # ...\n    answer: # ...\n  - context: # ...\n    question: # ...\n    answer: # ...\n  - context: # ...\n    question: # ...\n    answer: # ...\n  - context: # ...\n    question: # ...\n    answer: # ...\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#context-grounded-skills-only","title":"<code>context</code> (grounded skills only)","text":"<p>The <code>seed_examples</code> field does not have anything next to it because it is an array. An array in YAML is a collection of other values, and those values are indicated through indentation on subsequent lines.</p> UngroundedGrounded <p>N/A</p> qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  - context: |\n      | **Breed**      | **Size**     | **Barking** | **Energy** |\n      |----------------|--------------|-------------|------------|\n      | Afghan Hound   | 25-27 in     | 3/5         | 4/5        |\n      | Labrador       | 22.5-24.5 in | 3/5         | 5/5        |\n      | Cocker Spaniel | 14.5-15.5 in | 3/5         | 4/5        |\n      | Poodle (Toy)   | &lt;= 10 in     | 4/5         | 4/5        |\n    question: # ...\n    answer: # ...\n  - context: |\n      | **Name** | **Date** | **Color** | **Letter** | **Number** |\n      |----------|----------|-----------|------------|------------|\n      | George   | Mar 5    | Green     | A          | 1          |\n      | Gr\u00e1inne  | Dec 31   | Red       | B          | 2          |\n      | Abigail  | Jan 17   | Yellow    | C          | 3          |\n      | Bhavna   | Apr 29   | Purple    | D          | 4          |\n      | R\u00e9my     | Sep 9    | Blue      | E          | 5          |\n    question: # ...\n    answer: # ...\n  - context: |\n      | Banana | Apple      | Blueberry | Strawberry |\n      |--------|------------|-----------|------------|\n      | Yellow | Red, Green | Blue      | Red        |\n      | Large  | Medium     | Small     | Small      |\n      | Peel   | Peel       | No peel   | No peel    |\n    question: # ...\n    answer: # ...\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#question","title":"<code>question</code>","text":"<p>The <code>question</code> field is a sample question that a teacher model would use to train a student model on the skill.</p> UngroundedGrounded qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  - question: What are 5 words that rhyme with horn?\n    answer: # ...\n  - question: What are 5 words that rhyme with cat?\n    answer: # ...\n  - question: What are 5 words that rhyme with poor?\n    answer: # ...\n  - question: What are 5 words that rhyme with bank?\n    answer: # ...\n  - question: What are 5 words that rhyme with bake?\n    answer: # ...\n</code></pre> qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  - context: # ...\n    question: |\n      Which breed has the most energy?\n    answer: # ...\n  - context: # ...\n    question: |\n      What is Gr\u00e1inne's letter and what is her color?\n    answer: # ...\n  - context: # ...\n    question: |\n      Which fruit is blue, small, and has no peel?\n    answer: # ...\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#answer","title":"<code>answer</code>","text":"<p>The <code>answer</code> field is an example answer that the teacher model uses to check the answer from a student model and train it to answer more accurately. Note that this would not be an exact answer the final student model would eventually give every time as the goal of fine-tuning with InstructLab is that it will give similar answers.</p> UngroundedGrounded qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  - question: # ...\n    answer: warn, torn, born, thorn, and corn.\n  - question: # ...\n    answer: bat, gnat, rat, vat, and mat.\n  - question: # ...\n    answer: door, shore, core, bore, and tore.\n  - question: # ...\n    answer: tank, rank, prank, sank, and drank.\n  - question: # ...\n    answer: wake, lake, steak, make, and quake.\n</code></pre> qna.yaml<pre><code>version: # ...\ntask_description: # ...\ncreated_by: # ...\nseed_examples:\n  - context: # ...\n    question: # ...\n    answer: |\n      The breed with the most energy is the Labrador.\n  - context: # ...\n    question: # ...\n    answer: |\n      Gr\u00e1inne's letter is B and her color is red.\n  - context: # ...\n    question: # ...\n    answer: |\n      The blueberry is blue, small, and has no peel.\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#writing-yaml","title":"Writing YAML","text":"<p>If you have not written YAML before, YAML is a text file where indentation matters.</p> <p>Tip</p> <ul> <li>Spaces and indentation matter in YAML. Use two spaces to indent.<ul> <li>Don't use tabs!</li> </ul> </li> <li>Do not have trailing spaces at the end of a line.</li> <li>Each example in <code>seed_examples</code> begins with a dash (<code>-</code>). Place this dash in front of the first field (<code>question</code> or <code>context</code>). The remaining keys in the example should not have this dash.</li> <li>Some special characters such as a double quotation mark (<code>\"</code>) and an apostrophe or single quotation mark (<code>'</code>) need to be escaped with backslash. This is why some of the lines for keys in the example YAML start the value with the pipe character (<code>|</code>) followed a new line and then an indented multi-line string. This character disables all of the special characters in the value for the key.You might also want to use the pipe character (<code>|</code>) for multi-line strings.</li> <li>Consider quoting all values with double quotation marks (<code>\"</code>) to avoid surprising YAML parser behavior (e.g., Yes answer can be interpreted by the parser as a boolean of <code>True</code> value, unless \"Yes\" is quoted.)</li> <li>See yaml-multiline.info for more info.</li> </ul> <p>We recommend you lint, or verify, your YAML using a tool. One linter option is yamllint.com. You can copy/paste your YAML into the box and select Go to have it analyze your YAML and make recommendations. Online tools like prettified and yaml-validator can automatically reformat your YAML to adhere to default <code>yamllint</code> parameters, such as breaking lines longer than 80 characters.</p>"},{"location":"taxonomy/skills/file_structure/#examples","title":"Examples","text":""},{"location":"taxonomy/skills/file_structure/#ungrounded-compositional-skill-yaml-example","title":"Ungrounded compositional skill: YAML example","text":"<pre><code>version: 2\ntask_description: 'Teach the model how to rhyme.'\ncreated_by: juliadenham\nseed_examples:\n  - question: What are 5 words that rhyme with horn?\n    answer: warn, torn, born, thorn, and corn.\n  - question: What are 5 words that rhyme with cat?\n    answer: bat, gnat, rat, vat, and mat.\n  - question: What are 5 words that rhyme with poor?\n    answer: door, shore, core, bore, and tore.\n  - question: What are 5 words that rhyme with bank?\n    answer: tank, rank, prank, sank, and drank.\n  - question: What are 5 words that rhyme with bake?\n    answer: wake, lake, steak, make, and quake.\n</code></pre> <p>Here is the location of this YAML in a sample taxonomy tree. Note that the YAML file itself, plus any added directories that contain the file, is the entirety of the skill in terms of a taxonomy contribution:</p>"},{"location":"taxonomy/skills/file_structure/#ungrounded-compositional-skill-directory-tree-example","title":"Ungrounded compositional skill: Directory tree example","text":"<pre><code>[...]\ncompositional_skills\n\u2514\u2500\u2500 writing\n    \u2514\u2500\u2500 poetry\n    |   \u2514\u2500\u2500 haiku &lt;=== here it is :)\n    |   |   \u2514\u2500\u2500 qna.yaml\n    |   |       attribution.txt\n        [...]\n    \u2514\u2500\u2500 prose\n    |   \u2514\u2500\u2500 debate\n    |   |   \u2514\u2500\u2500 qna.yaml\n    |   |       attribution.txt\n    [...]\n\n[...]\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#grounded-compositional-skill-yaml-example","title":"Grounded compositional skill: YAML example","text":"<p>Remember that grounded compositional skills require additional context and include a <code>context</code> field.</p> <p>This example snippet assumes the GitHub username <code>mairin</code> and shows some of the question/answer pairs present in the actual file:</p> <pre><code>version: 2\ntask_description: |\n    This skill provides the ability to read a markdown-formatted table.\ncreated_by: mairin # Use your GitHub username; only one creator supported\nseed_examples:\n  - context: |\n      | **Breed**      | **Size**     | **Barking** | **Energy** |\n      |----------------|--------------|-------------|------------|\n      | Afghan Hound   | 25-27 in     | 3/5         | 4/5        |\n      | Labrador       | 22.5-24.5 in | 3/5         | 5/5        |\n      | Cocker Spaniel | 14.5-15.5 in | 3/5         | 4/5        |\n      | Poodle (Toy)   | &lt;= 10 in     | 4/5         | 4/5        |\n    question: |\n      Which breed has the most energy?\n    answer: |\n      The breed with the most energy is the Labrador.\n  - context: |\n      | **Name** | **Date** | **Color** | **Letter** | **Number** |\n      |----------|----------|-----------|------------|------------|\n      | George   | Mar 5    | Green     | A          | 1          |\n      | Gr\u00e1inne  | Dec 31   | Red       | B          | 2          |\n      | Abigail  | Jan 17   | Yellow    | C          | 3          |\n      | Bhavna   | Apr 29   | Purple    | D          | 4          |\n      | R\u00e9my     | Sep 9    | Blue      | E          | 5          |\n    question: |\n      What is Gr\u00e1inne's letter and what is her color?\n    answer: |\n      Gr\u00e1inne's letter is B and her color is red.\n  - context: |\n      | Banana | Apple      | Blueberry | Strawberry |\n      |--------|------------|-----------|------------|\n      | Yellow | Red, Green | Blue      | Red        |\n      | Large  | Medium     | Small     | Small      |\n      | Peel   | Peel       | No peel   | No peel    |\n    question: |\n      Which fruit is blue, small, and has no peel?\n    answer: |\n      The blueberry is blue, small, and has no peel.\n</code></pre>"},{"location":"taxonomy/skills/file_structure/#grounded-compositional-skill-directory-tree-example","title":"Grounded compositional skill: Directory tree example","text":"<pre><code>[...]\ncompositional_skills\n\u2514\u2500\u2500 grounded\n    \u2514\u2500\u2500 technology\n        \u2514\u2500\u2500 machine_learning\n            \u2514\u2500\u2500 natural_language_processing\n        |   |     \u2514\u2500\u2500 information_extraction\n        |            \u2514\u2500\u2500 inference\n        |   |            \u2514\u2500\u2500 qualitative\n        |   |               \u251c\u2500\u2500 sentiment\n        |   |               |     \u2514\u2500\u2500 qna.yaml\n        |   |               |         attribution.txt\n        \u2502                   \u251c\u2500\u2500 quantitative\n        \u2502   \u2502                   \u251c\u2500\u2500 table_analysis &lt;=== here it is :)\n        \u2502   |   |               |     \u2514\u2500\u2500 qna.yaml\n        \u2502   \u2502   \u2502               |         attribution.txt\n\n[...]\n</code></pre>"},{"location":"taxonomy/upstream/contribution_guidelines/","title":"Taxonomy Contribution Guidelines","text":"<p>We welcome contributions to the upstream InstructLab taxonomy.(1)</p> <ol> <li><code>upstream</code> here refers to the idea in open source of an \"upstream\" project that is before your product in a workstream.</li> </ol> <p>A contribution to the upstream taxonomy involves</p> <ul> <li>identifying whether your submission is a knowledge submission or a skills submission,</li> <li>defining the domain for the submission, or the branches and leaf node that will contain your knowledge,</li> <li>converting any source data to Markdown or PDF and storing them in a git-based repository (required for knowledge submissions),</li> <li>writing a YAML file called <code>qna.yaml</code> that contains the seed content that will inform synthetic data generation and the repo URL where your converted source data is stored,</li> <li>creating an attribution file called <code>attribution.txt</code> that provides licensing information for source data, and</li> <li>submitting those two files in the proper spot in the taxonomy tree to the InstructLab taxonomy.</li> </ul> <p>You can submit knowledge or compositional skills. More information on the two types of contribution can be found in our knowledge contribution details guide or our compositional skills contribution guide.</p>"},{"location":"taxonomy/upstream/contribution_guidelines/#attributiontxt","title":"<code>attribution.txt</code>","text":"<p>An important part of contributing to the InstructLab project is citing your sources of information. This comes in the form of your <code>attribution.txt</code> that you add to the pull requests. Almost all instances of attribution can be covered by the parameters required for Creative Commons Attribution licenses. Some parameters are as follows:</p> <ul> <li>Title of work</li> <li>Link to work<ul> <li>Include link to a specific revision where possible</li> </ul> </li> <li>License of the work<ul> <li>Include an SPDX identifier where possible</li> </ul> </li> <li>Creator names</li> <li>Copyright information</li> <li>Modification information<ul> <li>Indicate if work was itself derived from another openly licensed work</li> </ul> </li> </ul> <p>You can also see this citation style in the Data sources documentation.</p>"},{"location":"taxonomy/upstream/contribution_guidelines/#upstream-taxonomy-tree-layout","title":"Upstream taxonomy tree layout","text":"<p>The upstream taxonomy tree is organized in a cascading directory structure. At the end of each branch, there is a YAML file (<code>qna.yaml</code>) that contains the examples for that domain. Maintainers can decide to change the names of the existing branches or to add new branches.</p> <p>Important</p> <p>Folder names do not have spaces. Use underscores between words.</p> <p>Below is an illustrative directory structure to show this layout:</p> <pre><code>.\n\u2514\u2500\u2500 linguistics\n    \u251c\u2500\u2500 writing\n    \u2502   \u251c\u2500\u2500 brainstorming\n    \u2502   \u2502   \u251c\u2500\u2500 idea_generation\n    |   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2502   \u2502   \u251c\u2500\u2500 refute_claim\n    |   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2502   \u251c\u2500\u2500 prose\n    \u2502   \u2502   \u251c\u2500\u2500 articles\n    \u2502   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2514\u2500\u2500 grammar\n        \u2514\u2500\u2500 qna.yaml\n        \u2502   attribution.txt\n        \u2514\u2500\u2500 spelling\n            \u2514\u2500\u2500 qna.yaml\n                attribution.txt\n</code></pre>"},{"location":"taxonomy/upstream/contribution_guidelines/#upstream-taxonomy-diagram","title":"Upstream taxonomy diagram","text":"<p>Note</p> <p>This diagram shows a subset of the upstream taxonomy. It is not a complete representation.</p> <pre><code> flowchart TD;\n   na[not accepting contributions\\n at this time]:::na\n   taxonomy --&gt; foundational_skill &amp; compositional_skills &amp; knowledge\n\n   foundational_skill:::na --&gt; reasoning:::na\n   reasoning:::na --&gt; common_sense_reasoning:::na\n   reasoning:::na --&gt; mathematical_reasoning:::na\n   reasoning:::na --&gt; theory_of_mind:::na\n\n   compositional_skills --&gt; engineering\n   compositional_skills --&gt; grounded\n   compositional_skills --&gt; lingustics\n\n   grounded --&gt; grounded/arts\n   grounded --&gt; grounded/geography\n   grounded --&gt; grounded/history\n   grounded --&gt; grounded/science\n\n   knowledge --&gt; knowledge/arts\n\n   knowledge --&gt; knowledge/miscellaneous_unknown\n   knowledge --&gt; knowledge/science\n   knowledge --&gt; knowledge/technology\n   knowledge/science --&gt; animals --&gt; birds --&gt; black_capped_chickadee --&gt; black_capped_chickadee-a &amp; black_capped_chickadee-q\n   knowledge/science --&gt; astronomy --&gt; constellations --&gt; phoenix --&gt; phoenix-a &amp; phoenix-q\n\n   black_capped_chickadee-a{attribution.txt}\n   black_capped_chickadee-q{qna.yaml}\n   phoenix-a{attribution.txt}\n   phoenix-q{qna.yaml}\n   classDef na fill:#EEE</code></pre>"},{"location":"taxonomy/upstream/contribution_guidelines/#considerations","title":"Considerations","text":"<p>When working with the upstream taxonomy, there are some things to keep in mind, like topics to avoid, what LLMs are great at doing, and what LLMs will fail at. Explore this information before deciding what to contribute, especially the note about hallucinations at the end of this page.</p>"},{"location":"taxonomy/upstream/contribution_guidelines/#avoid-these-topics","title":"Avoid These Topics","text":"<p>While the tuning process may eventually benefit from being used to help the models work with complex social topics, at this time this is an area of active research we do not want to take lightly. Therefore, please keep your submissions clear of the following topics:</p> <ul> <li>PII (personally identifiable information) or any content invasive of individual privacy rights</li> <li>Violence including self-harm</li> <li>Cyber Bullying</li> <li>Internal documentation or other that is confidential to your employer or organization, e.g. trade secrets</li> <li>Discrimination</li> <li>Religion<ul> <li>Facts such as, \"Christianity is, according to the 2011 census, the fifth most practiced religion in Nepal, with 375,699 adherents, or 1.4% of the population\", are fine as a knowledge contribution. However, advocating in favor of or against any religious faith is not acceptable.</li> </ul> </li> <li>Medical or health information<ul> <li>Facts such as,  \"In mammals, pulmonary ventilation occurs via inhalation (breathing),\" are fine as a knowledge contribution. However, tailored medical/health advice is not acceptable.</li> </ul> </li> <li>Financial information<ul> <li>Facts such as \"laissez-faire economics ... argues that market forces alone should drive the economy and that governments should refrain from direct intervention in or moderation of the economic system,\" are fine as a knowledge contribution. However, tailored financial advice is not acceptable.</li> </ul> </li> <li>Legal settlements/mitigations</li> <li>Gender Bias</li> <li>Hostile language, threats, slurs, derogatory or insensitive jokes or comments </li> <li>Profanity </li> <li>Pornography and sexually explicit or suggestive content </li> <li>Any contributions that would allow for automated decision-making that affect an individual's rights or well-being, such as social scoring </li> <li>Any contributions that engage in political campaigning or lobbying</li> </ul> <p>We are also not accepting submissions of the following content:</p> <ul> <li>Jokes<ul> <li>We received many joke submissions at the beginning of the project, and with jokes being \"in the eye of the beholder\" and puns requiring nuance for native English speakers, we realized we were possibly unconsciously biasing our model. We have discovered that working with jokes has its own challenges, and if we want something generalized, finding consensus was unsuccessful. For now, we're not accepting additional submissions of jokes.</li> </ul> </li> <li>Poems<ul> <li>We received many poem submissions at the beginning of the project, and with poems being \"in the eye of the beholder\" and puns requiring nuance for native English speakers, we realized we were possibly unconsciously biasing our model. We have discovered that working with both topics has its own challenges, and if we want something generalized, finding consensus was unsuccessful. For now, we're not accepting additional submissions of poems.</li> </ul> </li> <li>Code<ul> <li>Anything code-related that can be traced back to code for a computer. Not limited to <code>sed</code> or <code>bash</code> but <code>yaml</code>s for OpenShift or Kubernetes, to <code>python</code> snippets to <code>Java</code> suggestions. There are specific models focused on this space and this isn't for this model for the time being.</li> </ul> </li> <li>\"Guardrails\" for AI<ul> <li>We expect our upstream engineering team to create these types of skills and safeguards. We appreciate our community wanting to help with this, but there are underlying engineering decisions and taking this from the community may conflict with these.</li> </ul> </li> </ul>"},{"location":"taxonomy/upstream/contribution_guidelines/#building-your-llm-intuition","title":"Building Your LLM Intuition","text":"<p>LLMs have inherent limitations that make certain tasks extremely difficult, like doing math problems. They're great at other tasks, like creative writing. And they could be better at things like logical reasoning.</p> <p>In regard to skills, consider these limitations and advantages when you're generating skills. Skills in the first and second categories are welcomed. Skills in the third category are usually borderline and may be rejected.</p> <p>In regard to knowledge, providing an LLM training pipeline with knowledge helps create a basis of information that the model can learn from. With InstructLab, you can teach it to use this knowledge via the <code>qna.yaml</code> files. For example, you can give an LLM the entire periodic table, then in a <code>qna.yaml</code> add something like:</p> <pre><code>question: What is the symbol and atomic number for Chlorine?\nanswer: |\n  The symbol for chlorine is \"Cl\", and the atomic number is 17.\n</code></pre> <p>With a few of these question-and-answer pairs, the model will learn the periodic table because it has the knowledge data.</p>"},{"location":"taxonomy/upstream/contribution_guidelines/#llms-are-great-at","title":"LLMs are great at","text":"<ul> <li>Brainstorming</li> <li>Creativity</li> <li>Connecting information</li> <li>Cross-lingual behavior</li> </ul> <p>For these, however, it's common for LLMs to already have excellent performance. Try 3-5 examples in <code>ilab model chat</code> to confirm a deficit in the model before you build your submission, and share the examples in your Pull Request (PR).</p> <p>Skills in this category are welcomed, as refining these abilities helps us get better at the kinds of tasks where LLMs can excel.</p>"},{"location":"taxonomy/upstream/contribution_guidelines/#llms-need-help-with","title":"LLMs need help with","text":"<ul> <li>Chains of reasoning</li> <li>Analysis</li> <li>Story plots</li> <li>Reassembling information</li> <li>Effective and succinct summaries</li> </ul> <p>LLM behavior in these sorts of topics are very difficult for the model to get right. Skills in this category are particularly welcome. Try several examples to understand the nuances of the model's ability to do these sorts of tasks, and then consider using corrections to the results you get in your tuning process.</p>"},{"location":"taxonomy/upstream/contribution_guidelines/#llms-are-not-so-great-at","title":"LLMs are not so great at","text":"<ul> <li>Math</li> <li>Computation</li> <li>\"Turing-complete\" type tasks</li> <li>Generating only true real-world information (they're prone to hallucinations)</li> </ul> <p>LLMs may struggle with solving math and computation problems, and LLMs may always struggle here. Solving math and computation problems via probability on natural language queries is probably not the best way to solve them. That said, improving some of these foundational skills may be something this work tackles in the future, but not at this time.</p> <p>Most skill submissions in these categories are likely to be rejected.</p> <p>On Hallucinations</p> <p>For hallucinations in particular, trying to solve this with a skill is unlikely to work. Consider contributing to the knowledge taxonomy instead to improve the model's understanding of facts.</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/","title":"Knowledge Contribution Guidelines","text":"<p>Info</p> <p>The following information is if you are comfortable contributing using GitHub, a version control system primarily used for code. If you are not comfortable with this platform, you can use the InstructLab UI to submit knowledge. To learn more, head to the UI overview page.</p> <p>You can create a Git repository to host your knowledge contributions anywhere (GitLab, Gerrit, etc.), but it might be favorable to create one on GitHub. At the current time, we require a GitHub username to contribute, and all work is done in GitHub.</p> <p>The following instructions show you how to create a knowledge repository in GitHub and contribute to the taxonomy.</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#prerequisites","title":"Prerequisites","text":"<p>If you are submitting to the repository directly: - You have a GitHub account - You have a forked copy of the taxonomy repository - You have verified that the model does not already know the knowledge you want to submit</p> <p>If you are using the UI to submit: - You have a GitHub account - You have verified that the model does not already know the knowledge you want to submit</p> <p>Note</p> <p>Due to the higher volume, it will naturally take longer to receive acceptance for a knowledge contribution pull request than for a skill pull request. Smaller pull requests are simpler and require less time and effort to review.</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#preparing-your-knowledge-documents","title":"Preparing your knowledge documents","text":"<p>You need to set up your source documents as Markdown or PDF files in a git repository. You can organize the knowledge files in your repository however you want. You just need to ensure the YAML is pointing to the correct file.</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#accepted-sources-of-knowledge","title":"Accepted Sources of Knowledge","text":"<p>Warning</p> <p>We are currently only accepting sources from this list at this time due to legal requirements to keep InstructLab open source. We prefer that you keep your submission to articles from Wikipedia at this time. Our taxonomy triage team will reject any contributions that do not match this pattern. Thanks for helping us keep InstructLab 100% open source!</p> <p>These are the main knowledge domains that we are currently accepting knowledge contributions for:  arts, engineering, geography, history, linguistics, mathematics, philosophy, religion, science, and technology.</p> <p>Due to the open source nature of InstructLab, all content has to meet specific licensing requirements. This list has currently approved sources for knowledge. If you wish to use a different source, we need to approve it, and that means your submission will be on hold until we get legal review and approval. Please be patient!</p> Domain Name Status Notes Wikipedia approved - Project Gutenberg approved Pre-1927 works; public domain under US copyright law Wikisource (library) approved \"free library that anyone can improve\" OpenStax textbooks family of publications approved - The Open Organization publications approved - The Scrum Guide approved - US Congress site reviewed - manually verify US government sources may have different licensing; a legal review will need to verify each source US White House site reviewed - manually verify US government sources may have different licensing; a legal review will need to verify each source US Senate site reviewed - manually verify US government sources may have different licensing; a legal review will need to verify each source US IRS site reviewed - manually verify US government sources may have different licensing; a legal review will need to verify each source NASA reviewed - manually verify See guidelines Smithsonian Libraries reviewed - manually verify For any material marked \\\"No Copyright - United States\" or \"CC0\" as described here European Union (EU) site reviewed - manually verify Specifically documents submitted under \"public registrars\" as described here Internet Archive reviewed - manually verify Pre-1927 works; public domain under US copyright law PLOS family of open access journals reviewed - manually verify - Open Practice Library reviewed - manually verify - Cynefin.io wiki reviewed - manually verify - The Open Education Project reviewed - manually verify -"},{"location":"taxonomy/upstream/knowledge_contribution_details/#creating-your-own-knowledge-repository","title":"Creating your own knowledge repository","text":"<p>To create a new GitHub repository, follow the GitHub documentation in Creating a new repository.</p> <p>The specific steps are listed as follows:</p> <ol> <li>In your GitHub profile page, navigate to the repositories tab. You will see a search bar where you can search your repositories or create a new one.</li> <li>This takes you to a page titled \u201cCreate a new repository\u201d. Create a custom name for your repository and add a <code>README.md</code> file. For example, \u201cknowledge_contributions\u201d could be a good name for your repository.</li> <li>Click \u201cCreate\u201d when you are all set.</li> </ol>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#convert-your-knowledge-documentation-to-markdown-or-pdf","title":"Convert your knowledge documentation to Markdown or PDF","text":"<p>There are many online tools that can help you convert your documents to Markdown. If you are using a wiki page for your contributions, you can use pandocs to convert the documents. For Wikipedia sources on pandoc, use <code>from: mediawiki</code> and convert <code>to: markdown_strict</code> to access the proper Markdown format.</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#add-the-markdown-or-pdf-file-to-your-repository","title":"Add the Markdown or PDF file to your repository","text":"<p>To add a file to your GitHub repository, follow the GitHub documentation in Adding a file to a repository.</p> <p>The specific steps are listed as follows:</p> <ol> <li>Navigate to \u201cAdd files\u201d. Click \u201cCreate new file\u201d if you want to manually add your Markdown content. Click \u201cUpload files\u201d if you have a file locally to add.</li> <li> <p>Add a description and commit your changes.</p> <p>Since this is your own repository, you can commit directly to the <code>main</code> branch.</p> </li> <li> <p>You can then see your new content in your repository.</p> </li> </ol> <p>Important</p> <p>Make a note of your commit SHA; you'll need it for your <code>qna.yaml</code>.</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#creating-your-knowledge-submission-in-github","title":"Creating your knowledge submission in GitHub","text":"<p>For knowledge submissions, we need a <code>qna.yaml</code> file and an <code>attribution.txt</code> file.</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#the-qnayaml-file","title":"The <code>qna.yaml</code> file","text":"<p>For the current version of the taxonomy, version 3, here are the available fields:</p> <p>Note</p> <p>Tokens in the case of context, questions, and answers can fit to \"words,\" but it's specifically tokens, and not words, that are the limitations.</p> Key Type Required Constraints Value Notes <code>version</code> Y integer - <code>3</code> The taxonomy schema version used in the <code>qna.yaml</code> file. Defined in instructlab/schema <code>created_by</code> Y string - Your GitHub username - <code>domain</code> Y string - Knowledge sub-category The knowledge domain which is used in prompts to the teacher model during synthetic data generation. The domain should be brief such as the title to a textbook chapter or section. <code>seed_examples</code> Y array at least 5 sets null This is a collection of questions and answers with context from the knowledge document that InstructLab uses to generate data synthetically. <code>context</code> Y string &lt; 500 tokens A chunk of the document showing off the different unique content to help guide the teacher model. If you have only text, that's one thing, but if you have tables or other content, be sure to add that, too. This should be a copy-paste from the Markdown version of your document <code>questions_and_answers</code> Y array at least 3 pairs per context null This is a collection of questions and answers. <code>question</code> Y string &gt; 250 tokens A question related to the grounded in the relevant context Questions are things you'd expect someone to ask the model based on the context given. This will be used for synthetic data generation. <code>answer</code> Y string &gt; 250 tokens An answer for the question, longer then a one-word answer. Answers are what you'd like the model to give as an answer. It will not be an exact answer the model always gives. <code>document_outline</code> Y string - This provides the context specific for each document chunk; this should be as specific as you possibly can get. <code>document</code> Y object - null The collection of data for the knowledge document. <code>repo</code> Y string a git URL The URL (with a <code>.git</code> suffix) that identifies your git repo where you've stored your knowledge documents - <code>commit</code> Y string full commit hash A SHA1 full commit hash that corresponds to the document in the repo This hash must be exactly where the system can find the document. <code>patterns</code> Y array <code>*.md</code>, <code>*.pdf</code> A list of glob patterns specifying the files in the repo. Any glob pattern that starts with <code>*</code> must be quoted due to YAML rules. Currently, the system accepts <code>.md</code> and <code>.pdf</code> files. <p>Important</p> <p>There must be at least 5 sets of 3 questions and 3 answers with context in every <code>qna.yaml</code> file. Also the \"context blocks\" should be as diverse and unique as possible. The goal is to get as much different information in so as the teacher LLM reads through the document it gets \"inspired\" by the different content.</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#an-example-file","title":"An example file","text":"<p>To build a strong taxonomy,</p>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#create-a-pull-request-in-the-taxonomy-repository","title":"Create a pull request in the taxonomy repository","text":"<p>Navigate to your forked taxonomy repository and ensure it is up-to-date.</p> <p>There are a few ways you can create a pull request:</p> <ul> <li>For details on the local process, check out The GitHub Workflow Guide in the Kubernetes documentation and the GitHub flow in the GitHub documentation.</li> <li>For details on contributing using the GitHub webpage UI, see Contributing using the GH UI or Creating a pull request in the GitHub documentation.</li> </ul>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#example-of-a-directory-tree","title":"Example of a directory tree","text":"<p>In the taxonomy repository, here's what the previously referenced knowledge might look like in the tree:</p> <pre><code>[...]\n\n\u2514\u2500\u2500 knowledge\n    \u2514\u2500\u2500 science\n        \u251c\u2500\u2500 astronomy\n        \u2502 \u2514\u2500\u2500 constellations\n        \u2502     \u2514\u2500\u2500 Phoenix &lt;=== here it is :)\n        \u2502     |    \u2514\u2500\u2500 qna.yaml\n        |     |        attribution.txt\n        \u2502     \u2514\u2500\u2500 Orion\n        \u2502          \u2514\u2500\u2500 qna.yaml\n        |              attribution.txt\n[...]\n</code></pre>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#verification","title":"Verification","text":"<p>Here are a few things to check before seeking reviews for your contribution:</p> <ul> <li>Your <code>qna.yaml</code> follows the proper formatting. See examples in Knowledge: YAML examples</li> <li>Ensure all parameters are set. Especially the <code>document</code>, <code>repo</code>, <code>commit</code> and <code>pattern</code> keys; these parameters are specific to knowledge contributions and require more analysis.</li> <li>Include an <code>attribution.txt</code> file for citing your sources. see For your attribution.txt file for more information.</li> </ul>"},{"location":"taxonomy/upstream/knowledge_contribution_details/#pr-upstream-workflow","title":"PR Upstream Workflow","text":"<p>The following table outlines the expected timing for the PRs you have submitted. The PRs go through a few steps, and checks, but you should be able to map your <code>label</code> to the place that it is in.</p> Label Actor Action Duration - Contributor Submit PR - - Contributor Fix failed PR checks - triage-needed Triager Review PR, ask for changes Days triage-dco-requested Contributor Fix DCO - triage-requested-changes Contributor Make requested changes Days precheck-generate-ready Triager Run prechecks and generate Days community-build-ready Backend Model gets retrained Weeks Triager Check the numbers and PR merged or closed -"},{"location":"taxonomy/upstream/knowledge_contribution_details/#submissions","title":"Submissions","text":"<p>To make the <code>qna.yaml</code> files easier and faster for humans to read, it is recommended to specify <code>version</code> first, followed by <code>task_description</code>, then <code>created_by</code>, and finally <code>seed_examples</code>. In <code>seed_examples</code>, it is recommended to specify <code>context</code> first (if applicable), followed by <code>question</code> and <code>answer</code>.</p> <p>Example <code>qna.yaml</code></p> <pre><code>version: 2\ntask_description: &lt;string&gt;\ncreated_by: &lt;string&gt;\nseed_examples:\n  - question: &lt;string&gt;\n    answer: |\n      &lt;multi-line string&gt;\n  - context: |\n      &lt;multi-line string&gt;\n    question: &lt;string&gt;\n    answer: |\n      &lt;multi-line string&gt;\n  # ...\n</code></pre> <p>Then, you create an <code>attribution.txt</code> file that includes the sources of your information, if any. These sources can also be self-authored sources for skills.</p> <p>Fields in <code>attribution.txt</code></p> <pre><code>[Link to source]\n[Link to work]\n[License of the work]\n[Creator name]\n</code></pre> <p>Example of a self-authored source <code>attribution.txt</code></p> <pre><code>Title of work: Customizing an order for tea\nLink to work: -\nLicense of the work: CC BY-SA-4.0\nCreator names: Jean-Luc Picard\n</code></pre> <p>You may copy this example and replace the title of the work (your skill) and the creator name to submit a skill. The license is Creative Commons Attribution-ShareAlike 4.0 International, which is shortened to <code>CC BY-SA-4.0</code>.</p> <p>For more information on what to include in your <code>attribution.txt</code> file, reference the general contribution guidelines.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/","title":"Skills contribution details","text":"<p>Info</p> <p>The following information is if you are comfortable contributing using GitHub, a version control system primarily used for code. If you are not comfortable with this platform, you can use the InstructLab UI to submit knowledge. To learn more, head to the UI overview page.</p> <p>Skills require a much smaller volume of content than knowledge contributions. An entire skill contribution to the taxonomy tree can be just a few lines of YAML in the <code>qna.yaml</code> file (\"qna\" is short for \"questions and answers\") and an <code>attribution.txt</code> file for citing sources.</p> <p>Your skills contribution pull requests include:</p> <ul> <li>A <code>qna.yaml</code> that contains a set of key/value entries</li> <li>An <code>attribution.txt</code> that includes the sources for the information used in the <code>qna.yaml</code>. Even if you are authoring the skill with no additional sources, you must have this file for legal purposes.</li> </ul> <p>Tip</p> <p>The skill taxonomy structure is used in several ways:</p> <ol> <li>To select the right subset of the taxonomy to use for data generation.</li> <li>To determine the interpretability by human contributors and maintainers.</li> <li>As part of the prompt to the LLM used to generate synthetic samples.</li> </ol> <p>Important</p> <p>There is a limit to how much content can exist in the question/answer pairs for the model to process. Due to this, only add a maximum of around 2300 words to your question and answer seed example pairs in the <code>qna.yaml</code> file.</p> <p>Compositional skills can either be grounded (includes a context) or ungrounded (does not include a context).  Grounded or ungrounded is declared in the taxonomy tree, for example: <code>linguistics/writing/poetry/haiku/</code> (ungrounded) or <code>grounded/linguistics/grammar</code> (grounded). The <code>qna.yaml</code> is in the final node.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#the-structure-of-the-qnayaml-file","title":"The structure of the <code>qna.yaml</code> file","text":"<p>Taxonomy skill files must be a valid YAML file named <code>qna.yaml</code>. Each <code>qna.yaml</code> file contains a set of key/value entries with the following keys:</p> Field Type Required? Content <code>version</code> integer yes The value must be the number 3. <code>task_description</code> string yes A description of the task which is used in prompts to the teacher model during synthetic data generation. The description should be detailed and prescriptive to improve the teacher model's responses. <code>created_by</code> string yes The GitHub username of the contributor. <code>seed_examples</code> array yes A collection of key/value entries. New submissions should have at least five entries, although older files may have fewer.Note collections are nested lists, like sub-entries in a bulleted list. <code>context</code> string only for grounded skills Part of the <code>seed_examples</code> collection.Grounded skills require the user to provide context containing information that the model is expected to take into account during processing. This is different from knowledge, where the model is expected to gain facts and background knowledge from the tuning process.Note: The context key should not be used for ungrounded skills. <code>question</code> string yes Part of the <code>seed_examples</code> collection.A question for the model. <code>answer</code> string yes Part of the <code>seed_examples</code> collection.The desired response from the model. <p>Other keys at any level are currently ignored. Learn more in the file structure overview for skills.</p> <p>Important</p> <p>Each <code>qna.yaml</code> file requires a minimum of five question and answer pairs.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#submissions","title":"Submissions","text":"<p>For the upstream taxonomy, there are two required files: the <code>qna.yaml</code> file and the <code>attribution.txt</code> file.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#the-qnayaml-file","title":"The <code>qna.yaml</code> file","text":"<p>To make the <code>qna.yaml</code> files easier and faster for humans to read, it is recommended to specify <code>version</code> first, followed by <code>task_description</code>, then <code>created_by</code>, and finally <code>seed_examples</code>. In <code>seed_examples</code>, it is recommended to specify <code>context</code> first (if applicable), followed by <code>question</code> and <code>answer</code>.</p> <p>Example <code>qna.yaml</code></p> <pre><code>version: 2\ntask_description: &lt;string&gt;\ncreated_by: &lt;string&gt;\nseed_examples:\n  - question: &lt;string&gt;\n    answer: |\n      &lt;multi-line string&gt;\n  - context: |\n      &lt;multi-line string&gt;\n    question: &lt;string&gt;\n    answer: |\n      &lt;multi-line string&gt;\n  # ...\n</code></pre>"},{"location":"taxonomy/upstream/skills_contribution_details/#the-attributiontxt-file","title":"The <code>attribution.txt</code> file","text":"<p>Then, you create an <code>attribution.txt</code> file that includes the sources of your information, if any. These sources can also be self-authored sources for skills.</p> <p>Fields in <code>attribution.txt</code></p> <pre><code>[Link to source]\n[Link to work]\n[License of the work]\n[Creator name]\n</code></pre> <p>Example of a self-authored source <code>attribution.txt</code></p> <pre><code>Title of work: Customizing an order for tea\nLink to work: -\nLicense of the work: CC BY-SA-4.0\nCreator names: Jean-Luc Picard\n</code></pre> <p>You may copy this example and replace the title of the work (your skill) and the creator name to submit a skill. The license is Creative Commons Attribution-ShareAlike 4.0 International, which is shortened to <code>CC BY-SA-4.0</code>.</p> <p>For more information on what to include in your <code>attribution.txt</code> file, reference the general contribution guidelines.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#writing-yaml","title":"Writing YAML","text":"<p>If you have not written YAML before, YAML is a text file where indentation matters.</p> <p>Tip</p> <ul> <li>Spaces and indentation matter in YAML. Use two spaces to indent.<ul> <li>Don't use tabs!</li> </ul> </li> <li>Do not have trailing spaces at the end of a line.</li> <li>Each example in <code>seed_examples</code> begins with a dash (<code>-</code>). Place this dash in front of the first field (<code>question</code> or <code>context</code>). The remaining keys in the example should not have this dash.</li> <li>Some special characters such as a double quotation mark (<code>\"</code>) and an apostrophe or single quotation mark (<code>'</code>) need to be escaped with backslash. This is why some of the lines for keys in the example YAML start the value with the pipe character (<code>|</code>) followed a new line and then an indented multi-line string. This character disables all of the special characters in the value for the key.You might also want to use the pipe character (<code>|</code>) for multi-line strings.</li> <li>Consider quoting all values with double quotation marks (<code>\"</code>) to avoid surprising YAML parser behavior (e.g., Yes answer can be interpreted by the parser as a boolean of <code>True</code> value, unless \"Yes\" is quoted.)</li> <li>See yaml-multiline.info for more info.</li> </ul> <p>We recommend you lint, or verify, your YAML using a tool. One linter option is yamllint.com. You can copy/paste your YAML into the box and select Go to have it analyze your YAML and make recommendations. Online tools like prettified and yaml-validator can automatically reformat your YAML to adhere to our <code>yamllint</code> PR checks, such as breaking lines longer than 120 characters.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#examples","title":"Examples","text":""},{"location":"taxonomy/upstream/skills_contribution_details/#ungrounded-compositional-skill-yaml-example","title":"Ungrounded compositional skill: YAML example","text":"<pre><code>version: 2\ntask_description: 'Teach the model how to rhyme.'\ncreated_by: juliadenham\nseed_examples:\n  - question: What are 5 words that rhyme with horn?\n    answer: warn, torn, born, thorn, and corn.\n  - question: What are 5 words that rhyme with cat?\n    answer: bat, gnat, rat, vat, and mat.\n  - question: What are 5 words that rhyme with poor?\n    answer: door, shore, core, bore, and tore.\n  - question: What are 5 words that rhyme with bank?\n    answer: tank, rank, prank, sank, and drank.\n  - question: What are 5 words that rhyme with bake?\n    answer: wake, lake, steak, make, and quake.\n</code></pre> <p>Here is the location of this YAML in the taxonomy tree. Note that the YAML file itself, plus any added directories that contain the file, is the entirety of the skill in terms of a taxonomy contribution:</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#ungrounded-compositional-skill-directory-tree-example","title":"Ungrounded compositional skill: Directory tree example","text":"<pre><code>[...]\n\n\u2514\u2500\u2500 writing\n    \u2514\u2500\u2500 poetry\n    |   \u2514\u2500\u2500 haiku &lt;=== here it is :)\n    |   |   \u2514\u2500\u2500 qna.yaml\n    |   |       attribution.txt\n        [...]\n    \u2514\u2500\u2500 prose\n    |   \u2514\u2500\u2500 debate\n    |   |   \u2514\u2500\u2500 qna.yaml\n    |   |       attribution.txt\n    [...]\n\n[...]\n</code></pre>"},{"location":"taxonomy/upstream/skills_contribution_details/#grounded-compositional-skill-yaml-example","title":"Grounded compositional skill: YAML example","text":"<p>Remember that grounded compositional skills require additional context and include a <code>context</code> field.</p> <p>This example snippet assumes the GitHub username <code>mairin</code> and shows some of the question/answer pairs present in the actual file:</p> <pre><code>version: 2\ntask_description: |\n    This skill provides the ability to read a markdown-formatted table.\ncreated_by: mairin # Use your GitHub username; only one creator supported\nseed_examples:\n  - context: |\n      | **Breed**      | **Size**     | **Barking** | **Energy** |\n      |----------------|--------------|-------------|------------|\n      | Afghan Hound   | 25-27 in     | 3/5         | 4/5        |\n      | Labrador       | 22.5-24.5 in | 3/5         | 5/5        |\n      | Cocker Spaniel | 14.5-15.5 in | 3/5         | 4/5        |\n      | Poodle (Toy)   | &lt;= 10 in     | 4/5         | 4/5        |\n    question: |\n      Which breed has the most energy?\n    answer: |\n      The breed with the most energy is the Labrador.\n  - context: |\n      | **Name** | **Date** | **Color** | **Letter** | **Number** |\n      |----------|----------|-----------|------------|------------|\n      | George   | Mar 5    | Green     | A          | 1          |\n      | Gr\u00e1inne  | Dec 31   | Red       | B          | 2          |\n      | Abigail  | Jan 17   | Yellow    | C          | 3          |\n      | Bhavna   | Apr 29   | Purple    | D          | 4          |\n      | R\u00e9my     | Sep 9    | Blue      | E          | 5          |\n    question: |\n      What is Gr\u00e1inne's letter and what is her color?\n    answer: |\n      Gr\u00e1inne's letter is B and her color is red.\n  - context: |\n      | Banana | Apple      | Blueberry | Strawberry |\n      |--------|------------|-----------|------------|\n      | Yellow | Red, Green | Blue      | Red        |\n      | Large  | Medium     | Small     | Small      |\n      | Peel   | Peel       | No peel   | No peel    |\n    question: |\n      Which fruit is blue, small, and has no peel?\n    answer: |\n      The blueberry is blue, small, and has no peel.\n</code></pre>"},{"location":"taxonomy/upstream/skills_contribution_details/#grounded-compositional-skill-directory-tree-example","title":"Grounded compositional skill: Directory tree example","text":"<pre><code>[...]\n\ngrounded\n\u2514\u2500\u2500 technology\n    \u2514\u2500\u2500 machine_learning\n        \u2514\u2500\u2500 natural_language_processing\n    |   |     \u2514\u2500\u2500 information_extraction\n    |            \u2514\u2500\u2500 inference\n    |   |            \u2514\u2500\u2500 qualitative\n    |   |               \u251c\u2500\u2500 sentiment\n    |   |               |     \u2514\u2500\u2500 qna.yaml\n    |   |               |         attribution.txt\n    \u2502                   \u251c\u2500\u2500 quantitative\n    \u2502   \u2502                   \u251c\u2500\u2500 table_analysis &lt;=== here it is :)\n    \u2502   |   |               |     \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502   \u2502               |         attribution.txt\n\n[...]\n</code></pre>"},{"location":"taxonomy/upstream/skills_contribution_details/#accepted-skills","title":"Accepted skills","text":"<p>Warning</p> <p>We are not accepting foundational or core skills at this time.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#creative-writing-and-poetics","title":"Creative writing and poetics","text":"<p>Adding new types of documents and writing styles to the LLM are welcome. Consider:</p> <ul> <li>Song lyrics</li> <li>Soliloquies</li> <li>Five paragraph essays</li> <li>Arguments</li> </ul>"},{"location":"taxonomy/upstream/skills_contribution_details/#learning-to-format-information","title":"Learning to format information","text":"<p>Skills to better format and reassemble information are helpful.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#table-analysis-and-processing","title":"Table analysis and processing","text":"<p>Consider:</p> <ul> <li>Drawing verbal inferences and conclusions about what's in a table</li> <li>Sorting</li> <li>Selecting</li> <li>Joining</li> </ul>"},{"location":"taxonomy/upstream/skills_contribution_details/#qualitative-inference-and-chain-of-thought-reasoning","title":"Qualitative inference and chain-of-thought reasoning","text":"<p>Example:</p> <p>Mary is taller than John. John is taller than Anna. Is Anna taller than Mary?</p> <p>Example:</p> <p>An elephant, a mouse and a horse are in a room. How would they be ordered if they were standing in order by size?</p> <p>Great skills in this category should include the correct line of reasoning in the answer, not just what the answer is.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#word-problems","title":"Word problems","text":"<p>Is your LLM smarter than a second-grader?</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#trust-and-safety","title":"Trust and safety","text":"<p>Please avoid HAP (hate, abuse and profanity) and PII (personal identifiable information) in your examples.</p> <p>Anything related to trust and safety will be flagged for higher-level review.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#searching-extraction-and-summarization","title":"Searching, extraction, and summarization","text":"<p>Skills to select odd information in a document, draw conclusions, pull out information, draw insights, or generate TODOs from information provided in the \"context\" field are welcome.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#complex-rulesets-and-games","title":"Complex rulesets and games","text":"<p>Note</p> <p>This is a good example of the need for a grounded skill. Grounded skills require the user to provide context containing information that the model is expected to take into account during processing. This is different from knowledge, where the model is expected to gain facts and background knowledge from the tuning process. Context added when tuning a grounded skill would need to be again provided by the end user at inference time. The skill here is better adherence to the rule set.</p> <p>To add a skill for a structured game or other task with a complex rule set, use a grounded skill. Add the rules to the game as \"context\" in every example. Add the interpretation as a question.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#writing-style-and-personalities","title":"Writing Style and Personalities","text":"<p>When adding a skill, expect that you're tuning a fairly general purpose LLM to behave better given particular circumstances.</p> <p>If you want to add a skill to better adopt a particular personality - say, \"a little boy living in the 1800s\" - that context needs to be provided in either the \"context\" or \"question\" field.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#instruction-following-behavior","title":"Instruction-Following Behavior","text":"<p>LLMs could be better at following extra instructions in a prompt about how to do a task, such as: \"Keep your response to 200 words.\" Or: \"Only produce 10 items.\" Skills to improve this behavior will help the model behave with more precision.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#skills-to-avoid","title":"Skills to Avoid","text":"<p>There are several types of skills that we don't expect this procedure to improve. Most skills in these categories will be rejected.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#math","title":"Math","text":"<p>Trying to make the LLM solve math problems will be rejected.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#real-world-knowledge-based-skills","title":"Real world knowledge-based skills","text":"<p>Unless it can be framed as a \"grounded skill\", where the user is expected to provide context, knowledge contributions will be a separate part of the taxonomy. Skills shouldn't expect the model to come up with its own facts, but instead assemble facts provided.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#red-teaming","title":"Red Teaming","text":"<p>Adversarial questions and answers will be rejected at this time.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#turing-complete-style-problems","title":"Turing-complete style problems","text":"<p>These are an edge case, but things like palindromes and regexes, where getting the right answer with a non-stochastic program would be easy, aren't good targets for the moment.</p> <p>Open an issue in the taxonomy repository if you have an idea in this space before submitting a PR.</p>"},{"location":"taxonomy/upstream/skills_contribution_details/#small-changes-to-original-response","title":"Small Changes to Original Response","text":"<p>If the original LLM response is pretty close, but it's not responding to your exact expectations, a skill is not the right way to solve that problem.</p>"},{"location":"user-interface/env_oauth_config/","title":"InstructLab UI .env & OAuth config.","text":"<p>This chapter is dedicated to the settings for running the User Interface locally.</p>"},{"location":"user-interface/env_oauth_config/#env","title":".env","text":"<pre><code>IL_UI_ADMIN_USERNAME=admin\nIL_UI_ADMIN_PASSWORD=password\nIL_UI_DEPLOYMENT=dev\n\nOAUTH_GITHUB_ID=&lt;OAUTH_APP_ID&gt;\nOAUTH_GITHUB_SECRET=&lt;OAUTH_APP_SECRET&gt;\n\nNEXTAUTH_SECRET=your_super_secret_random_string\nNEXTAUTH_URL=http://localhost:3000\n\nIL_GRANITE_API=&lt;GRANITE_HOST&gt;\nIL_GRANITE_MODEL_NAME=&lt;GRANITE_MODEL_NAME&gt;\nIL_MERLINITE_API=&lt;MERLINITE_HOST&gt;\nIL_MERLINITE_MODEL_NAME=&lt;MERLINITE_MODEL_NAME&gt;\n\nGITHUB_TOKEN=&lt;TOKEN FOR OAUTH INSTRUCTLAB MEMBER LOOKUP&gt;\nTAXONOMY_DOCUMENTS_REPO=github.com/instructlab-public/taxonomy-knowledge-docs\nNEXT_PUBLIC_AUTHENTICATION_ORG=&lt;AUTHENTICATION_ORG&gt;\nNEXT_PUBLIC_TAXONOMY_REPO_OWNER=&lt;GITHUB_ACCOUNT&gt;\nNEXT_PUBLIC_TAXONOMY_REPO=&lt;REPO_NAME&gt;\n</code></pre>"},{"location":"user-interface/env_oauth_config/#oauth","title":"OAuth","text":"<p>To set up the github OAuth settings for the UI, please refer to the steps below</p> <p><code>Github settings</code> -&gt; <code>Developer settings</code> -&gt; <code>OAuth Apps</code> -&gt; <code>Create New OAuth App</code> </p> <p>Fill in the fields as shown below:</p> <p></p> <p>Set the <code>Homepage URL</code> and the <code>Authorization Callback URL</code> to <code>http://localhost:3000</code>.</p> <p>Once the app is created, there will be an option to create a secret. Press the Create Secret button and it will generate a secret for you.</p> <p>Update the .env files with the new ID and secret ID generated, <code>OAUTH_GITHUB_ID</code> = Client ID, <code>OAUTH_GITHUB_SECRET</code> = Client Secret</p> <p>Note</p> <p>If you prefer to not set up the OAuth, we recommend for you to reach out to the UI Maintainers in our <code>#ui</code> discord server or slack channel , where they will provide details for setting up an OAuth app for the instructlab-public org.</p>"},{"location":"user-interface/knowledge_contributions/","title":"InstructLab UI Skills & Knowledge contributions","text":"<p>The UI Simplifies the process for Skills &amp; Knowledge contributions by:</p> <ul> <li> <p>Minimising risk of human error when writing YAML by using the web form. </p> </li> <li> <p>Directly submit a GitHub pull request with a press of a button.</p> </li> </ul> <p>When the form is filled out, you also are given the option to download the YAML and attribution files to your local machine, and to view the form in its original YAML structure before submission.</p> <p>You can view all your submissions on the dashboard page.</p> <p>Warning</p> <p>Even when running the UI locally, you must be logged in via GitHub to successfully submit your Knowledge and Skills contributions. You can still fill out the form, and download the YAML and attribution files.</p> <p>For tips on writing Skills &amp; Knowledge contributions, please visit the documentation under the Taxonomy heading.</p>"},{"location":"user-interface/knowledge_contributions/#knowledge-contributions","title":"Knowledge Contributions","text":"<p>Firstly you will need to find a source document for your knowledge. Accepted sources can be found here.</p> <p>Navigate to the Contribute section of the sidebar and click Knowledge. Here you will see the form to contribute Knowledge to the open-source taxonomy tree.</p>"},{"location":"user-interface/knowledge_contributions/#author-information","title":"Author Information","text":"<p>Use your GitHub account email address and full name here. This will make sure that this contribution and the data with it is properly signed off and credited to you.</p>"},{"location":"user-interface/knowledge_contributions/#knowledge-information","title":"Knowledge Information","text":"<p>In the box for the submission summary, give a brief description of what your knowledge is. This will be used in the PR description after you submit. Below that you will fill in the domain the knowledge you are adding would fall under. For example, if you want to teach the model the winners at the 2024 Olympics, you might put \"Olympic history\" as the domain.</p> <p>Note</p> <p>The task domain is a critical part of the SDG process and will be part of the prompt when generating synthetic data. Ask your self: \"What kind of textbook would contain the knowledge I am trying to teach the model?\"</p> <p>The final box in Knowledge Information will contain an outline of the document.</p>"},{"location":"user-interface/knowledge_contributions/#taxonomy-directory-path","title":"Taxonomy Directory Path","text":"<p>Using the dropdown menu, you will be able to select where you think your knowledge will fit the best in the taxonomy tree.</p> <p>Note</p> <p>Proper placement within the taxonomy tree will allow other users to more accurately locate existing leaf nodes. Ask yourself: \"What section of the library would I expect to find my skill under?\"</p>"},{"location":"user-interface/knowledge_contributions/#seed-examples","title":"Seed Examples","text":"<p>Here you will begin filling out your QNA examples that represent the knowledge you are trying to teach. There must be exactly 5 seed examples in this section. Each seed example needs one unique piece of context from your source document with no more than 500 characters. From this piece of context, you will need to create 3 QNA pairs that can be answered from the context you have selected.</p>"},{"location":"user-interface/knowledge_contributions/#document-information","title":"Document Information","text":"<p>You must prepare a markdown file version of the document you wish to use for the knowledge submission. By dragging and dropping the markdown file into the box, and clicking the submit files button, a forked version of the taxonomy repository will be automatically created on your GitHub profile. </p> <p></p> <p></p> <p>If you've already uploaded the markdown file to your GitHub, you can switch to manually adding the document, and entering the <code>commit sha</code>.</p> <p></p> <p>Note</p> <p>Your knowledge document must be place in a public GitHub repo in order to be accepted. When your submission is being reviewed and merged, this document will need to be accessible publicly.</p>"},{"location":"user-interface/knowledge_contributions/#attribution-information","title":"Attribution Information","text":"<p>Linking the source you used for information. Wikipedia articles change overtime, make sure to add the oid from the wikipedia article, which you can find by clicking on <code>View history</code>, and select the relevant version. </p> <p>The license for Wikipedia articles will be \"CC-BY-SA-4.0\" and Creator Name can simply be \"Wikipedia Authors\"</p>"},{"location":"user-interface/knowledge_contributions/#dashboard","title":"Dashboard","text":"<p>Once you have submitted a Skills or Knowledge Contribution, you can view it on your dashboard, and edit the submission via the UI if needed.</p> <p></p> <p>Next Steps</p>"},{"location":"user-interface/playground_chat/","title":"InstructLab UI Playground","text":"<p>To run with a locally run model, make sure that iLab model serve is running in a separate terminal. If you are unsure on how to do this, please visit the Intro to serve and chat section of this document.</p> <p>If you go to <code>Playground &gt; Chat</code> by using the side navigation bar, you can interact with the Merlinite and Granite models. </p> <p></p> <p>If you are running the ui within a dev environment, the model won't reply because a Granite/Merinite model endpoint hasn't been given. In this case, we will create a new custom model endpoint, using our locally hosted quantised model.</p> <p>To add a custom model endpoint, go to <code>Playground &gt; Custom Model Endpoints</code> and press the <code>Add Endpoint</code> button on the right side. </p> <p>You will have 3 fields to fill out:</p> <ul> <li> <p>The URL, where your customised model is hosted, if hosting locally, the URL would be <code>http://127.0.0.1:8000/</code></p> </li> <li> <p>The Model Name, <code>merlinite-7b-lab-Q4_K_M.gguf</code></p> </li> <li> <p>API Key, you may put any text in here; in this case I've used <code>randomCharacters</code>. If you are setting up an API key, please provide the key in this section.</p> </li> </ul> <p></p> <p>Go back to the playground chat, select newly added model and chat.</p> <p></p> <p>Next Steps</p>"},{"location":"user-interface/skills_contributions/","title":"InstructLab UI Skills & Knowledge contributions","text":"<p>The UI Simplifies the process for Skills &amp; Knowledge contributions by:</p> <ul> <li> <p>Minimising risk of human error when writing YAML by using the web form. </p> </li> <li> <p>Directly submit a GitHub pull request with a press of a button.</p> </li> </ul> <p>When the form is filled out, you also are given the option to download the YAML and attribution files to your local machine, and to view the form in its original YAML structure before submission.</p> <p>You can view all your submissions on the dashboard page.</p> <p>Warning</p> <p>Even when running the UI locally, you must be logged in via github to successfully submit your Knowledge and Skills contributions. You can still fill out the form, and download the YAML and attribution files.</p> <p>For tips on writing Skills &amp; Knowledge contributions, please visit the documentation under the Taxonomy heading.</p>"},{"location":"user-interface/skills_contributions/#skill-contributions","title":"Skill Contributions","text":"<p>If you have found a missing skill in granite chat, you can create a skill contribution easily through the UI. Remember, a guide to help you learn about what a Skill is can be found in the InstructLab GitHub</p> <p>Navigate to the Contribute section of the sidebar and click Skill. Here you will see the form to contribute a Skill to the open-source taxonomy tree.</p> <p>On this page you will find all the necessary pieces to fill in to create you Skill contribution, let's go through each one here.</p>"},{"location":"user-interface/skills_contributions/#author-information","title":"Author Information","text":"<p>Use your GitHub account email address and full name here. This will make sure that this contribution and the data with it is properly signed off and credited to you.</p>"},{"location":"user-interface/skills_contributions/#skill-information","title":"Skill Information","text":"<p>In the box for the submission summary, give a brief description of what your skill is. This will be used in the PR description after you submit. Below that you will fill in a detailed description of what your skill is teaching the model to do. For example, if your skill is extracting the date from a bean count ledger, you would fill in \"Teaching the model to extract the date from a bean count ledger\"</p> <p>Note</p> <p>The detailed description is very important for quality synthetic data generation. What you fill in for this box will be used as prompting during the SDG process. Imagine you are telling the teacher model: \"The task is (your description here)\"</p>"},{"location":"user-interface/skills_contributions/#taxonomy-directory-path","title":"Taxonomy Directory Path","text":"<p>Using the dropdown menu, you will be able to select where you think your skill will fit the best in the taxonomy tree. If your skill is a grounded skill (meaning that it requires context to answer) you will need to navigate into the \"grounded\" section of the taxonomy tree before selecting the proper subsection. Otherwise, select the section you think best represents the subject of your skill.</p> <p>Note</p> <p>Proper placement within the taxonomy tree will allow other users to more accurately locate existing leaf nodes. Ask yourself: \"What section of the library would I expect to find my skill under?\"</p>"},{"location":"user-interface/skills_contributions/#seed-examples","title":"Seed Examples","text":"<p>Here you will begin filling out your QNA examples that represent the skill you are trying to teach. You will need a minimum of 5 seed examples, which each example consisting of a Question and Answer. If your skill is a grounded skill and requires context, you will also add context for each QNA pair here.</p> <p>Note</p> <p>The QNA pairs that you create here should be diverse. You should try to rephrase questions in different ways and create unique contexts. For example, in the case of date extraction from bean count, you do not want to use the same date repeatedly in your seed examples, as this may have the unintended consequence of teaching the model to simply regurgitate the same date when asked any date extraction question.</p>"},{"location":"user-interface/skills_contributions/#attribution","title":"Attribution","text":"<p>The information filled in this section will create the attribution.txt file needed for your submission. Fill in a concise title for your skill. If your skill is entirely self created, you can put your name as the creator and use Apache-2.0 as the license. Otherwise please refer to your source material for creator and licensing information.</p> <p>Once you have filled in all the necessary information, you can finally submit your skill! If you would like to download or view the qna.yaml or attribution.txt, there are drop down menus provided for both. If you choose to, you can download these files to create a PR in the taxonomy repository yourself. Otherwise, you can press submit and the UI will automatically create a PR under your GitHub namespace in the InstructLab Taxonomy repository.</p>"},{"location":"user-interface/skills_contributions/#dashboard","title":"Dashboard","text":"<p>Once you have submitted a Skills or Knowledge Contribution, you can view it on your dashboard, and edit the submission via the UI if needed.</p> <p></p> <p>Next Steps</p>"},{"location":"user-interface/ui_overview/","title":"InstructLab UI Overview","text":"<p>The InstructLab User Interface (UI) is an additional component in the InstructLab project to make Knowledge/Skills  contributions more accessible to the general user. Instead of having to edit a <code>qna.yaml</code> file, users can fill out the contribution forms on the web. The user can also view all of their contributions in a centralised list. You can also chat with the models directly on the playground.</p> <p>There are 2 ways to access the UI:</p> <ol> <li> <p>Visit the website at ui.instructlab.ai.</p> </li> <li> <p>Install and run the UI locally.</p> </li> </ol> <p>Note</p> <p>To log into the UI and submit Knowledge and Skills contributions, you must be a member of the instructlab-public GitHub repository.</p> <p>Note</p> <p>If you aren't a member of the repository, but still wish to experiment with the UI, we recommend you to reach out to the UI Maintainers in our <code>#ui</code> Discord server or Slack channel for an invitation.</p> <p>If you wish to not join but still wish to experiment, download it locally.</p>"},{"location":"user-interface/ui_overview/#prerequisites","title":"Prerequisites","text":"<p>Before you can run the User Interface locally, you first must: </p> <ul> <li> <p>Have a GitHub account</p> </li> <li> <p>Have Node.js installed</p> </li> </ul> <p>Warning</p> <p>This guide was written with <code>Node.js v20.17.0</code>. If you are using an older version of Node.js, please be aware that the application may not work as intended. </p>"},{"location":"user-interface/ui_overview/#installation-guide","title":"Installation Guide","text":"<ol> <li> <p>Download the InstructLab UI by running the following command in your desired directory:</p> <pre><code>git clone https://github.com/instructlab/ui\n</code></pre> <p>You can also download by visiting the InstructLab UI GitHub page and clicking the code, and then the download button.</p> </li> <li> <p>Set up the .env file. Go into the UI folder by running</p> <pre><code>cd ui\n</code></pre> <p>Inside the UI folder, create a new file called <code>.env</code> and paste the following content inside:</p> <pre><code>IL_UI_ADMIN_USERNAME=admin\nIL_UI_ADMIN_PASSWORD=password\nIL_UI_DEPLOYMENT=dev\n\nOAUTH_GITHUB_ID=&lt;OAUTH_APP_ID&gt;\nOAUTH_GITHUB_SECRET=&lt;OAUTH_APP_SECRET&gt;\n\nNEXTAUTH_SECRET=your_super_secret_random_string\nNEXTAUTH_URL=http://localhost:3000\n\nIL_GRANITE_API=&lt;GRANITE_HOST&gt;\nIL_GRANITE_MODEL_NAME=&lt;GRANITE_MODEL_NAME&gt;\nIL_MERLINITE_API=&lt;MERLINITE_HOST&gt;\nIL_MERLINITE_MODEL_NAME=&lt;MERLINITE_MODEL_NAME&gt;\n\nGITHUB_TOKEN=&lt;TOKEN FOR OAUTH INSTRUCTLAB MEMBER LOOKUP&gt;\nTAXONOMY_DOCUMENTS_REPO=github.com/instructlab-public/taxonomy-knowledge-docs\nNEXT_PUBLIC_AUTHENTICATION_ORG=&lt;AUTHENTICATION_ORG&gt;\nNEXT_PUBLIC_TAXONOMY_REPO_OWNER=&lt;GITHUB_ACCOUNT&gt;\nNEXT_PUBLIC_TAXONOMY_REPO=&lt;REPO_NAME&gt;\n</code></pre> <p>As of now, we are only concerned with <code>IL_UI_ADMIN_USERNAME</code>, <code>IL_UI_ADMIN_PASSWORD</code> and <code>IL_UI_DEPLOYMENT</code>. But throughout this UI guide we will be visiting the .env frequently. </p> <p>Save and close the .env file.</p> </li> <li> <p>Running Node commands to install and run.</p> <p>Back inside the UI folder, run this command to install needed files for the UI to run:</p> <pre><code>npm install\n</code></pre> <p>Note</p> <p>Depending on your internet speed and connectivity, this process may take a few minutes.</p> <p>When the installation is finished, start the UI by running:</p> <pre><code>make start-dev-local\n</code></pre> <p>The UI should now be up and running, you can visit it by entering <code>localhost:3000</code> in your browser, where it should bring you to the login screen.</p> <p></p> <p>You may stop the UI at any time by running:</p> <pre><code>make stop-dev-local\n</code></pre> </li> <li> <p>Logging in </p> <p>For now, we will log in by entering <code>admin</code> and <code>password</code> in the username and password fields respectively. You can change the username and password by editing the <code>IL_UI_ADMIN_USERNAME</code> and <code>IL_UI_ADMIN_PASSWORD</code> values in the .env file.</p> <p>We are not able to log in with GitHub right now since we haven't set up the GitHub token in the .env file. When logged in, you may see a popup saying that the UI is fetching your submissions, exit out of this notification. If you wish to set up the OAuth, visit the .env and OAuth config page.</p> </li> </ol> <p>Next Steps</p>"}]}